{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/sgarc/SemEval-2023-Task-10/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["tr version 4.35.2\n","Using device =>  cpu  torch  2.1.1+cu121\n"]}],"source":["import os\n","import transformers\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaTokenizer\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.nn.functional as F\n","from torch.utils.tensorboard import SummaryWriter\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import confusion_matrix, classification_report\n","import tqdm\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","import pickle\n","import re\n","import copy\n","import pprint\n","import time\n","\n","MAX_NO_OF_SPEAKERS = 8\n","MAX_DIALOGUE_LEN   = 33\n","original_labels    = ['abuse', 'adoration', 'annoyance', 'awkwardness', 'benefit', 'boredom', 'calmness', 'challenge', 'cheer', 'confusion', 'curiosity', 'desire', 'excitement', 'guilt', 'horror', 'humour', 'impressed', 'loss', 'nervousness', 'nostalgia', 'pain', 'relief', 'satisfaction', 'scold', 'shock', 'sympathy', 'threat']\n","train_count        = [31, 190, 1051, 880, 220, 78, 752, 214, 534, 486, 545, 180, 867, 216, 280, 153, 257, 351, 398, 65, 36, 173, 136, 94, 372, 209, 263]\n","\n","EMOTIONS           = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n","\n","sent_model = 'roberta-base-nli-stsb-mean-tokens'\n","\n","print('tr version', transformers.__version__)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using device => \",device, ' torch ', torch.__version__)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:19.592349Z","iopub.status.busy":"2023-11-14T16:06:19.592071Z","iopub.status.idle":"2023-11-14T16:06:29.220772Z","shell.execute_reply":"2023-11-14T16:06:29.219872Z","shell.execute_reply.started":"2023-11-14T16:06:19.592324Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["model loaded\n"]}],"source":["class EmotionClassifier(nn.Module):\n","    def __init__(self, n_classes):\n","        super(EmotionClassifier, self).__init__()\n","        self.bert = RobertaModel.from_pretrained('roberta-base')\n","        self.drop = nn.Dropout(p=0.3)\n","        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n","    def forward(self, input_ids, attention_mask):\n","        op = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n","        output = self.drop(op[1])\n","        return self.out(output), op[1]\n","\n","# load finetuned roberta model\n","roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","roberta_finetuned = EmotionClassifier(7).to(device)\n","#roberta_tf_checkpoint = torch.load('dump_files/finetuned/best_model_state_roberta.bin', map_location=torch.device(device))\n","#roberta_finetuned.load_state_dict(roberta_tf_checkpoint)\n","print('model loaded')\n","\n","\n","# Helper functions\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.222235Z","iopub.status.busy":"2023-11-14T16:06:29.221960Z","iopub.status.idle":"2023-11-14T16:06:29.444188Z","shell.execute_reply":"2023-11-14T16:06:29.443408Z","shell.execute_reply.started":"2023-11-14T16:06:29.222211Z"},"trusted":true},"outputs":[],"source":["train_csv = pd.read_json(\"EDiReF-Train-Data/Task 3/MELD_train_efr.json\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.460576Z","iopub.status.busy":"2023-11-14T16:06:29.460285Z","iopub.status.idle":"2023-11-14T16:06:29.465753Z","shell.execute_reply":"2023-11-14T16:06:29.464909Z","shell.execute_reply.started":"2023-11-14T16:06:29.460552Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>episode</th>\n","      <th>speakers</th>\n","      <th>emotions</th>\n","      <th>utterances</th>\n","      <th>triggers</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>utterance_0</td>\n","      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n","      <td>[neutral, neutral, neutral, neutral, surprise]</td>\n","      <td>[also I was the point person on my company's t...</td>\n","      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>utterance_1</td>\n","      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n","      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n","      <td>[also I was the point person on my company's t...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>utterance_2</td>\n","      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n","      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n","      <td>[also I was the point person on my company's t...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>utterance_3</td>\n","      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n","      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n","      <td>[also I was the point person on my company's t...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>utterance_4</td>\n","      <td>[Joey, Rachel, Joey, Rachel]</td>\n","      <td>[surprise, sadness, surprise, fear]</td>\n","      <td>[But then who? The waitress I went out with la...</td>\n","      <td>[0.0, 0.0, 1.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3995</th>\n","      <td>utterance_3995</td>\n","      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n","      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n","      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3996</th>\n","      <td>utterance_3996</td>\n","      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n","      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n","      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3997</th>\n","      <td>utterance_3997</td>\n","      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n","      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n","      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3998</th>\n","      <td>utterance_3998</td>\n","      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n","      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n","      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3999</th>\n","      <td>utterance_3999</td>\n","      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n","      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n","      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4000 rows × 5 columns</p>\n","</div>"],"text/plain":["             episode                                           speakers  \\\n","0        utterance_0  [Chandler, The Interviewer, Chandler, The Inte...   \n","1        utterance_1  [Chandler, The Interviewer, Chandler, The Inte...   \n","2        utterance_2  [Chandler, The Interviewer, Chandler, The Inte...   \n","3        utterance_3  [Chandler, The Interviewer, Chandler, The Inte...   \n","4        utterance_4                       [Joey, Rachel, Joey, Rachel]   \n","...              ...                                                ...   \n","3995  utterance_3995  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n","3996  utterance_3996  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n","3997  utterance_3997  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n","3998  utterance_3998  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n","3999  utterance_3999  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n","\n","                                               emotions  \\\n","0        [neutral, neutral, neutral, neutral, surprise]   \n","1     [neutral, neutral, neutral, neutral, surprise,...   \n","2     [neutral, neutral, neutral, neutral, surprise,...   \n","3     [neutral, neutral, neutral, neutral, surprise,...   \n","4                   [surprise, sadness, surprise, fear]   \n","...                                                 ...   \n","3995  [neutral, joy, neutral, neutral, surprise, dis...   \n","3996  [neutral, joy, neutral, neutral, surprise, dis...   \n","3997  [neutral, joy, neutral, neutral, surprise, dis...   \n","3998  [neutral, joy, neutral, neutral, surprise, dis...   \n","3999  [neutral, joy, neutral, neutral, surprise, dis...   \n","\n","                                             utterances  \\\n","0     [also I was the point person on my company's t...   \n","1     [also I was the point person on my company's t...   \n","2     [also I was the point person on my company's t...   \n","3     [also I was the point person on my company's t...   \n","4     [But then who? The waitress I went out with la...   \n","...                                                 ...   \n","3995  [Hey., Hey!, So how was Joan?, I broke up with...   \n","3996  [Hey., Hey!, So how was Joan?, I broke up with...   \n","3997  [Hey., Hey!, So how was Joan?, I broke up with...   \n","3998  [Hey., Hey!, So how was Joan?, I broke up with...   \n","3999  [Hey., Hey!, So how was Joan?, I broke up with...   \n","\n","                                               triggers  \n","0                             [0.0, 0.0, 0.0, 1.0, 0.0]  \n","1                   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n","2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n","3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","4                                  [0.0, 0.0, 1.0, 0.0]  \n","...                                                 ...  \n","3995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","3996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","3997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","3998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","3999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","\n","[4000 rows x 5 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train_df = pd.DataFrame(train_csv)\n","train_df"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.467119Z","iopub.status.busy":"2023-11-14T16:06:29.466837Z","iopub.status.idle":"2023-11-14T16:06:29.475801Z","shell.execute_reply":"2023-11-14T16:06:29.474939Z","shell.execute_reply.started":"2023-11-14T16:06:29.467097Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['neutral', 'neutral', 'neutral', 'neutral', 'surprise'] ['Chandler', 'The Interviewer', 'Chandler', 'The Interviewer', 'Chandler']\n"]}],"source":["print(train_df['emotions'][0], train_df['speakers'][0])"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.477471Z","iopub.status.busy":"2023-11-14T16:06:29.477156Z","iopub.status.idle":"2023-11-14T16:06:29.495667Z","shell.execute_reply":"2023-11-14T16:06:29.494870Z","shell.execute_reply.started":"2023-11-14T16:06:29.477441Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0     True\n","1    False\n","2    False\n","3    False\n","4    False\n","5    False\n","6    False\n","Name: anger, dtype: bool"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["dummies = pd.get_dummies(EMOTIONS)\n","dummies['anger']"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.497013Z","iopub.status.busy":"2023-11-14T16:06:29.496741Z","iopub.status.idle":"2023-11-14T16:06:29.672035Z","shell.execute_reply":"2023-11-14T16:06:29.671283Z","shell.execute_reply.started":"2023-11-14T16:06:29.496987Z"},"trusted":true},"outputs":[],"source":["listaEmo = []\n","for i in train_df['emotions']:\n","    listtemp = []\n","    for j in i:\n","        listtemp.append(dummies[j])\n","    listaEmo.append(listtemp)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>1st Customer</th>\n","      <th>2nd Customer</th>\n","      <th>3rd Customer</th>\n","      <th>A Female Student</th>\n","      <th>A Student</th>\n","      <th>Alice</th>\n","      <th>All</th>\n","      <th>Allesandro</th>\n","      <th>Angela</th>\n","      <th>Annabelle</th>\n","      <th>...</th>\n","      <th>Tour Guide</th>\n","      <th>Trudie Styler</th>\n","      <th>Ursula</th>\n","      <th>Voice</th>\n","      <th>Waiter</th>\n","      <th>Wayne</th>\n","      <th>Woman</th>\n","      <th>Woman On Train</th>\n","      <th>Young Ethan</th>\n","      <th>an</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>226</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>227</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>228</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>229</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>230</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>231 rows × 231 columns</p>\n","</div>"],"text/plain":["     1st Customer  2nd Customer  3rd Customer  A Female Student  A Student  \\\n","0            True         False         False             False      False   \n","1           False          True         False             False      False   \n","2           False         False          True             False      False   \n","3           False         False         False              True      False   \n","4           False         False         False             False       True   \n","..            ...           ...           ...               ...        ...   \n","226         False         False         False             False      False   \n","227         False         False         False             False      False   \n","228         False         False         False             False      False   \n","229         False         False         False             False      False   \n","230         False         False         False             False      False   \n","\n","     Alice    All  Allesandro  Angela  Annabelle  ...  Tour Guide  \\\n","0    False  False       False   False      False  ...       False   \n","1    False  False       False   False      False  ...       False   \n","2    False  False       False   False      False  ...       False   \n","3    False  False       False   False      False  ...       False   \n","4    False  False       False   False      False  ...       False   \n","..     ...    ...         ...     ...        ...  ...         ...   \n","226  False  False       False   False      False  ...       False   \n","227  False  False       False   False      False  ...       False   \n","228  False  False       False   False      False  ...       False   \n","229  False  False       False   False      False  ...       False   \n","230  False  False       False   False      False  ...       False   \n","\n","     Trudie Styler  Ursula  Voice  Waiter  Wayne  Woman  Woman On Train  \\\n","0            False   False  False   False  False  False           False   \n","1            False   False  False   False  False  False           False   \n","2            False   False  False   False  False  False           False   \n","3            False   False  False   False  False  False           False   \n","4            False   False  False   False  False  False           False   \n","..             ...     ...    ...     ...    ...    ...             ...   \n","226          False   False  False   False   True  False           False   \n","227          False   False  False   False  False   True           False   \n","228          False   False  False   False  False  False            True   \n","229          False   False  False   False  False  False           False   \n","230          False   False  False   False  False  False           False   \n","\n","     Young Ethan     an  \n","0          False  False  \n","1          False  False  \n","2          False  False  \n","3          False  False  \n","4          False  False  \n","..           ...    ...  \n","226        False  False  \n","227        False  False  \n","228        False  False  \n","229         True  False  \n","230        False   True  \n","\n","[231 rows x 231 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["listSpk = []\n","for i in train_df['speakers']:\n","    for j in i:\n","        if j in listSpk:\n","            continue\n","        else:\n","            listSpk.append(j)\n","listSpk.sort()\n","speaker_specific = pd.get_dummies(listSpk)\n","speaker_specific"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# listasp = []\n","# for i in train_df['speakers']:\n","#     listatemp = []\n","#     for j in i:\n","#         listatemp.append(speaker_specific[j])\n","#     listasp.append(listatemp)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# train_df['speakers'] = listasp\n","# train_df"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.802230Z","iopub.status.busy":"2023-11-14T16:06:29.801452Z","iopub.status.idle":"2023-11-14T16:07:18.894222Z","shell.execute_reply":"2023-11-14T16:07:18.893211Z","shell.execute_reply.started":"2023-11-14T16:06:29.802200Z"},"trusted":true},"outputs":[],"source":["# i = 0\n","# sentence_embeddings = []\n","#     # sent_emb = model.encode('')\n","# while i < len(train_df):\n","#     utt = train_df['utterances'][i]\n","#     encodings = roberta_tokenizer.encode_plus(utt, max_length=100, padding = 'max_length', add_special_tokens=True, return_token_type_ids=True, return_attention_mask=True, truncation=True, return_tensors='pt').to(device)\n","#     utt_emb = roberta_finetuned(encodings['input_ids'], encodings['attention_mask'])[1].detach().tolist()[0]\n","#     utt_emb = np.round(utt_emb, decimals = 10)\n","#     # utt_emb = model.encode(utt)\n","#     sent_emb = utt_emb\n","#     i += 1\n","#     sentence_embeddings.append(copy.deepcopy(sent_emb))\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:07:18.900062Z","iopub.status.busy":"2023-11-14T16:07:18.899761Z","iopub.status.idle":"2023-11-14T16:07:20.219604Z","shell.execute_reply":"2023-11-14T16:07:20.218823Z","shell.execute_reply.started":"2023-11-14T16:07:18.900037Z"},"trusted":true},"outputs":[],"source":["# train_df['sentence_embeddings'] = sentence_embeddings\n","# df_sent = pd.DataFrame(sentence_embeddings)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:07:20.221010Z","iopub.status.busy":"2023-11-14T16:07:20.220720Z","iopub.status.idle":"2023-11-14T16:07:20.257434Z","shell.execute_reply":"2023-11-14T16:07:20.256631Z","shell.execute_reply.started":"2023-11-14T16:07:20.220985Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["           Valence  Arousal  Dominance\n","aaaaaaah     0.479    0.606      0.291\n","aaaah        0.520    0.636      0.282\n","aardvark     0.427    0.490      0.437\n","aback        0.385    0.407      0.288\n","abacus       0.510    0.276      0.485\n","...            ...      ...        ...\n","zoo          0.760    0.520      0.580\n","zoological   0.667    0.458      0.492\n","zoology      0.568    0.347      0.509\n","zoom         0.490    0.520      0.462\n","zucchini     0.510    0.321      0.250\n","\n","[19971 rows x 3 columns]\n"]}],"source":["csvread = pd.read_csv(\"./EDiReF-Train-Data/Task 3/out.csv\",names=[\"Valence\", \"Arousal\", \"Dominance\"])\n","print(csvread)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:07:20.259187Z","iopub.status.busy":"2023-11-14T16:07:20.258489Z","iopub.status.idle":"2023-11-14T16:10:25.727400Z","shell.execute_reply":"2023-11-14T16:10:25.726609Z","shell.execute_reply.started":"2023-11-14T16:07:20.259163Z"},"trusted":true},"outputs":[],"source":["import re\n","from collections import defaultdict\n","\n","track = defaultdict(list)\n","\n","for i in train_df['utterances']:\n","    for sentence in i:\n","        sentence = sentence.lower().split()\n","        for word in sentence:\n","            cleaned_word = re.sub(r'[^a-zA-Z]', '', word)\n","            if cleaned_word in csvread.index and cleaned_word not in track:\n","                track[cleaned_word].append(csvread['Valence'][cleaned_word])\n","                track[cleaned_word].append(csvread['Arousal'][cleaned_word])\n","                track[cleaned_word].append(csvread['Dominance'][cleaned_word])\n","                \n","\n","# Ahora, track contendrá las palabras limpias como claves y listas de diccionarios como valores, \n","# donde cada diccionario contiene las propiedades Valence, Arousal y Dominance para esa palabra.\n"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:10:25.811361Z","iopub.status.busy":"2023-11-14T16:10:25.811026Z","iopub.status.idle":"2023-11-14T16:10:26.829379Z","shell.execute_reply":"2023-11-14T16:10:26.828591Z","shell.execute_reply.started":"2023-11-14T16:10:25.811331Z"},"trusted":true},"outputs":[],"source":["valen = []\n","aros = []\n","domi = []\n","for i in train_df['utterances']:\n","    listVal = []\n","    listAro = []\n","    listDom = []\n","    for sentence in i:\n","        valence_sen = []\n","        arousal_sen = []\n","        dominance_sen = []\n","        sentence = sentence.lower().split()\n","        for word in sentence:\n","            cleaned_word = re.sub(r'[^a-zA-Z]', '', word)\n","            if cleaned_word in track:\n","                val, aro, dom = track[cleaned_word]\n","                valence_sen.append(float(val))\n","                arousal_sen.append(float(aro))\n","                dominance_sen.append(float(dom))\n","            else:\n","                valence_sen.append(0)\n","                arousal_sen.append(0)\n","                dominance_sen.append(0)\n","        listVal.append(valence_sen)\n","        listAro.append(arousal_sen)\n","        listDom.append(dominance_sen)\n","    valen.append(listVal)\n","    aros.append(listAro)\n","    domi.append(listDom)"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:10:26.843222Z","iopub.status.busy":"2023-11-14T16:10:26.842944Z","iopub.status.idle":"2023-11-14T16:10:27.342890Z","shell.execute_reply":"2023-11-14T16:10:27.341959Z","shell.execute_reply.started":"2023-11-14T16:10:26.843190Z"},"trusted":true},"outputs":[],"source":["meanT = []\n","for i in valen:\n","    mean = []\n","    for j in i:\n","        mean.append(np.mean(j))\n","    meanT.append(mean)"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:16:34.031493Z","iopub.status.busy":"2023-11-14T16:16:34.031157Z","iopub.status.idle":"2023-11-14T16:16:34.038656Z","shell.execute_reply":"2023-11-14T16:16:34.037717Z","shell.execute_reply.started":"2023-11-14T16:16:34.031468Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["4000 [[0], [0], [0, 0, 0, 0], [0, 0.245, 0, 0, 0], [0, 0.551, 0, 0, 0, 0, 0.802, 0.396, 0.449], [0, 0, 0.573, 0, 0, 0, 0, 0, 0, 0, 0], [0.529, 0, 0, 0, 0, 0, 0.573], [0, 0, 0, 0, 0.57, 0.438, 0, 0, 0.635, 0, 0.667], [0, 0, 0, 0.823, 0, 0, 0, 0, 0.06, 0, 0, 0, 0.357, 0.083, 0.519], [0.448, 0, 0.448, 0], [0, 0, 0.439, 0, 0.51, 0, 0, 0.542], [0, 0, 0.625, 0, 0, 0, 0.847, 0, 0, 0, 0, 0.806, 0, 0.49, 0, 0, 0, 0, 0.49, 0.958, 0.844, 0, 0, 0, 0, 0, 0, 0.811], [0, 0, 0, 0.594], [0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0.757, 0, 0], [0, 0, 0, 0, 0, 0, 0.74], [0]]\n"]},{"data":{"text/plain":["['Hey.',\n"," 'Hey!',\n"," 'So how was Joan?',\n"," 'I broke up with her.',\n"," \"Don't tell me, because of the big nostril thing?\",\n"," 'They were huge. When she sneezed, bats flew out of them.',\n"," 'Come on, they were not that huge.',\n"," \"I'm tellin' you, she leaned back; I could see her brain.\",\n"," 'How many perfectly fine women are you gonna reject over the most superficial insignificant things?',\n"," 'Hold it, hold it.',\n"," 'I gotta side with Chandler on this one.',\n"," \"When I first moved to the city, I went out a couple of times with this girl, really hot, great kisser, but she had the biggest Adam's apple.\",\n"," 'It made me nuts.',\n"," 'You or me?',\n"," \"I got it. Uh, Joey, women don't have Adam's apples.\",\n"," 'You guys are messing with me, right?',\n"," 'Yeah.']"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["print(len(valen),valen[3999])\n","\n","train_df['utterances'][3999]"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["valen_padded = [[subsublista + [0.0] * (MAX_DIALOGUE_LEN - len(subsublista)) for subsublista in sublista] for sublista in valen]"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["train_df = pickle.load(open('train_df.pkl', 'rb'))\n","df_sent = pickle.load(open('df_sent (1).pkl', 'rb'))"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["train_df['valence'] = valen_padded\n","train_df['sentence_embeddings'] = df_sent\n","train_df.drop(columns=['valence_speaker'], inplace=True)"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["pickle.dump(train_df,open('train_df.pkl', 'wb'))"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:10:27.363832Z","iopub.status.busy":"2023-11-14T16:10:27.362915Z","iopub.status.idle":"2023-11-14T16:10:27.375483Z","shell.execute_reply":"2023-11-14T16:10:27.374630Z","shell.execute_reply.started":"2023-11-14T16:10:27.363801Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([nan, nan,  5., nan, nan])"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["nanlist = np.full(len(meanT[0]), np.nan)\n","nanlist[2] = 5\n","nanlist"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T18:04:19.837008Z","iopub.status.busy":"2023-11-14T18:04:19.836150Z","iopub.status.idle":"2023-11-14T18:04:19.946830Z","shell.execute_reply":"2023-11-14T18:04:19.945961Z","shell.execute_reply.started":"2023-11-14T18:04:19.836975Z"},"trusted":true},"outputs":[],"source":["import copy\n","lista = []\n","\n","for i in range(len(train_df['speakers'])):\n","    diccionario = {}\n","    nanlist = np.full(len(meanT[i]), np.nan)\n","    \n","    for j, speaker in enumerate(train_df['speakers'][i]):\n","        if speaker in diccionario:\n","            diccionario[speaker][j] = meanT[i][j]\n","        else:\n","            nanlist_copy = copy.copy(nanlist)  # Crea una copia independiente de nanlist\n","            nanlist_copy[j] = meanT[i][j]\n","            diccionario[speaker] = nanlist_copy\n","\n","    lista.append(list(diccionario.values()))\n","\n"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T18:12:09.405386Z","iopub.status.busy":"2023-11-14T18:12:09.404664Z","iopub.status.idle":"2023-11-14T18:12:09.528413Z","shell.execute_reply":"2023-11-14T18:12:09.527705Z","shell.execute_reply.started":"2023-11-14T18:12:09.405353Z"},"trusted":true},"outputs":[],"source":["replaced_valance = [np.nan_to_num(x, nan=-1) for x in lista]\n","train_df['valence_speaker']=replaced_valance"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T18:12:45.518020Z","iopub.status.busy":"2023-11-14T18:12:45.517288Z","iopub.status.idle":"2023-11-14T18:12:45.525033Z","shell.execute_reply":"2023-11-14T18:12:45.524183Z","shell.execute_reply.started":"2023-11-14T18:12:45.517988Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([[ 0.19283333, -1.        ,  0.412875  , -1.        , -1.        ,\n","        -1.        , -1.        , -1.        , -1.        ,  0.14285714,\n","        -1.        ,  0.255     , -1.        , -1.        ],\n","       [-1.        ,  0.501125  , -1.        , -1.        , -1.        ,\n","         0.        , -1.        ,  0.170125  , -1.        , -1.        ,\n","         0.        , -1.        , -1.        ,  0.32129412],\n","       [-1.        , -1.        , -1.        ,  0.1855    , -1.        ,\n","        -1.        , -1.        , -1.        , -1.        , -1.        ,\n","        -1.        , -1.        , -1.        , -1.        ],\n","       [-1.        , -1.        , -1.        , -1.        ,  0.27875   ,\n","        -1.        ,  0.40166667, -1.        , -1.        , -1.        ,\n","        -1.        , -1.        , -1.        , -1.        ],\n","       [-1.        , -1.        , -1.        , -1.        , -1.        ,\n","        -1.        , -1.        , -1.        ,  0.4185    , -1.        ,\n","        -1.        , -1.        ,  0.2858    , -1.        ]])"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["train_df['valence_speaker'][174]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T20:26:27.018924Z","iopub.status.busy":"2023-11-14T20:26:27.018087Z","iopub.status.idle":"2023-11-14T20:26:27.127066Z","shell.execute_reply":"2023-11-14T20:26:27.126146Z","shell.execute_reply.started":"2023-11-14T20:26:27.018877Z"},"trusted":true},"outputs":[],"source":["train_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","i = 62\n","# Elegir un hablante específico y sus valores\n","speaker1_data = lista[i][0]\n","speaker2_data = lista[i][1]\n","# Crear una secuencia de índices para el eje x\n","x = train_df['triggers'][i]\n","\n","# Crear una gráfica de líneas\n","plt.scatter( [i for i,x in enumerate(speaker1_data)],speaker1_data)\n","plt.scatter( [i for i,x in enumerate(speaker2_data)],speaker2_data)\n","plt.scatter( [i for i,x in enumerate(x)],x)\n","plt.xlabel('Índice')\n","plt.ylabel('Valor')\n","plt.title('Gráfica de Líneas del Hablante')\n","\n","# Mostrar la gráfica\n","plt.show()\n","print(speaker1_data, speaker2_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2488965,"sourceId":4222789,"sourceType":"datasetVersion"},{"datasetId":3829061,"sourceId":6632753,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
