{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/sgarc/SemEval-2023-Task-10/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["tr version 4.35.2\n","Using device =>  cpu  torch  2.1.1+cu121\n"]}],"source":["import os\n","import transformers\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaTokenizer\n","import torch\n","from sklearn.preprocessing import MinMaxScaler\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.nn.functional as F\n","from torch.utils.tensorboard import SummaryWriter\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import confusion_matrix, classification_report\n","import tqdm\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","import pickle\n","import re\n","import copy\n","import pprint\n","import time\n","\n","MAX_NO_OF_SPEAKERS = 8\n","MAX_DIALOGUE_LEN   = 33\n","MAX_SEQUENCE_LEN   = 24\n","original_labels    = ['abuse', 'adoration', 'annoyance', 'awkwardness', 'benefit', 'boredom', 'calmness', 'challenge', 'cheer', 'confusion', 'curiosity', 'desire', 'excitement', 'guilt', 'horror', 'humour', 'impressed', 'loss', 'nervousness', 'nostalgia', 'pain', 'relief', 'satisfaction', 'scold', 'shock', 'sympathy', 'threat']\n","train_count        = [31, 190, 1051, 880, 220, 78, 752, 214, 534, 486, 545, 180, 867, 216, 280, 153, 257, 351, 398, 65, 36, 173, 136, 94, 372, 209, 263]\n","\n","EMOTIONS           = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n","\n","sent_model = 'roberta-base-nli-stsb-mean-tokens'\n","\n","print('tr version', transformers.__version__)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using device => \",device, ' torch ', torch.__version__)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:19.592349Z","iopub.status.busy":"2023-11-14T16:06:19.592071Z","iopub.status.idle":"2023-11-14T16:06:29.220772Z","shell.execute_reply":"2023-11-14T16:06:29.219872Z","shell.execute_reply.started":"2023-11-14T16:06:19.592324Z"},"trusted":true},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","model = SentenceTransformer('all-mpnet-base-v2')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.222235Z","iopub.status.busy":"2023-11-14T16:06:29.221960Z","iopub.status.idle":"2023-11-14T16:06:29.444188Z","shell.execute_reply":"2023-11-14T16:06:29.443408Z","shell.execute_reply.started":"2023-11-14T16:06:29.222211Z"},"trusted":true},"outputs":[],"source":["test_csv = pd.read_json(\"EDiReF-Test-Data/MELD_test_efr.json\")\n","train_csv = pd.read_json(\"EDiReF-Train-Data/Task 3/MELD_train_efr.json\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.460576Z","iopub.status.busy":"2023-11-14T16:06:29.460285Z","iopub.status.idle":"2023-11-14T16:06:29.465753Z","shell.execute_reply":"2023-11-14T16:06:29.464909Z","shell.execute_reply.started":"2023-11-14T16:06:29.460552Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>episode</th>\n","      <th>speakers</th>\n","      <th>emotions</th>\n","      <th>utterances</th>\n","      <th>triggers</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>utterance_0</td>\n","      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n","      <td>[neutral, neutral, neutral, neutral, surprise]</td>\n","      <td>[also I was the point person on my company's t...</td>\n","      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>utterance_1</td>\n","      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n","      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n","      <td>[also I was the point person on my company's t...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>utterance_2</td>\n","      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n","      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n","      <td>[also I was the point person on my company's t...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>utterance_3</td>\n","      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n","      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n","      <td>[also I was the point person on my company's t...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>utterance_4</td>\n","      <td>[Joey, Rachel, Joey, Rachel]</td>\n","      <td>[surprise, sadness, surprise, fear]</td>\n","      <td>[But then who? The waitress I went out with la...</td>\n","      <td>[0.0, 0.0, 1.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3995</th>\n","      <td>utterance_3995</td>\n","      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n","      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n","      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3996</th>\n","      <td>utterance_3996</td>\n","      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n","      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n","      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3997</th>\n","      <td>utterance_3997</td>\n","      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n","      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n","      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3998</th>\n","      <td>utterance_3998</td>\n","      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n","      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n","      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3999</th>\n","      <td>utterance_3999</td>\n","      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n","      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n","      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4000 rows × 5 columns</p>\n","</div>"],"text/plain":["             episode                                           speakers  \\\n","0        utterance_0  [Chandler, The Interviewer, Chandler, The Inte...   \n","1        utterance_1  [Chandler, The Interviewer, Chandler, The Inte...   \n","2        utterance_2  [Chandler, The Interviewer, Chandler, The Inte...   \n","3        utterance_3  [Chandler, The Interviewer, Chandler, The Inte...   \n","4        utterance_4                       [Joey, Rachel, Joey, Rachel]   \n","...              ...                                                ...   \n","3995  utterance_3995  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n","3996  utterance_3996  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n","3997  utterance_3997  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n","3998  utterance_3998  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n","3999  utterance_3999  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n","\n","                                               emotions  \\\n","0        [neutral, neutral, neutral, neutral, surprise]   \n","1     [neutral, neutral, neutral, neutral, surprise,...   \n","2     [neutral, neutral, neutral, neutral, surprise,...   \n","3     [neutral, neutral, neutral, neutral, surprise,...   \n","4                   [surprise, sadness, surprise, fear]   \n","...                                                 ...   \n","3995  [neutral, joy, neutral, neutral, surprise, dis...   \n","3996  [neutral, joy, neutral, neutral, surprise, dis...   \n","3997  [neutral, joy, neutral, neutral, surprise, dis...   \n","3998  [neutral, joy, neutral, neutral, surprise, dis...   \n","3999  [neutral, joy, neutral, neutral, surprise, dis...   \n","\n","                                             utterances  \\\n","0     [also I was the point person on my company's t...   \n","1     [also I was the point person on my company's t...   \n","2     [also I was the point person on my company's t...   \n","3     [also I was the point person on my company's t...   \n","4     [But then who? The waitress I went out with la...   \n","...                                                 ...   \n","3995  [Hey., Hey!, So how was Joan?, I broke up with...   \n","3996  [Hey., Hey!, So how was Joan?, I broke up with...   \n","3997  [Hey., Hey!, So how was Joan?, I broke up with...   \n","3998  [Hey., Hey!, So how was Joan?, I broke up with...   \n","3999  [Hey., Hey!, So how was Joan?, I broke up with...   \n","\n","                                               triggers  \n","0                             [0.0, 0.0, 0.0, 1.0, 0.0]  \n","1                   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n","2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n","3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","4                                  [0.0, 0.0, 1.0, 0.0]  \n","...                                                 ...  \n","3995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","3996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","3997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","3998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","3999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","\n","[4000 rows x 5 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_df = pd.DataFrame(train_csv)\n","train_df"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.477471Z","iopub.status.busy":"2023-11-14T16:06:29.477156Z","iopub.status.idle":"2023-11-14T16:06:29.495667Z","shell.execute_reply":"2023-11-14T16:06:29.494870Z","shell.execute_reply.started":"2023-11-14T16:06:29.477441Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0     True\n","1    False\n","2    False\n","3    False\n","4    False\n","5    False\n","6    False\n","Name: anger, dtype: bool"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["dummies = pd.get_dummies(EMOTIONS)\n","dummies['anger']"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.497013Z","iopub.status.busy":"2023-11-14T16:06:29.496741Z","iopub.status.idle":"2023-11-14T16:06:29.672035Z","shell.execute_reply":"2023-11-14T16:06:29.671283Z","shell.execute_reply.started":"2023-11-14T16:06:29.496987Z"},"trusted":true},"outputs":[],"source":["listaEmo = []\n","for i in train_df['emotions']:\n","    listtemp = []\n","    for j in i:\n","        listtemp.append(dummies[j])\n","    listaEmo.append(listtemp)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>episode</th>\n","      <th>speakers</th>\n","      <th>emotions</th>\n","      <th>utterances</th>\n","      <th>triggers</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>utterance_0</td>\n","      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n","      <td>[[False, False, False, False, True, False, Fal...</td>\n","      <td>[also I was the point person on my company's t...</td>\n","      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>utterance_1</td>\n","      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n","      <td>[[False, False, False, False, True, False, Fal...</td>\n","      <td>[also I was the point person on my company's t...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>utterance_2</td>\n","      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n","      <td>[[False, False, False, False, True, False, Fal...</td>\n","      <td>[also I was the point person on my company's t...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>utterance_3</td>\n","      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n","      <td>[[False, False, False, False, True, False, Fal...</td>\n","      <td>[also I was the point person on my company's t...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>utterance_4</td>\n","      <td>[Joey, Rachel, Joey, Rachel]</td>\n","      <td>[[False, False, False, False, False, False, Tr...</td>\n","      <td>[But then who? The waitress I went out with la...</td>\n","      <td>[0.0, 0.0, 1.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3995</th>\n","      <td>utterance_3995</td>\n","      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n","      <td>[[False, False, False, False, True, False, Fal...</td>\n","      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3996</th>\n","      <td>utterance_3996</td>\n","      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n","      <td>[[False, False, False, False, True, False, Fal...</td>\n","      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3997</th>\n","      <td>utterance_3997</td>\n","      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n","      <td>[[False, False, False, False, True, False, Fal...</td>\n","      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3998</th>\n","      <td>utterance_3998</td>\n","      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n","      <td>[[False, False, False, False, True, False, Fal...</td>\n","      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3999</th>\n","      <td>utterance_3999</td>\n","      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n","      <td>[[False, False, False, False, True, False, Fal...</td>\n","      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4000 rows × 5 columns</p>\n","</div>"],"text/plain":["             episode                                           speakers  \\\n","0        utterance_0  [Chandler, The Interviewer, Chandler, The Inte...   \n","1        utterance_1  [Chandler, The Interviewer, Chandler, The Inte...   \n","2        utterance_2  [Chandler, The Interviewer, Chandler, The Inte...   \n","3        utterance_3  [Chandler, The Interviewer, Chandler, The Inte...   \n","4        utterance_4                       [Joey, Rachel, Joey, Rachel]   \n","...              ...                                                ...   \n","3995  utterance_3995  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n","3996  utterance_3996  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n","3997  utterance_3997  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n","3998  utterance_3998  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n","3999  utterance_3999  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n","\n","                                               emotions  \\\n","0     [[False, False, False, False, True, False, Fal...   \n","1     [[False, False, False, False, True, False, Fal...   \n","2     [[False, False, False, False, True, False, Fal...   \n","3     [[False, False, False, False, True, False, Fal...   \n","4     [[False, False, False, False, False, False, Tr...   \n","...                                                 ...   \n","3995  [[False, False, False, False, True, False, Fal...   \n","3996  [[False, False, False, False, True, False, Fal...   \n","3997  [[False, False, False, False, True, False, Fal...   \n","3998  [[False, False, False, False, True, False, Fal...   \n","3999  [[False, False, False, False, True, False, Fal...   \n","\n","                                             utterances  \\\n","0     [also I was the point person on my company's t...   \n","1     [also I was the point person on my company's t...   \n","2     [also I was the point person on my company's t...   \n","3     [also I was the point person on my company's t...   \n","4     [But then who? The waitress I went out with la...   \n","...                                                 ...   \n","3995  [Hey., Hey!, So how was Joan?, I broke up with...   \n","3996  [Hey., Hey!, So how was Joan?, I broke up with...   \n","3997  [Hey., Hey!, So how was Joan?, I broke up with...   \n","3998  [Hey., Hey!, So how was Joan?, I broke up with...   \n","3999  [Hey., Hey!, So how was Joan?, I broke up with...   \n","\n","                                               triggers  \n","0                             [0.0, 0.0, 0.0, 1.0, 0.0]  \n","1                   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n","2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n","3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","4                                  [0.0, 0.0, 1.0, 0.0]  \n","...                                                 ...  \n","3995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","3996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","3997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","3998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","3999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","\n","[4000 rows x 5 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["train_df['emotions'] = listaEmo\n","train_df"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def speaker_transform(df: pd.DataFrame) -> list:\n","    listasp = []\n","    for conversation in df:\n","        listSpk = sorted(set(conversation))\n","        zejo = np.zeros((len(conversation),MAX_NO_OF_SPEAKERS))\n","        for i in range(len(conversation)):\n","            zejo[i][listSpk.index(conversation[i])] = 1\n","        # Crear un DataFrame de variables dummy para los speakers de esta conversación\n","        # Almacenar el DataFrame en la lista\n","        listasp.append(zejo)\n","    return listasp\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["train_df['speakers'] = speaker_transform(train_df['speakers'])"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.802230Z","iopub.status.busy":"2023-11-14T16:06:29.801452Z","iopub.status.idle":"2023-11-14T16:07:18.894222Z","shell.execute_reply":"2023-11-14T16:07:18.893211Z","shell.execute_reply.started":"2023-11-14T16:06:29.802200Z"},"trusted":true},"outputs":[],"source":["i = 0\n","sentence_embeddings = []\n","    # sent_emb = model.encode('')\n","if not os.path.exists('df_sent.pkl'):\n","    while i < len(train_df):\n","        utt = train_df['utterances'][i]\n","        utt_emb = model.encode(utt)\n","        # utt_emb = model.encode(utt)\n","        sent_emb = utt_emb\n","        i += 1\n","        sentence_embeddings.append(copy.deepcopy(sent_emb))\n","else:\n","    with open('df_sent.pkl', 'rb') as f:\n","        sentence_embeddings = pickle.load(f)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:07:18.900062Z","iopub.status.busy":"2023-11-14T16:07:18.899761Z","iopub.status.idle":"2023-11-14T16:07:20.219604Z","shell.execute_reply":"2023-11-14T16:07:20.218823Z","shell.execute_reply.started":"2023-11-14T16:07:18.900037Z"},"trusted":true},"outputs":[],"source":["train_df['sentence_embeddings'] = sentence_embeddings"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:07:20.221010Z","iopub.status.busy":"2023-11-14T16:07:20.220720Z","iopub.status.idle":"2023-11-14T16:07:20.257434Z","shell.execute_reply":"2023-11-14T16:07:20.256631Z","shell.execute_reply.started":"2023-11-14T16:07:20.220985Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["           Valence  Arousal  Dominance\n","aaaaaaah     0.479    0.606      0.291\n","aaaah        0.520    0.636      0.282\n","aardvark     0.427    0.490      0.437\n","aback        0.385    0.407      0.288\n","abacus       0.510    0.276      0.485\n","...            ...      ...        ...\n","zoo          0.760    0.520      0.580\n","zoological   0.667    0.458      0.492\n","zoology      0.568    0.347      0.509\n","zoom         0.490    0.520      0.462\n","zucchini     0.510    0.321      0.250\n","\n","[19971 rows x 3 columns]\n"]}],"source":["csvread = pd.read_csv(\"./EDiReF-Train-Data/Task 3/out.csv\",names=[\"Valence\", \"Arousal\", \"Dominance\"])\n","print(csvread)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:07:20.259187Z","iopub.status.busy":"2023-11-14T16:07:20.258489Z","iopub.status.idle":"2023-11-14T16:10:25.727400Z","shell.execute_reply":"2023-11-14T16:10:25.726609Z","shell.execute_reply.started":"2023-11-14T16:07:20.259163Z"},"trusted":true},"outputs":[],"source":["import re\n","from collections import defaultdict\n","\n","track = defaultdict(list)\n","\n","for i in train_df['utterances']:\n","    for sentence in i:\n","        sentence = sentence.lower().split()\n","        for word in sentence:\n","            cleaned_word = re.sub(r'[^a-zA-Z]', '', word)\n","            if cleaned_word in csvread.index and cleaned_word not in track:\n","                track[cleaned_word].append(csvread['Valence'][cleaned_word])\n","                #track[cleaned_word].append(csvread['Arousal'][cleaned_word])\n","                #track[cleaned_word].append(csvread['Dominance'][cleaned_word])\n","                \n","\n","# Ahora, track contendrá las palabras limpias como claves y listas de diccionarios como valores, \n","# donde cada diccionario contiene las propiedades Valence, Arousal y Dominance para esa palabra.\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["69"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["max_word_count = max(\n","    len(sentence.lower().split())\n","    for utterances in train_df['utterances']\n","    for sentence in utterances\n",")\n","max_word_count"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:10:25.811361Z","iopub.status.busy":"2023-11-14T16:10:25.811026Z","iopub.status.idle":"2023-11-14T16:10:26.829379Z","shell.execute_reply":"2023-11-14T16:10:26.828591Z","shell.execute_reply.started":"2023-11-14T16:10:25.811331Z"},"trusted":true},"outputs":[],"source":["valen = []\n","#aros = []\n","#domi = []\n","\n","for utterances in train_df['utterances']:\n","    valen.append([[float(track[re.sub(r'[^a-zA-Z]', '', word)][0]) if re.sub(r'[^a-zA-Z]', '', word) in track else 0 for word in sentence.lower().split()] for sentence in utterances])\n","    #aros.append([[float(track[re.sub(r'[^a-zA-Z]', '', word)][1]) if re.sub(r'[^a-zA-Z]', '', word) in track else 0 for word in sentence.lower().split()] for sentence in utterances])\n","    #domi.append([[float(track[re.sub(r'[^a-zA-Z]', '', word)][2]) if re.sub(r'[^a-zA-Z]', '', word) in track else 0 for word in sentence.lower().split()] for sentence in utterances])\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def pad_sequences_mean(sequences, max_len):\n","    padded_sequences = np.zeros((max_len))\n","    mask = np.ones((max_len))\n","\n","    for i, sentence in enumerate(sequences):\n","        padded_sequences[i] = sentence\n","        mask[i] = 0\n","    \n","    return padded_sequences, mask"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["padded_valen = [[pad_sequences_mean(sentence, 69)[0] for sentence in utterances] for utterances in valen]\n","mask_valen = [[pad_sequences_mean(sentence, 69)[1] for sentence in utterances] for utterances in valen]"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["#meanlist = [[[np.mean(l), np.mean(r), np.mean(o)] for l, r ,o in zip(v, a, d)] for v, a, d in zip(valen, aros, domi)]"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["#hstack_meanlist = [np.hstack((v, a, d)) for v, a, d in zip(temp_valen, temp_aros, temp_domi)]"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def pad_sequences(sequences: pd.DataFrame, num_features = 0, max_len = MAX_SEQUENCE_LEN , only_mask = False):\n","    if only_mask:\n","        mask = np.ones((max_len))\n","        mask[:len(sequences)] = 0\n","        return mask\n","    else:\n","        zejo = np.zeros((max_len, num_features)) if num_features != 0 else np.zeros((max_len))\n","        zejo[:len(sequences)] = sequences\n","        return zejo"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["padded_speakers = [pad_sequences(sequence, 8) for sequence in train_df['speakers']]\n","padded_emotions = [pad_sequences(sequence, 7) for sequence in train_df['emotions']]\n","padded_sentence_embeddings = [pad_sequences(sequence, 768) for sequence in train_df['sentence_embeddings']]\n","padded_trigger = [pad_sequences(sequence) for sequence in train_df['triggers']]\n","mask = [pad_sequences(sequence,only_mask=True) for sequence in train_df['sentence_embeddings']]"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["train_df['speakers'] = padded_speakers\n","train_df['emotions'] = padded_emotions\n","train_df['sentence_embeddings'] = padded_sentence_embeddings\n","train_df['valence'] = padded_valen\n","train_df['valence_mask'] = mask_valen\n","train_df['triggers'] = padded_trigger\n","train_df['mask'] = mask"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["#pickle.dump(train_df,open('test_df.pkl', 'wb'))"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["train_df['triggers'] = train_df['triggers'].apply(lambda x: [0 if pd.isna(item) else item for item in x])"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["pickle.dump(train_df,open('train_df.pkl', 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:10:27.363832Z","iopub.status.busy":"2023-11-14T16:10:27.362915Z","iopub.status.idle":"2023-11-14T16:10:27.375483Z","shell.execute_reply":"2023-11-14T16:10:27.374630Z","shell.execute_reply.started":"2023-11-14T16:10:27.363801Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([nan, nan,  5., nan, nan])"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["nanlist = np.full(len(meanT[0]), np.nan)\n","nanlist[2] = 5\n","nanlist"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T18:04:19.837008Z","iopub.status.busy":"2023-11-14T18:04:19.836150Z","iopub.status.idle":"2023-11-14T18:04:19.946830Z","shell.execute_reply":"2023-11-14T18:04:19.945961Z","shell.execute_reply.started":"2023-11-14T18:04:19.836975Z"},"trusted":true},"outputs":[],"source":["import copy\n","lista = []\n","\n","for i in range(len(train_df['speakers'])):\n","    diccionario = {}\n","    nanlist = np.full(len(meanT[i]), np.nan)\n","    \n","    for j, speaker in enumerate(train_df['speakers'][i]):\n","        if speaker in diccionario:\n","            diccionario[speaker][j] = meanT[i][j]\n","        else:\n","            nanlist_copy = copy.copy(nanlist)  # Crea una copia independiente de nanlist\n","            nanlist_copy[j] = meanT[i][j]\n","            diccionario[speaker] = nanlist_copy\n","\n","    lista.append(list(diccionario.values()))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T18:12:09.405386Z","iopub.status.busy":"2023-11-14T18:12:09.404664Z","iopub.status.idle":"2023-11-14T18:12:09.528413Z","shell.execute_reply":"2023-11-14T18:12:09.527705Z","shell.execute_reply.started":"2023-11-14T18:12:09.405353Z"},"trusted":true},"outputs":[],"source":["replaced_valance = [np.nan_to_num(x, nan=-1) for x in lista]\n","train_df['valence_speaker']=replaced_valance"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T18:12:45.518020Z","iopub.status.busy":"2023-11-14T18:12:45.517288Z","iopub.status.idle":"2023-11-14T18:12:45.525033Z","shell.execute_reply":"2023-11-14T18:12:45.524183Z","shell.execute_reply.started":"2023-11-14T18:12:45.517988Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([[ 0.19283333, -1.        ,  0.412875  , -1.        , -1.        ,\n","        -1.        , -1.        , -1.        , -1.        ,  0.14285714,\n","        -1.        ,  0.255     , -1.        , -1.        ],\n","       [-1.        ,  0.501125  , -1.        , -1.        , -1.        ,\n","         0.        , -1.        ,  0.170125  , -1.        , -1.        ,\n","         0.        , -1.        , -1.        ,  0.32129412],\n","       [-1.        , -1.        , -1.        ,  0.1855    , -1.        ,\n","        -1.        , -1.        , -1.        , -1.        , -1.        ,\n","        -1.        , -1.        , -1.        , -1.        ],\n","       [-1.        , -1.        , -1.        , -1.        ,  0.27875   ,\n","        -1.        ,  0.40166667, -1.        , -1.        , -1.        ,\n","        -1.        , -1.        , -1.        , -1.        ],\n","       [-1.        , -1.        , -1.        , -1.        , -1.        ,\n","        -1.        , -1.        , -1.        ,  0.4185    , -1.        ,\n","        -1.        , -1.        ,  0.2858    , -1.        ]])"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["train_df['valence_speaker'][174]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T20:26:27.018924Z","iopub.status.busy":"2023-11-14T20:26:27.018087Z","iopub.status.idle":"2023-11-14T20:26:27.127066Z","shell.execute_reply":"2023-11-14T20:26:27.126146Z","shell.execute_reply.started":"2023-11-14T20:26:27.018877Z"},"trusted":true},"outputs":[],"source":["train_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","i = 62\n","# Elegir un hablante específico y sus valores\n","speaker1_data = lista[i][0]\n","speaker2_data = lista[i][1]\n","# Crear una secuencia de índices para el eje x\n","x = train_df['triggers'][i]\n","\n","# Crear una gráfica de líneas\n","plt.scatter( [i for i,x in enumerate(speaker1_data)],speaker1_data)\n","plt.scatter( [i for i,x in enumerate(speaker2_data)],speaker2_data)\n","plt.scatter( [i for i,x in enumerate(x)],x)\n","plt.xlabel('Índice')\n","plt.ylabel('Valor')\n","plt.title('Gráfica de Líneas del Hablante')\n","\n","# Mostrar la gráfica\n","plt.show()\n","print(speaker1_data, speaker2_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2488965,"sourceId":4222789,"sourceType":"datasetVersion"},{"datasetId":3829061,"sourceId":6632753,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
