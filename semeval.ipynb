{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"46f7d28b-b3b2-4ae2-92ad-83cddfbcf8df","_uuid":"c1fe2705-c7fe-4328-a52a-aac80fffd541","collapsed":false,"execution":{"iopub.execute_input":"2023-11-14T16:06:06.641557Z","iopub.status.busy":"2023-11-14T16:06:06.641068Z","iopub.status.idle":"2023-11-14T16:06:19.590296Z","shell.execute_reply":"2023-11-14T16:06:19.589303Z","shell.execute_reply.started":"2023-11-14T16:06:06.641526Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'transformers'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m/home/sgarc/SemEval/semeval (1).ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/sgarc/SemEval/semeval%20%281%29.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/sgarc/SemEval/semeval%20%281%29.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/sgarc/SemEval/semeval%20%281%29.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaTokenizer\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/sgarc/SemEval/semeval%20%281%29.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"]}],"source":["import os\n","import transformers\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaTokenizer\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.nn.functional as F\n","from torch.utils.tensorboard import SummaryWriter\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import confusion_matrix, classification_report\n","import tqdm\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","import pickle\n","import re\n","import copy\n","import pprint\n","import time\n","\n","MAX_NO_OF_SPEAKERS = 8\n","MAX_DIALOGUE_LEN   = 33\n","original_labels    = ['abuse', 'adoration', 'annoyance', 'awkwardness', 'benefit', 'boredom', 'calmness', 'challenge', 'cheer', 'confusion', 'curiosity', 'desire', 'excitement', 'guilt', 'horror', 'humour', 'impressed', 'loss', 'nervousness', 'nostalgia', 'pain', 'relief', 'satisfaction', 'scold', 'shock', 'sympathy', 'threat']\n","train_count        = [31, 190, 1051, 880, 220, 78, 752, 214, 534, 486, 545, 180, 867, 216, 280, 153, 257, 351, 398, 65, 36, 173, 136, 94, 372, 209, 263]\n","\n","EMOTIONS           = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n","\n","sent_model = 'roberta-base-nli-stsb-mean-tokens'\n","\n","print('tr version', transformers.__version__)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using device => \",device, ' torch ', torch.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'transformers'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n","\u001b[1;32m/home/sgarc/SemEval/semeval (1).ipynb Cell 1\u001b[0m line \u001b[0;36m2\n","\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/sgarc/SemEval/semeval%20%281%29.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n","\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/sgarc/SemEval/semeval%20%281%29.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\n","\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/sgarc/SemEval/semeval%20%281%29.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaTokenizer\n","\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/sgarc/SemEval/semeval%20%281%29.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n","\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"]}],"source":["import os\n","import transformers\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaTokenizer\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.nn.functional as F\n","from torch.utils.tensorboard import SummaryWriter\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import confusion_matrix, classification_report\n","import tqdm\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","import pickle\n","import re\n","import copy\n","import pprint\n","import time\n","\n","MAX_NO_OF_SPEAKERS = 8\n","MAX_DIALOGUE_LEN   = 33\n","original_labels    = ['abuse', 'adoration', 'annoyance', 'awkwardness', 'benefit', 'boredom', 'calmness', 'challenge', 'cheer', 'confusion', 'curiosity', 'desire', 'excitement', 'guilt', 'horror', 'humour', 'impressed', 'loss', 'nervousness', 'nostalgia', 'pain', 'relief', 'satisfaction', 'scold', 'shock', 'sympathy', 'threat']\n","train_count        = [31, 190, 1051, 880, 220, 78, 752, 214, 534, 486, 545, 180, 867, 216, 280, 153, 257, 351, 398, 65, 36, 173, 136, 94, 372, 209, 263]\n","\n","EMOTIONS           = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n","\n","sent_model = 'roberta-base-nli-stsb-mean-tokens'\n","\n","print('tr version', transformers.__version__)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using device => \",device, ' torch ', torch.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:19.592349Z","iopub.status.busy":"2023-11-14T16:06:19.592071Z","iopub.status.idle":"2023-11-14T16:06:29.220772Z","shell.execute_reply":"2023-11-14T16:06:29.219872Z","shell.execute_reply.started":"2023-11-14T16:06:19.592324Z"},"trusted":true},"outputs":[],"source":["class EmotionClassifier(nn.Module):\n","    def __init__(self, n_classes):\n","        super(EmotionClassifier, self).__init__()\n","        self.bert = RobertaModel.from_pretrained('roberta-base')\n","        self.drop = nn.Dropout(p=0.3)\n","        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n","    def forward(self, input_ids, attention_mask):\n","        op = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n","        output = self.drop(op[1])\n","        return self.out(output), op[1]\n","\n","# load finetuned roberta model\n","roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","roberta_finetuned = EmotionClassifier(7).to(device)\n","#roberta_tf_checkpoint = torch.load('dump_files/finetuned/best_model_state_roberta.bin', map_location=torch.device(device))\n","#roberta_finetuned.load_state_dict(roberta_tf_checkpoint)\n","print('model loaded')\n","\n","\n","# Helper functions\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.222235Z","iopub.status.busy":"2023-11-14T16:06:29.221960Z","iopub.status.idle":"2023-11-14T16:06:29.444188Z","shell.execute_reply":"2023-11-14T16:06:29.443408Z","shell.execute_reply.started":"2023-11-14T16:06:29.222211Z"},"trusted":true},"outputs":[],"source":["train_csv = pd.read_json(\"Code/EDiReF-Train-Data/Task 3/MELD_train_efr.json\")\n","dev_csv = pd.read_json(\"Code/EDiReF-Train-Data/Task 3/MELD_train_efr.json\")\n","test_csv = pd.read_json(\"Code/EDiReF-Train-Data/Task 3/MELD_train_efr.json\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.445519Z","iopub.status.busy":"2023-11-14T16:06:29.445244Z","iopub.status.idle":"2023-11-14T16:06:29.456985Z","shell.execute_reply":"2023-11-14T16:06:29.455999Z","shell.execute_reply.started":"2023-11-14T16:06:29.445495Z"},"trusted":true},"outputs":[],"source":["train_csv['emotions'][0:20]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.460576Z","iopub.status.busy":"2023-11-14T16:06:29.460285Z","iopub.status.idle":"2023-11-14T16:06:29.465753Z","shell.execute_reply":"2023-11-14T16:06:29.464909Z","shell.execute_reply.started":"2023-11-14T16:06:29.460552Z"},"trusted":true},"outputs":[],"source":["train_df = pd.DataFrame(train_csv)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.467119Z","iopub.status.busy":"2023-11-14T16:06:29.466837Z","iopub.status.idle":"2023-11-14T16:06:29.475801Z","shell.execute_reply":"2023-11-14T16:06:29.474939Z","shell.execute_reply.started":"2023-11-14T16:06:29.467097Z"},"trusted":true},"outputs":[],"source":["print(train_df['emotions'][0], train_df['speakers'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.477471Z","iopub.status.busy":"2023-11-14T16:06:29.477156Z","iopub.status.idle":"2023-11-14T16:06:29.495667Z","shell.execute_reply":"2023-11-14T16:06:29.494870Z","shell.execute_reply.started":"2023-11-14T16:06:29.477441Z"},"trusted":true},"outputs":[],"source":["dummies = pd.get_dummies(EMOTIONS)\n","dummies['anger']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.497013Z","iopub.status.busy":"2023-11-14T16:06:29.496741Z","iopub.status.idle":"2023-11-14T16:06:29.672035Z","shell.execute_reply":"2023-11-14T16:06:29.671283Z","shell.execute_reply.started":"2023-11-14T16:06:29.496987Z"},"trusted":true},"outputs":[],"source":["listaEmo = []\n","for i in train_df['emotions']:\n","    listtemp = []\n","    for j in i:\n","        listtemp.append(dummies[j])\n","    listaEmo.append(listtemp)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.673857Z","iopub.status.busy":"2023-11-14T16:06:29.673246Z","iopub.status.idle":"2023-11-14T16:06:29.751500Z","shell.execute_reply":"2023-11-14T16:06:29.750624Z","shell.execute_reply.started":"2023-11-14T16:06:29.673824Z"},"trusted":true},"outputs":[],"source":["train_df['emotions'] = listaEmo\n","train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.753388Z","iopub.status.busy":"2023-11-14T16:06:29.752902Z","iopub.status.idle":"2023-11-14T16:06:29.761872Z","shell.execute_reply":"2023-11-14T16:06:29.760991Z","shell.execute_reply.started":"2023-11-14T16:06:29.753347Z"},"trusted":true},"outputs":[],"source":["train_df['speakers']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.763801Z","iopub.status.busy":"2023-11-14T16:06:29.763194Z","iopub.status.idle":"2023-11-14T16:06:29.789251Z","shell.execute_reply":"2023-11-14T16:06:29.788533Z","shell.execute_reply.started":"2023-11-14T16:06:29.763767Z"},"trusted":true},"outputs":[],"source":["listSpk = []\n","for i in train_df['speakers']:\n","    dic = {}\n","    tempskp = []\n","    count = 0\n","    for speaker in i:\n","        if speaker not in dic:\n","            count = count + 1\n","            dic[speaker] = count\n","        tempskp.append(dic[speaker])\n","    listSpk.append(tempskp)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.790397Z","iopub.status.busy":"2023-11-14T16:06:29.790174Z","iopub.status.idle":"2023-11-14T16:06:29.800345Z","shell.execute_reply":"2023-11-14T16:06:29.799472Z","shell.execute_reply.started":"2023-11-14T16:06:29.790377Z"},"trusted":true},"outputs":[],"source":["train_df['speakers'] = listSpk\n","train_df['speakers']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:06:29.802230Z","iopub.status.busy":"2023-11-14T16:06:29.801452Z","iopub.status.idle":"2023-11-14T16:07:18.894222Z","shell.execute_reply":"2023-11-14T16:07:18.893211Z","shell.execute_reply.started":"2023-11-14T16:06:29.802200Z"},"trusted":true},"outputs":[],"source":["i = 0\n","sentence_embeddings = []\n","    # sent_emb = model.encode('')\n","while i < len(train_df):\n","    utt = train_df['utterances'][i]\n","    encodings = roberta_tokenizer.encode_plus(utt, max_length=100, padding = 'max_length', add_special_tokens=True, return_token_type_ids=True, return_attention_mask=True, truncation=True, return_tensors='pt').to(device)\n","    utt_emb = roberta_finetuned(encodings['input_ids'], encodings['attention_mask'])[1].detach().tolist()[0]\n","    utt_emb = np.round(utt_emb, decimals = 10)\n","    # utt_emb = model.encode(utt)\n","    sent_emb = utt_emb\n","    i += 1\n","    sentence_embeddings.append(copy.deepcopy(sent_emb))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:07:18.900062Z","iopub.status.busy":"2023-11-14T16:07:18.899761Z","iopub.status.idle":"2023-11-14T16:07:20.219604Z","shell.execute_reply":"2023-11-14T16:07:20.218823Z","shell.execute_reply.started":"2023-11-14T16:07:18.900037Z"},"trusted":true},"outputs":[],"source":["train_df['sentence_embeddings'] = sentence_embeddings\n","df_sent = pd.DataFrame(sentence_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:07:20.221010Z","iopub.status.busy":"2023-11-14T16:07:20.220720Z","iopub.status.idle":"2023-11-14T16:07:20.257434Z","shell.execute_reply":"2023-11-14T16:07:20.256631Z","shell.execute_reply.started":"2023-11-14T16:07:20.220985Z"},"trusted":true},"outputs":[],"source":["csvread = pd.read_csv('/kaggle/input/djejeje/out.csv',names=[\"Valence\", \"Arousal\", \"Dominance\"])\n","print(csvread)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:07:20.259187Z","iopub.status.busy":"2023-11-14T16:07:20.258489Z","iopub.status.idle":"2023-11-14T16:10:25.727400Z","shell.execute_reply":"2023-11-14T16:10:25.726609Z","shell.execute_reply.started":"2023-11-14T16:07:20.259163Z"},"trusted":true},"outputs":[],"source":["import re\n","from collections import defaultdict\n","\n","track = defaultdict(list)\n","\n","for i in train_df['utterances']:\n","    for sentence in i:\n","        sentence = sentence.lower().split()\n","        for word in sentence:\n","            cleaned_word = re.sub(r'[^a-zA-Z]', '', word)\n","            if cleaned_word in csvread.index and cleaned_word not in track:\n","                track[cleaned_word].append(csvread['Valence'][cleaned_word])\n","                track[cleaned_word].append(csvread['Arousal'][cleaned_word])\n","                track[cleaned_word].append(csvread['Dominance'][cleaned_word])\n","                \n","\n","# Ahora, track contendrá las palabras limpias como claves y listas de diccionarios como valores, \n","# donde cada diccionario contiene las propiedades Valence, Arousal y Dominance para esa palabra.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["track"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:10:25.811361Z","iopub.status.busy":"2023-11-14T16:10:25.811026Z","iopub.status.idle":"2023-11-14T16:10:26.829379Z","shell.execute_reply":"2023-11-14T16:10:26.828591Z","shell.execute_reply.started":"2023-11-14T16:10:25.811331Z"},"trusted":true},"outputs":[],"source":["valen = []\n","aros = []\n","domi = []\n","for i in train_df['utterances']:\n","    listVal = []\n","    listAro = []\n","    listDom = []\n","    for sentence in i:\n","        valence_sen = []\n","        arousal_sen = []\n","        dominance_sen = []\n","        sentence = sentence.lower().split()\n","        for word in sentence:\n","            cleaned_word = re.sub(r'[^a-zA-Z]', '', word)\n","            if cleaned_word in track:\n","                val, aro, dom = track[cleaned_word]\n","                valence_sen.append(float(val))\n","                arousal_sen.append(float(aro))\n","                dominance_sen.append(float(dom))\n","            else:\n","                valence_sen.append(0)\n","                arousal_sen.append(0)\n","                dominance_sen.append(0)\n","        listVal.append(valence_sen)\n","        listAro.append(arousal_sen)\n","        listDom.append(dominance_sen)\n","    valen.append(listVal)\n","    aros.append(listAro)\n","    domi.append(listDom)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:10:26.831271Z","iopub.status.busy":"2023-11-14T16:10:26.830555Z","iopub.status.idle":"2023-11-14T16:10:26.841844Z","shell.execute_reply":"2023-11-14T16:10:26.840804Z","shell.execute_reply.started":"2023-11-14T16:10:26.831236Z"},"trusted":true},"outputs":[],"source":["npvalen = np.array(valen)\n","# N-Grams, sacar unigramas, bigramas, trigramas\n","# Sacar promedio de las valencias\n","npvalen[3999]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:10:26.843222Z","iopub.status.busy":"2023-11-14T16:10:26.842944Z","iopub.status.idle":"2023-11-14T16:10:27.342890Z","shell.execute_reply":"2023-11-14T16:10:27.341959Z","shell.execute_reply.started":"2023-11-14T16:10:26.843190Z"},"trusted":true},"outputs":[],"source":["meanT = []\n","for i in npvalen:\n","    mean = []\n","    for j in i:\n","        mean.append(np.mean(j))\n","    meanT.append(mean)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:16:34.031493Z","iopub.status.busy":"2023-11-14T16:16:34.031157Z","iopub.status.idle":"2023-11-14T16:16:34.038656Z","shell.execute_reply":"2023-11-14T16:16:34.037717Z","shell.execute_reply.started":"2023-11-14T16:16:34.031468Z"},"trusted":true},"outputs":[],"source":["print(npvalen[3999])\n","train_df['utterances'][3999]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T20:26:21.260686Z","iopub.status.busy":"2023-11-14T20:26:21.259816Z","iopub.status.idle":"2023-11-14T20:26:21.266129Z","shell.execute_reply":"2023-11-14T20:26:21.265120Z","shell.execute_reply.started":"2023-11-14T20:26:21.260653Z"},"trusted":true},"outputs":[],"source":["train_df['valence'] = meanT"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:10:27.363832Z","iopub.status.busy":"2023-11-14T16:10:27.362915Z","iopub.status.idle":"2023-11-14T16:10:27.375483Z","shell.execute_reply":"2023-11-14T16:10:27.374630Z","shell.execute_reply.started":"2023-11-14T16:10:27.363801Z"},"trusted":true},"outputs":[],"source":["nanlist = np.full(len(meanT[0]), np.nan)\n","nanlist[2] = 5\n","nanlist"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T18:04:19.837008Z","iopub.status.busy":"2023-11-14T18:04:19.836150Z","iopub.status.idle":"2023-11-14T18:04:19.946830Z","shell.execute_reply":"2023-11-14T18:04:19.945961Z","shell.execute_reply.started":"2023-11-14T18:04:19.836975Z"},"trusted":true},"outputs":[],"source":["import copy\n","lista = []\n","\n","for i in range(len(train_df['speakers'])):\n","    diccionario = {}\n","    nanlist = np.full(len(meanT[i]), np.nan)\n","    \n","    for j, speaker in enumerate(train_df['speakers'][i]):\n","        if speaker in diccionario:\n","            diccionario[speaker][j] = meanT[i][j]\n","        else:\n","            nanlist_copy = copy.copy(nanlist)  # Crea una copia independiente de nanlist\n","            nanlist_copy[j] = meanT[i][j]\n","            diccionario[speaker] = nanlist_copy\n","\n","    lista.append(list(diccionario.values()))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T18:12:09.405386Z","iopub.status.busy":"2023-11-14T18:12:09.404664Z","iopub.status.idle":"2023-11-14T18:12:09.528413Z","shell.execute_reply":"2023-11-14T18:12:09.527705Z","shell.execute_reply.started":"2023-11-14T18:12:09.405353Z"},"trusted":true},"outputs":[],"source":["replaced_valance = [np.nan_to_num(x, nan=-1) for x in lista]\n","train_df['valence_speaker']=replaced_valance"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T18:12:45.518020Z","iopub.status.busy":"2023-11-14T18:12:45.517288Z","iopub.status.idle":"2023-11-14T18:12:45.525033Z","shell.execute_reply":"2023-11-14T18:12:45.524183Z","shell.execute_reply.started":"2023-11-14T18:12:45.517988Z"},"trusted":true},"outputs":[],"source":["train_df['valence_speaker'][174]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T20:26:27.018924Z","iopub.status.busy":"2023-11-14T20:26:27.018087Z","iopub.status.idle":"2023-11-14T20:26:27.127066Z","shell.execute_reply":"2023-11-14T20:26:27.126146Z","shell.execute_reply.started":"2023-11-14T20:26:27.018877Z"},"trusted":true},"outputs":[],"source":["train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T16:10:27.654638Z","iopub.status.busy":"2023-11-14T16:10:27.654382Z","iopub.status.idle":"2023-11-14T16:10:27.973217Z","shell.execute_reply":"2023-11-14T16:10:27.972296Z","shell.execute_reply.started":"2023-11-14T16:10:27.654608Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","i = 62\n","# Elegir un hablante específico y sus valores\n","speaker1_data = list(lista[i].values())[0]\n","speaker2_data = list(lista[i].values())[1]\n","# Crear una secuencia de índices para el eje x\n","x = train_df['triggers'][i]\n","\n","# Crear una gráfica de líneas\n","plt.scatter( [i for i,x in enumerate(speaker1_data)],speaker1_data)\n","plt.scatter( [i for i,x in enumerate(speaker2_data)],speaker2_data)\n","plt.scatter( [i for i,x in enumerate(x)],x)\n","plt.xlabel('Índice')\n","plt.ylabel('Valor')\n","plt.title('Gráfica de Líneas del Hablante')\n","\n","# Mostrar la gráfica\n","plt.show()\n","print(speaker1_data, speaker2_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T20:27:31.411474Z","iopub.status.busy":"2023-11-14T20:27:31.411122Z","iopub.status.idle":"2023-11-14T20:27:31.419834Z","shell.execute_reply":"2023-11-14T20:27:31.418827Z","shell.execute_reply.started":"2023-11-14T20:27:31.411447Z"},"trusted":true},"outputs":[],"source":["class SemEvalDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","        self.len = len(self.data)\n","        print(list(train_df.columns))\n","        \n","    def __len__(self):\n","        return self.len\n","    \n","    def __getitem__(self, index):\n","        dict_x = {}\n","        dict_x['speaker'] = torch.tensor(self.data['speakers'][index], dtype=torch.int)\n","        dict_x['emotion'] = torch.tensor(self.data['emotions'][index], dtype=torch.int)\n","        dict_x['sentence_embeddings'] = torch.tensor(self.data['sentence_embeddings'][index], dtype=torch.float64)\n","        dict_x['valence'] = torch.tensor(self.data['valence'][index])\n","\n","        dict_y = {}\n","        dict_y['triggers'] =  torch.tensor(self.data['triggers'][index], dtype=torch.float32)\n","\n","        return dict_x, dict_y"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T20:27:34.333188Z","iopub.status.busy":"2023-11-14T20:27:34.332815Z","iopub.status.idle":"2023-11-14T20:27:34.338069Z","shell.execute_reply":"2023-11-14T20:27:34.337130Z","shell.execute_reply.started":"2023-11-14T20:27:34.333160Z"},"trusted":true},"outputs":[],"source":["train_dataset = SemEvalDataset(train_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T20:25:01.554296Z","iopub.status.busy":"2023-11-14T20:25:01.553658Z","iopub.status.idle":"2023-11-14T20:25:01.560760Z","shell.execute_reply":"2023-11-14T20:25:01.559810Z","shell.execute_reply.started":"2023-11-14T20:25:01.554264Z"},"trusted":true},"outputs":[],"source":["train_df['valence'][9]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T20:27:38.539944Z","iopub.status.busy":"2023-11-14T20:27:38.539550Z","iopub.status.idle":"2023-11-14T20:27:38.557326Z","shell.execute_reply":"2023-11-14T20:27:38.556432Z","shell.execute_reply.started":"2023-11-14T20:27:38.539912Z"},"trusted":true},"outputs":[],"source":["train_dataset.__getitem__(9)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T20:28:14.534054Z","iopub.status.busy":"2023-11-14T20:28:14.533303Z","iopub.status.idle":"2023-11-14T20:28:14.541711Z","shell.execute_reply":"2023-11-14T20:28:14.540738Z","shell.execute_reply.started":"2023-11-14T20:28:14.534021Z"},"trusted":true},"outputs":[],"source":["class MELDCollate:\n","    def __init__(self, pad_value = 0):\n","        self.pad_value = pad_value\n","    def __call__(self, batch):\n","        speaker             = pad_sequence([item[0]['speaker'] for item in batch], batch_first = True)\n","        emotion             = pad_sequence([item[0]['emotion'] for item in batch], batch_first = True)\n","        sentence_embeddings = pad_sequence([item[0]['sentence_embeddings'] for item in batch], batch_first = True)\n","        valence             = pad_sequence([item[0]['valence'] for item in batch], batch_first = True)\n","        # print('\\noriginal list : ',[item[0]['speaker'] for item in batch], '\\n\\npadded list : ', speaker)\n","        labels              = pad_sequence([item[1]['triggers'] for item in batch], batch_first = True)\n","\n","        dict_x = { 'speaker': speaker, 'emotion':emotion,  'sentence_embeddings':sentence_embeddings, 'valence':valence}\n","        dict_y = {'labels': labels}\n","\n","        return dict_x, dict_y"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T01:15:44.716112Z","iopub.status.busy":"2023-11-15T01:15:44.715450Z","iopub.status.idle":"2023-11-15T01:15:44.741037Z","shell.execute_reply":"2023-11-15T01:15:44.739996Z","shell.execute_reply.started":"2023-11-15T01:15:44.716079Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'DataLoader' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loader  \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(dataset \u001b[38;5;241m=\u001b[39m train_dataset, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39m MELDCollate())\n","\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"]}],"source":["train_loader  = DataLoader(dataset = train_dataset, batch_size = 64, shuffle=True, collate_fn= MELDCollate())"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T01:15:40.470064Z","iopub.status.busy":"2023-11-15T01:15:40.469677Z","iopub.status.idle":"2023-11-15T01:15:40.501160Z","shell.execute_reply":"2023-11-15T01:15:40.499947Z","shell.execute_reply.started":"2023-11-15T01:15:40.470038Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'train_loader' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m desired_batch_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_loader\u001b[49m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m desired_batch_index:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# 'batch' contendrá el batch en el índice especificado\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"]}],"source":["desired_batch_index = 6\n","for i, batch in enumerate(train_loader):\n","    if i == desired_batch_index:\n","        # 'batch' contendrá el batch en el índice especificado\n","        print(f\"Batch {i}:\")\n","        print(batch['emotions'])\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2488965,"sourceId":4222789,"sourceType":"datasetVersion"},{"datasetId":3829061,"sourceId":6632753,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
