{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4222789,"sourceType":"datasetVersion","datasetId":2488965},{"sourceId":6632753,"sourceType":"datasetVersion","datasetId":3829061}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n#import lightning as L\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\nimport tqdm\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport pickle\nimport re\nimport copy\nimport pprint\nimport time\n\nMAX_NO_OF_SPEAKERS = 8\nMAX_DIALOGUE_LEN   = 33\noriginal_labels    = ['abuse', 'adoration', 'annoyance', 'awkwardness', 'benefit', 'boredom', 'calmness', 'challenge', 'cheer', 'confusion', 'curiosity', 'desire', 'excitement', 'guilt', 'horror', 'humour', 'impressed', 'loss', 'nervousness', 'nostalgia', 'pain', 'relief', 'satisfaction', 'scold', 'shock', 'sympathy', 'threat']\ntrain_count        = [31, 190, 1051, 880, 220, 78, 752, 214, 534, 486, 545, 180, 867, 216, 280, 153, 257, 351, 398, 65, 36, 173, 136, 94, 372, 209, 263]\n\nEMOTIONS           = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n#OUTPUT_MASK        = pickle.load(open('dump_files/mask/top10_mask0_dict.pkl', 'rb'))['mask_tensor']\n\n#@title Import Files { display-mode: \"both\" }\nsent_model = 'roberta-base-nli-stsb-mean-tokens' #@param {type:\"string\"}\n\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n\nfrom torch import nn, optim\n\n\nprint('tr version', transformers.__version__)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device => \",device, ' torch ', torch.__version__)\n\n# for finetuned sentence embeddings\nfrom transformers import RobertaModel, RobertaTokenizer, AdamW, get_linear_schedule_with_warmup","metadata":{"_uuid":"c1fe2705-c7fe-4328-a52a-aac80fffd541","_cell_guid":"46f7d28b-b3b2-4ae2-92ad-83cddfbcf8df","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-14T16:06:06.641068Z","iopub.execute_input":"2023-11-14T16:06:06.641557Z","iopub.status.idle":"2023-11-14T16:06:19.590296Z","shell.execute_reply.started":"2023-11-14T16:06:06.641526Z","shell.execute_reply":"2023-11-14T16:06:19.589303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EmotionClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(EmotionClassifier, self).__init__()\n        self.bert = RobertaModel.from_pretrained('roberta-base')\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n    def forward(self, input_ids, attention_mask):\n        op = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n        output = self.drop(op[1])\n        return self.out(output), op[1]\n\n# load finetuned roberta model\nroberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nroberta_finetuned = EmotionClassifier(7).to(device)\n#roberta_tf_checkpoint = torch.load('dump_files/finetuned/best_model_state_roberta.bin', map_location=torch.device(device))\n#roberta_finetuned.load_state_dict(roberta_tf_checkpoint)\nprint('model loaded')\n\n\n# Helper functions\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:06:19.592071Z","iopub.execute_input":"2023-11-14T16:06:19.592349Z","iopub.status.idle":"2023-11-14T16:06:29.220772Z","shell.execute_reply.started":"2023-11-14T16:06:19.592324Z","shell.execute_reply":"2023-11-14T16:06:29.219872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv = pd.read_json(\"Code/EDiReF-Train-Data/Task 3/MELD_train_efr.json\")\ndev_csv = pd.read_json(\"Code/EDiReF-Train-Data/Task 3/MELD_train_efr.json\")\ntest_csv = pd.read_json(\"Code/EDiReF-Train-Data/Task 3/MELD_train_efr.json\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:06:29.221960Z","iopub.execute_input":"2023-11-14T16:06:29.222235Z","iopub.status.idle":"2023-11-14T16:06:29.444188Z","shell.execute_reply.started":"2023-11-14T16:06:29.222211Z","shell.execute_reply":"2023-11-14T16:06:29.443408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv['emotions'][0:20]","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:06:29.445244Z","iopub.execute_input":"2023-11-14T16:06:29.445519Z","iopub.status.idle":"2023-11-14T16:06:29.456985Z","shell.execute_reply.started":"2023-11-14T16:06:29.445495Z","shell.execute_reply":"2023-11-14T16:06:29.455999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.DataFrame(train_csv)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:06:29.460285Z","iopub.execute_input":"2023-11-14T16:06:29.460576Z","iopub.status.idle":"2023-11-14T16:06:29.465753Z","shell.execute_reply.started":"2023-11-14T16:06:29.460552Z","shell.execute_reply":"2023-11-14T16:06:29.464909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df['emotions'][0], train_df['speakers'][0])","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:06:29.466837Z","iopub.execute_input":"2023-11-14T16:06:29.467119Z","iopub.status.idle":"2023-11-14T16:06:29.475801Z","shell.execute_reply.started":"2023-11-14T16:06:29.467097Z","shell.execute_reply":"2023-11-14T16:06:29.474939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummies = pd.get_dummies(EMOTIONS)\ndummies['anger']","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:06:29.477156Z","iopub.execute_input":"2023-11-14T16:06:29.477471Z","iopub.status.idle":"2023-11-14T16:06:29.495667Z","shell.execute_reply.started":"2023-11-14T16:06:29.477441Z","shell.execute_reply":"2023-11-14T16:06:29.494870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"listaEmo = []\nfor i in train_df['emotions']:\n    listtemp = []\n    for j in i:\n        listtemp.append(dummies[j])\n    listaEmo.append(listtemp)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:06:29.496741Z","iopub.execute_input":"2023-11-14T16:06:29.497013Z","iopub.status.idle":"2023-11-14T16:06:29.672035Z","shell.execute_reply.started":"2023-11-14T16:06:29.496987Z","shell.execute_reply":"2023-11-14T16:06:29.671283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['emotions'] = listaEmo\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:06:29.673246Z","iopub.execute_input":"2023-11-14T16:06:29.673857Z","iopub.status.idle":"2023-11-14T16:06:29.751500Z","shell.execute_reply.started":"2023-11-14T16:06:29.673824Z","shell.execute_reply":"2023-11-14T16:06:29.750624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['speakers']","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:06:29.752902Z","iopub.execute_input":"2023-11-14T16:06:29.753388Z","iopub.status.idle":"2023-11-14T16:06:29.761872Z","shell.execute_reply.started":"2023-11-14T16:06:29.753347Z","shell.execute_reply":"2023-11-14T16:06:29.760991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"listSpk = []\nfor i in train_df['speakers']:\n    dic = {}\n    tempskp = []\n    count = 0\n    for speaker in i:\n        if speaker not in dic:\n            count = count + 1\n            dic[speaker] = count\n        tempskp.append(dic[speaker])\n    listSpk.append(tempskp)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:06:29.763194Z","iopub.execute_input":"2023-11-14T16:06:29.763801Z","iopub.status.idle":"2023-11-14T16:06:29.789251Z","shell.execute_reply.started":"2023-11-14T16:06:29.763767Z","shell.execute_reply":"2023-11-14T16:06:29.788533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['speakers'] = listSpk\ntrain_df['speakers']","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:06:29.790174Z","iopub.execute_input":"2023-11-14T16:06:29.790397Z","iopub.status.idle":"2023-11-14T16:06:29.800345Z","shell.execute_reply.started":"2023-11-14T16:06:29.790377Z","shell.execute_reply":"2023-11-14T16:06:29.799472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nsentence_embeddings = []\n    # sent_emb = model.encode('')\nwhile i < len(train_df):\n    utt = train_df['utterances'][i]\n    encodings = roberta_tokenizer.encode_plus(utt, max_length=100, padding = 'max_length', add_special_tokens=True, return_token_type_ids=True, return_attention_mask=True, truncation=True, return_tensors='pt').to(device)\n    utt_emb = roberta_finetuned(encodings['input_ids'], encodings['attention_mask'])[1].detach().tolist()[0]\n    utt_emb = np.round(utt_emb, decimals = 10)\n    # utt_emb = model.encode(utt)\n    sent_emb = utt_emb\n    i += 1\n    sentence_embeddings.append(copy.deepcopy(sent_emb))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:06:29.801452Z","iopub.execute_input":"2023-11-14T16:06:29.802230Z","iopub.status.idle":"2023-11-14T16:07:18.894222Z","shell.execute_reply.started":"2023-11-14T16:06:29.802200Z","shell.execute_reply":"2023-11-14T16:07:18.893211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['sentence_embeddings'] = sentence_embeddings\ndf_sent = pd.DataFrame(sentence_embeddings)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:07:18.899761Z","iopub.execute_input":"2023-11-14T16:07:18.900062Z","iopub.status.idle":"2023-11-14T16:07:20.219604Z","shell.execute_reply.started":"2023-11-14T16:07:18.900037Z","shell.execute_reply":"2023-11-14T16:07:20.218823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csvread = pd.read_csv('/kaggle/input/djejeje/out.csv',names=[\"Valence\", \"Arousal\", \"Dominance\"])\nprint(csvread)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:07:20.220720Z","iopub.execute_input":"2023-11-14T16:07:20.221010Z","iopub.status.idle":"2023-11-14T16:07:20.257434Z","shell.execute_reply.started":"2023-11-14T16:07:20.220985Z","shell.execute_reply":"2023-11-14T16:07:20.256631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom collections import defaultdict\n\ntrack = defaultdict(list)\n\nfor i in train_df['utterances']:\n    for sentence in i:\n        sentence = sentence.lower().split()\n        for word in sentence:\n            cleaned_word = re.sub(r'[^a-zA-Z]', '', word)\n            if cleaned_word in csvread.index and cleaned_word not in track:\n                track[cleaned_word].append(csvread['Valence'][cleaned_word])\n                track[cleaned_word].append(csvread['Arousal'][cleaned_word])\n                track[cleaned_word].append(csvread['Dominance'][cleaned_word])\n                \n\n# Ahora, track contendrá las palabras limpias como claves y listas de diccionarios como valores, \n# donde cada diccionario contiene las propiedades Valence, Arousal y Dominance para esa palabra.\n","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:07:20.258489Z","iopub.execute_input":"2023-11-14T16:07:20.259187Z","iopub.status.idle":"2023-11-14T16:10:25.727400Z","shell.execute_reply.started":"2023-11-14T16:07:20.259163Z","shell.execute_reply":"2023-11-14T16:10:25.726609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"track","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valen = []\naros = []\ndomi = []\nfor i in train_df['utterances']:\n    listVal = []\n    listAro = []\n    listDom = []\n    for sentence in i:\n        valence_sen = []\n        arousal_sen = []\n        dominance_sen = []\n        sentence = sentence.lower().split()\n        for word in sentence:\n            cleaned_word = re.sub(r'[^a-zA-Z]', '', word)\n            if cleaned_word in track:\n                val, aro, dom = track[cleaned_word]\n                valence_sen.append(float(val))\n                arousal_sen.append(float(aro))\n                dominance_sen.append(float(dom))\n            else:\n                valence_sen.append(0)\n                arousal_sen.append(0)\n                dominance_sen.append(0)\n        listVal.append(valence_sen)\n        listAro.append(arousal_sen)\n        listDom.append(dominance_sen)\n    valen.append(listVal)\n    aros.append(listAro)\n    domi.append(listDom)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:25.811026Z","iopub.execute_input":"2023-11-14T16:10:25.811361Z","iopub.status.idle":"2023-11-14T16:10:26.829379Z","shell.execute_reply.started":"2023-11-14T16:10:25.811331Z","shell.execute_reply":"2023-11-14T16:10:26.828591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"npvalen = np.array(valen)\n# N-Grams, sacar unigramas, bigramas, trigramas\n# Sacar promedio de las valencias\nnpvalen[3999]","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:26.830555Z","iopub.execute_input":"2023-11-14T16:10:26.831271Z","iopub.status.idle":"2023-11-14T16:10:26.841844Z","shell.execute_reply.started":"2023-11-14T16:10:26.831236Z","shell.execute_reply":"2023-11-14T16:10:26.840804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meanT = []\nfor i in npvalen:\n    mean = []\n    for j in i:\n        mean.append(np.mean(j))\n    meanT.append(mean)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:26.842944Z","iopub.execute_input":"2023-11-14T16:10:26.843222Z","iopub.status.idle":"2023-11-14T16:10:27.342890Z","shell.execute_reply.started":"2023-11-14T16:10:26.843190Z","shell.execute_reply":"2023-11-14T16:10:27.341959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(npvalen[3999])\ntrain_df['utterances'][3999]","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:16:34.031157Z","iopub.execute_input":"2023-11-14T16:16:34.031493Z","iopub.status.idle":"2023-11-14T16:16:34.038656Z","shell.execute_reply.started":"2023-11-14T16:16:34.031468Z","shell.execute_reply":"2023-11-14T16:16:34.037717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['valence'] = meanT","metadata":{"execution":{"iopub.status.busy":"2023-11-14T20:26:21.259816Z","iopub.execute_input":"2023-11-14T20:26:21.260686Z","iopub.status.idle":"2023-11-14T20:26:21.266129Z","shell.execute_reply.started":"2023-11-14T20:26:21.260653Z","shell.execute_reply":"2023-11-14T20:26:21.265120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nanlist = np.full(len(meanT[0]), np.nan)\nnanlist[2] = 5\nnanlist","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:27.362915Z","iopub.execute_input":"2023-11-14T16:10:27.363832Z","iopub.status.idle":"2023-11-14T16:10:27.375483Z","shell.execute_reply.started":"2023-11-14T16:10:27.363801Z","shell.execute_reply":"2023-11-14T16:10:27.374630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\nlista = []\n\nfor i in range(len(train_df['speakers'])):\n    diccionario = {}\n    nanlist = np.full(len(meanT[i]), np.nan)\n    \n    for j, speaker in enumerate(train_df['speakers'][i]):\n        if speaker in diccionario:\n            diccionario[speaker][j] = meanT[i][j]\n        else:\n            nanlist_copy = copy.copy(nanlist)  # Crea una copia independiente de nanlist\n            nanlist_copy[j] = meanT[i][j]\n            diccionario[speaker] = nanlist_copy\n\n    lista.append(list(diccionario.values()))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-14T18:04:19.836150Z","iopub.execute_input":"2023-11-14T18:04:19.837008Z","iopub.status.idle":"2023-11-14T18:04:19.946830Z","shell.execute_reply.started":"2023-11-14T18:04:19.836975Z","shell.execute_reply":"2023-11-14T18:04:19.945961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"replaced_valance = [np.nan_to_num(x, nan=-1) for x in lista]\ntrain_df['valence_speaker']=replaced_valance","metadata":{"execution":{"iopub.status.busy":"2023-11-14T18:12:09.404664Z","iopub.execute_input":"2023-11-14T18:12:09.405386Z","iopub.status.idle":"2023-11-14T18:12:09.528413Z","shell.execute_reply.started":"2023-11-14T18:12:09.405353Z","shell.execute_reply":"2023-11-14T18:12:09.527705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['valence_speaker'][174]","metadata":{"execution":{"iopub.status.busy":"2023-11-14T18:12:45.517288Z","iopub.execute_input":"2023-11-14T18:12:45.518020Z","iopub.status.idle":"2023-11-14T18:12:45.525033Z","shell.execute_reply.started":"2023-11-14T18:12:45.517988Z","shell.execute_reply":"2023-11-14T18:12:45.524183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2023-11-14T20:26:27.018087Z","iopub.execute_input":"2023-11-14T20:26:27.018924Z","iopub.status.idle":"2023-11-14T20:26:27.127066Z","shell.execute_reply.started":"2023-11-14T20:26:27.018877Z","shell.execute_reply":"2023-11-14T20:26:27.126146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ni = 62\n# Elegir un hablante específico y sus valores\nspeaker1_data = list(lista[i].values())[0]\nspeaker2_data = list(lista[i].values())[1]\n# Crear una secuencia de índices para el eje x\nx = train_df['triggers'][i]\n\n# Crear una gráfica de líneas\nplt.scatter( [i for i,x in enumerate(speaker1_data)],speaker1_data)\nplt.scatter( [i for i,x in enumerate(speaker2_data)],speaker2_data)\nplt.scatter( [i for i,x in enumerate(x)],x)\nplt.xlabel('Índice')\nplt.ylabel('Valor')\nplt.title('Gráfica de Líneas del Hablante')\n\n# Mostrar la gráfica\nplt.show()\nprint(speaker1_data, speaker2_data)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:27.654382Z","iopub.execute_input":"2023-11-14T16:10:27.654638Z","iopub.status.idle":"2023-11-14T16:10:27.973217Z","shell.execute_reply.started":"2023-11-14T16:10:27.654608Z","shell.execute_reply":"2023-11-14T16:10:27.972296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SemEvalDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        self.len = len(self.data)\n        print(list(train_df.columns))\n        \n    def __len__(self):\n        return self.len\n    \n    def __getitem__(self, index):\n        dict_x = {}\n        dict_x['speaker'] = torch.tensor(self.data['speakers'][index], dtype=torch.int)\n        dict_x['emotion'] = torch.tensor(self.data['emotions'][index], dtype=torch.int)\n        dict_x['sentence_embeddings'] = torch.tensor(self.data['sentence_embeddings'][index], dtype=torch.float64)\n        dict_x['valence'] = torch.tensor(self.data['valence'][index])\n\n        dict_y = {}\n        dict_y['triggers'] =  torch.tensor(self.data['triggers'][index], dtype=torch.float32)\n\n        return dict_x, dict_y","metadata":{"execution":{"iopub.status.busy":"2023-11-14T20:27:31.411122Z","iopub.execute_input":"2023-11-14T20:27:31.411474Z","iopub.status.idle":"2023-11-14T20:27:31.419834Z","shell.execute_reply.started":"2023-11-14T20:27:31.411447Z","shell.execute_reply":"2023-11-14T20:27:31.418827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = SemEvalDataset(train_df)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T20:27:34.332815Z","iopub.execute_input":"2023-11-14T20:27:34.333188Z","iopub.status.idle":"2023-11-14T20:27:34.338069Z","shell.execute_reply.started":"2023-11-14T20:27:34.333160Z","shell.execute_reply":"2023-11-14T20:27:34.337130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['valence'][9]","metadata":{"execution":{"iopub.status.busy":"2023-11-14T20:25:01.553658Z","iopub.execute_input":"2023-11-14T20:25:01.554296Z","iopub.status.idle":"2023-11-14T20:25:01.560760Z","shell.execute_reply.started":"2023-11-14T20:25:01.554264Z","shell.execute_reply":"2023-11-14T20:25:01.559810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.__getitem__(9)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T20:27:38.539550Z","iopub.execute_input":"2023-11-14T20:27:38.539944Z","iopub.status.idle":"2023-11-14T20:27:38.557326Z","shell.execute_reply.started":"2023-11-14T20:27:38.539912Z","shell.execute_reply":"2023-11-14T20:27:38.556432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MELDCollate:\n    def __init__(self, pad_value = 0):\n        self.pad_value = pad_value\n    def __call__(self, batch):\n        speaker             = pad_sequence([item[0]['speaker'] for item in batch], batch_first = True)\n        emotion             = pad_sequence([item[0]['emotion'] for item in batch], batch_first = True)\n        sentence_embeddings = pad_sequence([item[0]['sentence_embeddings'] for item in batch], batch_first = True)\n        valence             = pad_sequence([item[0]['valence'] for item in batch], batch_first = True)\n        # print('\\noriginal list : ',[item[0]['speaker'] for item in batch], '\\n\\npadded list : ', speaker)\n        labels              = pad_sequence([item[1]['triggers'] for item in batch], batch_first = True)\n\n        dict_x = { 'speaker': speaker, 'emotion':emotion,  'sentence_embeddings':sentence_embeddings, 'valence':valence}\n        dict_y = {'labels': labels}\n\n        return dict_x, dict_y","metadata":{"execution":{"iopub.status.busy":"2023-11-14T20:28:14.533303Z","iopub.execute_input":"2023-11-14T20:28:14.534054Z","iopub.status.idle":"2023-11-14T20:28:14.541711Z","shell.execute_reply.started":"2023-11-14T20:28:14.534021Z","shell.execute_reply":"2023-11-14T20:28:14.540738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader  = DataLoader(dataset = train_dataset, batch_size = 64, shuffle=True, collate_fn= MELDCollate())","metadata":{"execution":{"iopub.status.busy":"2023-11-15T01:15:44.715450Z","iopub.execute_input":"2023-11-15T01:15:44.716112Z","iopub.status.idle":"2023-11-15T01:15:44.741037Z","shell.execute_reply.started":"2023-11-15T01:15:44.716079Z","shell.execute_reply":"2023-11-15T01:15:44.739996Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loader  \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(dataset \u001b[38;5;241m=\u001b[39m train_dataset, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39m MELDCollate())\n","\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"],"ename":"NameError","evalue":"name 'DataLoader' is not defined","output_type":"error"}]},{"cell_type":"code","source":"desired_batch_index = 6\nfor i, batch in enumerate(train_loader):\n    if i == desired_batch_index:\n        # 'batch' contendrá el batch en el índice especificado\n        print(f\"Batch {i}:\")\n        print(batch['emotions'])\n        break","metadata":{"execution":{"iopub.status.busy":"2023-11-15T01:15:40.469677Z","iopub.execute_input":"2023-11-15T01:15:40.470064Z","iopub.status.idle":"2023-11-15T01:15:40.501160Z","shell.execute_reply.started":"2023-11-15T01:15:40.470038Z","shell.execute_reply":"2023-11-15T01:15:40.499947Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m desired_batch_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_loader\u001b[49m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m desired_batch_index:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# 'batch' contendrá el batch en el índice especificado\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"],"ename":"NameError","evalue":"name 'train_loader' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}