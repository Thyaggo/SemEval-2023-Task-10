{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device =>  cpu  torch  2.1.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import lightning as L\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# Scoring\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device => \",device, ' torch ', torch.__version__)\n",
    "\n",
    "\n",
    "# hyper parameters\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "#@title Hyper Parameters { display-mode: \"both\" }\n",
    "\n",
    "EPOCHS             = 20\n",
    "MAX_NO_OF_SPEAKERS = 232\n",
    "MAX_DIALOGUE_LEN   = 33\n",
    "original_labels    = ['abuse', 'adoration', 'annoyance', 'awkwardness', 'benefit', 'boredom', 'calmness', 'challenge', 'cheer', 'confusion', 'curiosity', 'desire', 'excitement', 'guilt', 'horror', 'humour', 'impressed', 'loss', 'nervousness', 'nostalgia', 'pain', 'relief', 'satisfaction', 'scold', 'shock', 'sympathy', 'threat']\n",
    "train_count        = [31, 190, 1051, 880, 220, 78, 752, 214, 534, 486, 545, 180, 867, 216, 280, 153, 257, 351, 398, 65, 36, 173, 136, 94, 372, 209, 263]\n",
    "\n",
    "EMOTIONS           = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "# DataLoader Hyperparamaters\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Module 1 hyperparamaters(speaker_specific_emotion_sequence) : GRU n-n\n",
    "input_size_1  = 7\n",
    "hidden_size_1 = 10 \n",
    "num_layers_1  = 2 \n",
    "output_size_1 = 10\n",
    "\n",
    "\n",
    "# Module 2 hyperparamaters(utterance_context) : Transformer Enc\n",
    "input_size_2 = 768\n",
    "n_head_2     = 4\n",
    "dm_ff_2      = 2048\n",
    "dp_2         = 0.2\n",
    "num_layers_2 = 4 \n",
    "act_fn_2     = 'relu'\n",
    "\n",
    "# Module 3 hyperparamaters(speaker_context) : Transformer Enc\n",
    "input_size_3 = 232\n",
    "n_head_3     = 4\n",
    "dm_ff_3      = 2048\n",
    "dp_3         = 0.2\n",
    "num_layers_3 = 4 \n",
    "act_fn_3     = 'relu'\n",
    "\n",
    "# Module 4 hyperparamaters(global_emotion_sequence) : GRU\n",
    "input_size_4  = 7\n",
    "hidden_size_4 = 10 \n",
    "num_layers_4  = 2 \n",
    "output_size_4 = 7\n",
    "\n",
    "# Module 5 hyperparamaters(valence) : Transformer Enc\n",
    "input_size_3 = 1\n",
    "n_head_3     = 4\n",
    "dm_ff_3      = 2048\n",
    "dp_3         = 0.2\n",
    "num_layers_3 = 4 \n",
    "act_fn_3     = 'relu'\n",
    "\n",
    "# Module 6 hyperparamaters(speaker_specific_valence_sequence) : GRU\n",
    "input_size_4  = 1\n",
    "hidden_size_4 = 10 \n",
    "num_layers_4  = 2 \n",
    "output_size_4 = 10\n",
    "\n",
    "# Final Model Hyperparamerters:\n",
    "fc1_out = 800\n",
    "fc2_out = 800\n",
    "fc3_out = 400\n",
    "fc4_out = 100\n",
    "fc5_out = len(original_labels)\n",
    "\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_df.pkl', 'rb') as f:\n",
    "    train_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemEvalDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.len = len(self.data)\n",
    "        print(list(train_df.columns))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        dict_x = {}\n",
    "        dict_x['speaker'] = torch.tensor(self.data['speakers'][index], dtype=torch.int)\n",
    "        dict_x['emotion'] = torch.tensor(self.data['emotions'][index], dtype=torch.int)\n",
    "        dict_x['sentence_embeddings'] = torch.tensor(self.data['sentence_embeddings'][index], dtype=torch.float64)\n",
    "        dict_x['valence'] = torch.tensor(self.data['valence'][index])\n",
    "\n",
    "        dict_y = {}\n",
    "        dict_y['triggers'] =  torch.tensor(self.data['triggers'][index], dtype=torch.float32)\n",
    "\n",
    "        return dict_x, dict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['episode', 'speakers', 'emotions', 'utterances', 'triggers', 'sentence_embeddings', 'valence', 'valence_speaker']\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SemEvalDataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(i) for i in train_df['speakers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MELDCollate:\n",
    "    def __init__(self, pad_value = 0):\n",
    "        self.pad_value = pad_value\n",
    "    def __call__(self, batch):\n",
    "        speaker             = pad_sequence([item[0]['speaker'] for item in batch], batch_first = True)\n",
    "        emotion             = pad_sequence([item[0]['emotion'] for item in batch], batch_first = True)\n",
    "        sentence_embeddings = pad_sequence([item[0]['sentence_embeddings'] for item in batch], batch_first = True)\n",
    "        valence             = pad_sequence([item[0]['valence'] for item in batch], batch_first = True)\n",
    "        # print('\\noriginal list : ',[item[0]['speaker'] for item in batch], '\\n\\npadded list : ', speaker)\n",
    "        labels              = pad_sequence([item[1]['triggers'] for item in batch], batch_first = True)\n",
    "\n",
    "        dict_x = { 'speaker': speaker, 'emotion':emotion,  'sentence_embeddings':sentence_embeddings, 'valence':valence}\n",
    "        dict_y = {'labels': labels}\n",
    "\n",
    "        return dict_x, dict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader  = DataLoader(dataset = train_dataset, batch_size = 64, shuffle=True, collate_fn= MELDCollate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6:\n",
      "({'speaker': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.int32), 'emotion': tensor([[[0, 1, 0,  ..., 0, 0, 0],\n",
      "         [0, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 1],\n",
      "         [0, 0, 0,  ..., 0, 1, 0],\n",
      "         [0, 0, 0,  ..., 0, 1, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 1],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.int32), 'sentence_embeddings': tensor([[ 0.0128,  0.1396, -0.1457,  ...,  0.2445,  0.4455,  0.0840],\n",
      "        [ 0.0154,  0.1910, -0.1446,  ...,  0.2375,  0.4624,  0.0881],\n",
      "        [ 0.0281,  0.1540, -0.1443,  ...,  0.2614,  0.4542,  0.0734],\n",
      "        ...,\n",
      "        [ 0.0128,  0.1396, -0.1457,  ...,  0.2445,  0.4455,  0.0840],\n",
      "        [ 0.0148,  0.1619, -0.1450,  ...,  0.2485,  0.4606,  0.0839],\n",
      "        [ 0.0270,  0.1875, -0.1426,  ...,  0.2428,  0.4625,  0.0803]],\n",
      "       dtype=torch.float64), 'valence': tensor([[0.2346, 0.1061, 0.1770,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.3982, 0.0000, 0.2389,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1619, 0.1675, 0.1596,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.2488, 0.2645, 0.0713,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.3002, 0.0000, 0.4120,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1058, 0.1292, 0.1597,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       dtype=torch.float64)}, {'labels': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])})\n"
     ]
    }
   ],
   "source": [
    "desired_batch_index = 6\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i == desired_batch_index:\n",
    "        # 'batch' contendrá el batch en el índice especificado\n",
    "        print(f\"Batch {i}:\")\n",
    "        print(batch)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "tensor([0.2188, 0.1796, 0.3178, 0.1020, 0.4790, 0.4790, 0.2708, 0.1875, 0.4185,\n",
      "        0.2677], dtype=torch.float64)\n",
      "(tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32), tensor([0, 2, 0, 4, 1, 1, 0, 3, 3, 1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([0.2188, 0.0000, 0.3178, 0.0000, 0.0000, 0.0000, 0.2708, 0.0000, 0.0000,\n",
       "         0.0000], dtype=torch.float64),\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.4790, 0.4790, 0.0000, 0.0000, 0.0000,\n",
       "         0.2677], dtype=torch.float64),\n",
       " tensor([0.0000, 0.1796, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000], dtype=torch.float64),\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1875, 0.4185,\n",
       "         0.0000], dtype=torch.float64),\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.1020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000], dtype=torch.float64)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tesst = train_dataset.__getitem__(45)\n",
    "print(len(tesst[0]['speaker'].unique(dim=0)))\n",
    "print(tesst[0]['valence'])\n",
    "print(tesst[0]['speaker'].unique(dim = 0, return_inverse=True ))\n",
    "test = tesst[0]['speaker'].unique(dim = 0, return_inverse=True)[1]\n",
    "[torch.where(test == i , tesst[0]['valence'], 0) for i in test.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module6GRU(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, output_size):\n",
    "        super(Module6GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Since there are maximum of 8 speakers in a dialogue, so we decided to make 8 GRUs one for each speaker.\n",
    "        self.gru_list= []\n",
    "        for id in range(MAX_NO_OF_SPEAKERS):\n",
    "            self.gru_list.append(nn.GRU(input_size, hidden_size, num_layers, batch_first = True))\n",
    "        self.gru_modules = nn.ModuleList(self.gru_list)\n",
    "\n",
    "    def valence_specific(valence, speaker):\n",
    "        speaker = speaker.unique(dim = 0, return_inverse=True)[1]\n",
    "        return [torch.where(speaker == i , valence, 0) for i in speaker.unique()]\n",
    "\n",
    "    def applyGRU(self, speaker_specific, utt_id, seq_len):\n",
    "        speaker_output = torch.zeros(seq_len, self.output_size)  \n",
    "        for sp_idx in range(len(utt_id)):\n",
    "            h0 = torch.zeros(self.num_layers, 1, self.hidden_size).to(device)\n",
    "            out, hn = self.gru_list[sp_idx](speaker_specific[sp_idx], h0)\n",
    "            for uid in range(utt_id[sp_idx].size(0)):\n",
    "                speaker_output[utt_id[sp_idx][uid]] = out[0][uid].clone()\n",
    "        return speaker_output\n",
    "\n",
    "    def forward(self, x, speakers):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len    = x.size(1)\n",
    "        outputs = []\n",
    "        for i in range(batch_size):\n",
    "            speaker_specific, utt_id = self.segregateEmotions(x[i], speakers[i])\n",
    "            out = self.applyGRU(speaker_specific, utt_id, seq_len)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        final_output = torch.cat([outputs[i].unsqueeze(2) for i in range(len(outputs))], 2).permute(2,0,1)\n",
    "        \n",
    "        return final_output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module5TransformerEnc(nn.Module):\n",
    "    # S, N, E : (seq_len, batch_size, input/embedding_size)\n",
    "    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n",
    "        super(Module5TransformerEnc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src): # src_shape : (S, N, E)\n",
    "        pad_value = torch.zeros(self.input_size).to(device)\n",
    "        # pad_value shape : (E), value : [0,0,0, ...]\n",
    "        ##src = src.transpose(0,1)\n",
    "        # src_shape : (N, S, E)\n",
    "\n",
    "        src_mask = torch.all(torch.eq(src,pad_value),2)\n",
    "        \n",
    "        # src_mask shape : (N, S), value : for each batch, it is contains seq_len sized tensors and contains true for pad and false for others\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: seq_len, batch_size, input_size \n",
    "        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n",
    "        x_mask = self.make_src_mask(x)\n",
    "        out = self.encoder(x, src_key_padding_mask = x_mask)  \n",
    "        # out shape : (S, N, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module4GRU(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, output_size):\n",
    "        super(Module4GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc  = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, hn = self.gru(x, h0)\n",
    "        \n",
    "        # shape of out :  (N, seq_len, hidden_size)     (torch.Size([10, 33, 8])) \n",
    "        # shape of hn  :  (num_layers, N, hidden_size)     (torch.Size([2, 10, 8]))\n",
    "        hn = torch.flatten(hn.permute(1,0,2), start_dim=1)\n",
    "        # shape of hn  :  (N, num_layers, hidden_size) and then flatten it to (N, num_layers*hiddem_size) 3D to 2D\n",
    "        output = self.fc(out)\n",
    "        # shape of output : [N, output_size]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module3TransformerEnc(nn.Module):\n",
    "    # S, N, E : (seq_len, batch_size, input/embedding_size)\n",
    "    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n",
    "        super(Module3TransformerEnc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src): # src_shape : (S, N, E)\n",
    "        pad_value = torch.zeros(self.input_size).to(device)\n",
    "        # pad_value shape : (E), value : [0,0,0, ...]\n",
    "        ##src = src.transpose(0,1)\n",
    "        # src_shape : (N, S, E)\n",
    "\n",
    "        src_mask = torch.all(torch.eq(src,pad_value),2)\n",
    "        \n",
    "        # src_mask shape : (N, S), value : for each batch, it is contains seq_len sized tensors and contains true for pad and false for others\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: seq_len, batch_size, input_size \n",
    "        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n",
    "        x_mask = self.make_src_mask(x)\n",
    "        out = self.encoder(x, src_key_padding_mask = x_mask)  \n",
    "        # out shape : (S, N, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module2TransformerEnc(nn.Module):\n",
    "    # S, N, E : (seq_len, batch_size, input/embedding_size)\n",
    "    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n",
    "        super(Module2TransformerEnc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src): # src_shape : (S, N, E)\n",
    "        pad_value = torch.zeros(self.input_size).to(device)\n",
    "        # pad_value shape : (E), value : [0,0,0, ...]\n",
    "        ##src = src.transpose(0,1)\n",
    "        # src_shape : (N, S, E)\n",
    "\n",
    "        src_mask = torch.all(torch.eq(src,pad_value),2)\n",
    "        \n",
    "        # src_mask shape : (N, S), value : for each batch, it is contains seq_len sized tensors and contains true for pad and false for others\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: seq_len, batch_size, input_size \n",
    "        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n",
    "        x_mask = self.make_src_mask(x)\n",
    "        out = self.encoder(x, src_key_padding_mask = x_mask)  \n",
    "        # out shape : (S, N, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module1GRU(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, output_size):\n",
    "        super(Module1GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Since there are maximum of 8 speakers in a dialogue, so we decided to make 8 GRUs one for each speaker.\n",
    "        self.gru_list= []\n",
    "        for id in range(MAX_NO_OF_SPEAKERS):\n",
    "            self.gru_list.append(nn.GRU(input_size, hidden_size, num_layers, batch_first = True))\n",
    "        self.gru_modules = nn.ModuleList(self.gru_list)\n",
    "        # self.fc  = nn.Linear(num_layers*hidden_size, output_size)\n",
    "            \n",
    "    \n",
    "    def segregateEmotions(self, emotions, speakers):\n",
    "        speaker_specific = []\n",
    "        utt_id = []\n",
    "        for i in range(MAX_NO_OF_SPEAKERS):\n",
    "            speaker_tensor = torch.zeros(MAX_NO_OF_SPEAKERS, dtype = float).to(device)\n",
    "            speaker_tensor[i] = 1\n",
    "            emo = emotions[torch.nonzero((speakers == speaker_tensor).sum(dim=1) == speakers.size(1))].permute(1,0,2)\n",
    "            if(emo.size(1) == 0):\n",
    "                continue\n",
    "            utt_id.append(torch.nonzero((speakers == speaker_tensor).sum(dim=1) == speakers.size(1)).permute(1,0)[0])\n",
    "            speaker_specific.append(emo)\n",
    "#             print('\\n emo size : ',emo.size())\n",
    "#         print('\\n emo concat size : ',speaker_specific, utt_id)\n",
    "        return speaker_specific, utt_id\n",
    "    \n",
    "    def applyGRU(self, speaker_specific, utt_id, seq_len):\n",
    "        speaker_output = torch.zeros(seq_len, self.output_size)  \n",
    "        for sp_idx in range(len(utt_id)):\n",
    "            h0 = torch.zeros(self.num_layers, 1, self.hidden_size).to(device)\n",
    "            out, hn = self.gru_list[sp_idx](speaker_specific[sp_idx], h0)\n",
    "            for uid in range(utt_id[sp_idx].size(0)):\n",
    "                speaker_output[utt_id[sp_idx][uid]] = out[0][uid].clone()\n",
    "        return speaker_output\n",
    "\n",
    "    def forward(self, x, speakers):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len    = x.size(1)\n",
    "        outputs = []\n",
    "        for i in range(batch_size):\n",
    "            speaker_specific, utt_id = self.segregateEmotions(x[i], speakers[i])\n",
    "            out = self.applyGRU(speaker_specific, utt_id, seq_len)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        final_output = torch.cat([outputs[i].unsqueeze(2) for i in range(len(outputs))], 2).permute(2,0,1)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (1828343006.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    class FinalModel(L.Li)\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "class FinalModel(L.LightningModule):\n",
    "    def __init__(self, \n",
    "            input_size_1, hidden_size_1, num_layers_1, output_size_1,      # module 1    \n",
    "            input_size_2, n_head_2, dm_ff_2, dp_2, num_layers_2, act_fn_2, # module 2\n",
    "            input_size_3, n_head_3, dm_ff_3, dp_3, num_layers_3, act_fn_3, # module 3\n",
    "            input_size_4, hidden_size_4, num_layers_4, output_size_4,      # module 4\n",
    "            input_size_5, n_head_5, dm_ff_5, dp_5, num_layers_5, act_fn_5, # module 5\n",
    "            input_size_6, hidden_size_6, num_layers_6, output_size_6,      # module 6\n",
    "            fc1_out, fc2_out, fc3_out, fc4_out, fc5_out, dp, masking = False            # final Model parameters\n",
    "            ):\n",
    "        super(FinalModel, self).__init__()\n",
    "\n",
    "        self.masking = masking\n",
    "\n",
    "        self.module1 = Module1GRU(input_size = input_size_1, num_layers = num_layers_1, hidden_size = hidden_size_1, output_size = output_size_1)\n",
    "        self.module2 = Module2TransformerEnc(input_size = input_size_2, n_head = n_head_2, dim_ff = dm_ff_2, dp = dp_2, num_layers = num_layers_2, act_fn = act_fn_2)\n",
    "        self.module3 = Module3TransformerEnc(input_size = input_size_3, n_head = n_head_3, dim_ff = dm_ff_3, dp = dp_3, num_layers = num_layers_3, act_fn = act_fn_3)\n",
    "        self.module4 = Module4GRU(input_size = input_size_4, num_layers = num_layers_4, hidden_size = hidden_size_4, output_size = output_size_4)\n",
    "        self.module5 = Module5TransformerEnc(input_size = input_size_5, n_head = n_head_5, dim_ff = dm_ff_5, dp = dp_5, num_layers = num_layers_5, act_fn = act_fn_5)\n",
    "        self.module6 = Module6GRU(input_size = input_size_6, num_layers = num_layers_6, hidden_size = hidden_size_6, output_size = output_size_6)\n",
    "\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc1 = nn.Linear(input_size_2+input_size_3, fc1_out)\n",
    "        self.classification = nn.Sequential(\n",
    "                nn.Linear(2*(output_size_1 + fc1_out + output_size_4), fc2_out),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dp), \n",
    "                nn.Linear(fc2_out, fc3_out),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(fc3_out, fc4_out),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(fc4_out, fc5_out),\n",
    "                # nn.Sigmoid()\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
