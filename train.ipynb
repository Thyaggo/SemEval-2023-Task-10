{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgarc/SemEval/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device =>  cpu  torch  2.1.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import lightning as L\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# Scoring\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device => \",device, ' torch ', torch.__version__)\n",
    "\n",
    "\n",
    "# hyper parameters\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "#@title Hyper Parameters { display-mode: \"both\" }\n",
    "\n",
    "EPOCHS             = 20\n",
    "MAX_NO_OF_SPEAKERS = 231\n",
    "MAX_DIALOGUE_LEN   = 33\n",
    "original_labels    = ['abuse', 'adoration', 'annoyance', 'awkwardness', 'benefit', 'boredom', 'calmness', 'challenge', 'cheer', 'confusion', 'curiosity', 'desire', 'excitement', 'guilt', 'horror', 'humour', 'impressed', 'loss', 'nervousness', 'nostalgia', 'pain', 'relief', 'satisfaction', 'scold', 'shock', 'sympathy', 'threat']\n",
    "train_count        = [31, 190, 1051, 880, 220, 78, 752, 214, 534, 486, 545, 180, 867, 216, 280, 153, 257, 351, 398, 65, 36, 173, 136, 94, 372, 209, 263]\n",
    "\n",
    "EMOTIONS           = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "# DataLoader Hyperparamaters\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Module 1 hyperparamaters(speaker_specific_emotion_sequence) : GRU n-n\n",
    "input_size_1  = 7\n",
    "hidden_size_1 = 10 \n",
    "num_layers_1  = 2 \n",
    "output_size_1 = 10\n",
    "\n",
    "\n",
    "# Module 2 hyperparamaters(utterance_context) : Transformer Enc\n",
    "input_size_2 = 768\n",
    "n_head_2     = 4\n",
    "dm_ff_2      = 2048\n",
    "dp_2         = 0.2\n",
    "num_layers_2 = 4 \n",
    "act_fn_2     = 'relu'\n",
    "\n",
    "# Module 3 hyperparamaters(speaker_context) : Transformer Enc\n",
    "input_size_3 = 231\n",
    "n_head_3     = 3\n",
    "dm_ff_3      = 2048\n",
    "dp_3         = 0.2\n",
    "num_layers_3 = 4 \n",
    "act_fn_3     = 'relu'\n",
    "\n",
    "# Module 4 hyperparamaters(global_emotion_sequence) : GRU\n",
    "input_size_4  = 7\n",
    "hidden_size_4 = 10 \n",
    "num_layers_4  = 2 \n",
    "output_size_4 = 7\n",
    "\n",
    "# Module 5 hyperparamaters(valence) : Transformer Enc\n",
    "input_size_5 = 69\n",
    "n_head_5     = 3\n",
    "dm_ff_5      = 2048\n",
    "dp_5         = 0.2\n",
    "num_layers_5 = 4 \n",
    "act_fn_5     = 'relu'\n",
    "\n",
    "# Module 6 hyperparamaters(speaker_specific_valence_sequence) : GRU\n",
    "input_size_6  = 1\n",
    "hidden_size_6 = 10\n",
    "num_layers_6  = 2\n",
    "output_size_6 = 10\n",
    "\n",
    "# Final Model Hyperparamerters:\n",
    "fc1_out = 800\n",
    "fc2_out = 800\n",
    "fc3_out = 400\n",
    "fc4_out = 100\n",
    "fc5_out = 1\n",
    "\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_df.pkl', 'rb') as f:\n",
    "    train_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemEvalDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.len = len(self.data)\n",
    "        print(list(train_df.columns))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        dict_x = {}\n",
    "        dict_x['speaker'] = torch.tensor(self.data['speakers'][index], dtype=torch.float32)\n",
    "        dict_x['emotion'] = torch.tensor(self.data['emotions'][index], dtype=torch.float32)\n",
    "        dict_x['sentence_embeddings'] = torch.tensor(self.data['sentence_embeddings'][index], dtype=torch.float32)\n",
    "        dict_x['valence'] = torch.tensor(self.data['valence'][index], dtype=torch.float32)\n",
    "\n",
    "        dict_y = {}\n",
    "        dict_y['triggers'] =  torch.tensor(self.data['triggers'][index], dtype=torch.float32)\n",
    "\n",
    "        return dict_x, dict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['episode', 'speakers', 'emotions', 'utterances', 'triggers', 'sentence_embeddings', 'valence']\n"
     ]
    }
   ],
   "source": [
    "dataset = SemEvalDataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "total_size = len(dataset)\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = int(total_size * val_ratio)\n",
    "\n",
    "# Dividir el conjunto de datos\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MELDCollate:\n",
    "    def __init__(self, pad_value = 0):\n",
    "        self.pad_value = pad_value\n",
    "    def __call__(self, batch):\n",
    "        speaker             = pad_sequence([item[0]['speaker'] for item in batch], batch_first = True)\n",
    "        emotion             = pad_sequence([item[0]['emotion'] for item in batch], batch_first = True)\n",
    "        sentence_embeddings = pad_sequence([item[0]['sentence_embeddings'] for item in batch], batch_first = True)\n",
    "        valence             = pad_sequence([item[0]['valence'] for item in batch], batch_first = True)\n",
    "        # print('\\noriginal list : ',[item[0]['speaker'] for item in batch], '\\n\\npadded list : ', speaker)\n",
    "        labels              = pad_sequence([item[1]['triggers'] for item in batch], batch_first = True)\n",
    "\n",
    "        dict_x = { 'speaker': speaker, 'emotion':emotion,  'sentence_embeddings':sentence_embeddings, 'valence':valence}\n",
    "        dict_y = {'labels': labels}\n",
    "\n",
    "        return dict_x, dict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader  = DataLoader(dataset = train_data, batch_size = 64, shuffle=True, collate_fn= MELDCollate())\n",
    "val_loader    = DataLoader(dataset = val_data, batch_size = 64, shuffle=True, collate_fn= MELDCollate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65109/422226933.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  dict_x['sentence_embeddings'] = torch.tensor(self.data['sentence_embeddings'][index], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4:\n",
      "torch.Size([64, 22])\n"
     ]
    }
   ],
   "source": [
    "desired_batch_index = 4\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i == desired_batch_index:\n",
    "        # 'batch' contendrá el batch en el índice especificado\n",
    "        print(f\"Batch {i}:\")\n",
    "        print(batch[1]['labels'].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 231]) torch.Size([7, 7]) torch.Size([7, 768]) torch.Size([7, 69])\n"
     ]
    }
   ],
   "source": [
    "tesst = train_data.__getitem__(1)\n",
    "print(tesst[0]['speaker'].shape, tesst[0]['emotion'].shape, tesst[0]['sentence_embeddings'].shape, tesst[0]['valence'].shape)\n",
    "# print(tesst[0]['valence'].shape)\n",
    "# print(tesst[0]['speaker'].size())\n",
    "# test = tesst[0]['speaker'].unique(dim=0, return_inverse=True)[1]\n",
    "# [torch.where(test == i, tesst[0]['valence'].mean(1), 0) for i in test.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module6GRU(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, output_size):\n",
    "        super(Module6GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Since there are maximum of 8 speakers in a dialogue, so we decided to make 8 GRUs one for each speaker.\n",
    "        self.gru_list= []\n",
    "        for id in range(MAX_NO_OF_SPEAKERS):\n",
    "            self.gru_list.append(nn.GRU(input_size, hidden_size, num_layers, batch_first = True))\n",
    "        self.gru_modules = nn.ModuleList(self.gru_list)\n",
    "\n",
    "    def valence_specific(self, valence, speaker):\n",
    "        speaker = speaker.unique(dim = 0, return_inverse=True)[1]\n",
    "        return [torch.where(speaker == i , valence.mean(1), 0) for i in speaker.unique()]\n",
    "\n",
    "    def applyGRU(self, speaker_valence, seq_len):\n",
    "        speaker_output = torch.zeros(seq_len, self.output_size)\n",
    "\n",
    "        for sp_idx, valence in enumerate(speaker_valence):\n",
    "            # Verificar si hay alguna entrada para este hablante\n",
    "            if valence.nonzero().size(0) == 0:\n",
    "                continue\n",
    "\n",
    "            # Asegúrate de que valence tenga al menos dos dimensiones\n",
    "            valence = valence.unsqueeze(1)\n",
    "\n",
    "            # Inicializar h0 como un tensor 2D\n",
    "            h0 = torch.zeros(self.num_layers, self.hidden_size)  # Ahora h0 es 2D\n",
    "\n",
    "            out, _ = self.gru_modules[sp_idx](valence, h0)\n",
    "\n",
    "            # Rellenar speaker_output con la salida correspondiente\n",
    "            for uid, output in enumerate(out.squeeze(0)):\n",
    "                speaker_output[uid] = output\n",
    "\n",
    "        return speaker_output\n",
    "\n",
    "\n",
    "    def forward(self, x, speakers):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len    = x.size(1)\n",
    "        outputs = []\n",
    "        for i in range(batch_size):\n",
    "            speaker_specific = self.valence_specific(x[i], speakers[i])\n",
    "            out = self.applyGRU(speaker_specific, seq_len)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        final_output = torch.cat([outputs[i].unsqueeze(2) for i in range(len(outputs))], 2).permute(2,0,1)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module5TransformerEnc(nn.Module):\n",
    "    # S, N, E : (seq_len, batch_size, input/embedding_size)\n",
    "    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n",
    "        super(Module5TransformerEnc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src): # src_shape : (S, N, E)\n",
    "        pad_value = torch.zeros(self.input_size).to(device)\n",
    "        # pad_value shape : (E), value : [0,0,0, ...]\n",
    "        src = src.transpose(0,1)\n",
    "        # src_shape : (N, S, E)\n",
    "\n",
    "        src_mask = torch.all(torch.eq(src,pad_value),2)\n",
    "        \n",
    "        # src_mask shape : (N, S), value : for each batch, it is contains seq_len sized tensors and contains true for pad and false for others\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: seq_len, batch_size, input_size \n",
    "        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n",
    "        x_mask = self.make_src_mask(x)\n",
    "        out = self.encoder(x, src_key_padding_mask = x_mask)  \n",
    "        # out shape : (S, N, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module4GRU(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, output_size):\n",
    "        super(Module4GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc  = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('x shape : ', x.shape)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        \n",
    "        # shape of out :  (N, seq_len, hidden_size)     (torch.Size([10, 33, 8])) \n",
    "        # shape of hn  :  (num_layers, N, hidden_size)     (torch.Size([2, 10, 8]))\n",
    "        # shape of hn  :  (N, num_layers, hidden_size) and then flatten it to (N, num_layers*hiddem_size) 3D to 2D\n",
    "        output = self.fc(out)\n",
    "        # shape of output : [N, output_size]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module3TransformerEnc(nn.Module):\n",
    "    # S, N, E : (seq_len, batch_size, input/embedding_size)\n",
    "    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n",
    "        super(Module3TransformerEnc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src): # src_shape : (S, N, E)\n",
    "        pad_value = torch.zeros(self.input_size).to(device)\n",
    "        # pad_value shape : (E), value : [0,0,0, ...]\n",
    "        src = src.transpose(0,1)\n",
    "        # src_shape : (N, S, E)\n",
    "\n",
    "        src_mask = torch.all(torch.eq(src,pad_value),2)\n",
    "        \n",
    "        # src_mask shape : (N, S), value : for each batch, it is contains seq_len sized tensors and contains true for pad and false for others\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: seq_len, batch_size, input_size \n",
    "        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n",
    "        x_mask = self.make_src_mask(x)\n",
    "        out = self.encoder(x, src_key_padding_mask = x_mask)  \n",
    "        # out shape : (S, N, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module2TransformerEnc(nn.Module):\n",
    "    # S, N, E : (seq_len, batch_size, input/embedding_size)\n",
    "    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n",
    "        super(Module2TransformerEnc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src): # src_shape : (S, N, E)\n",
    "        pad_value = torch.zeros(self.input_size)\n",
    "        # pad_value shape : (E), value : [0,0,0, ...]\n",
    "        src = src.transpose(0,1)\n",
    "        # src_shape : (N, S, E)\n",
    "\n",
    "        src_mask = torch.all(torch.eq(src,pad_value),2)\n",
    "        \n",
    "        # src_mask shape : (N, S), value : for each batch, it is contains seq_len sized tensors and contains true for pad and false for others\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: seq_len, batch_size, input_size \n",
    "        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n",
    "        x_mask = self.make_src_mask(x)\n",
    "        out = self.encoder(x, src_key_padding_mask = x_mask)  \n",
    "        # out shape : (S, N, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module1GRU(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, output_size):\n",
    "        super(Module1GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Since there are maximum of 8 speakers in a dialogue, so we decided to make 8 GRUs one for each speaker.\n",
    "        self.gru_list= []\n",
    "        for id in range(MAX_NO_OF_SPEAKERS):\n",
    "            self.gru_list.append(nn.GRU(input_size, hidden_size, num_layers, batch_first = True))\n",
    "        self.gru_modules = nn.ModuleList(self.gru_list)\n",
    "        # self.fc  = nn.Linear(num_layers*hidden_size, output_size)\n",
    "            \n",
    "    \n",
    "    def segregateEmotions(self, emotions, speakers):\n",
    "        speaker_specific = []\n",
    "        utt_id = []\n",
    "        for i in range(MAX_NO_OF_SPEAKERS):\n",
    "            speaker_tensor = torch.zeros(MAX_NO_OF_SPEAKERS, dtype = float)\n",
    "            speaker_tensor[i] = 1\n",
    "            emo = emotions[torch.nonzero((speakers == speaker_tensor).sum(dim=1) == speakers.size(1))].permute(1,0,2)\n",
    "            if(emo.size(1) == 0):\n",
    "                continue\n",
    "            utt_id.append(torch.nonzero((speakers == speaker_tensor).sum(dim=1) == speakers.size(1))[0])\n",
    "            speaker_specific.append(emo)\n",
    "#             print('\\n emo size : ',emo.size())\n",
    "#         print('\\n emo concat size : ',speaker_specific, utt_id)\n",
    "        return speaker_specific, utt_id\n",
    "    \n",
    "    def applyGRU(self, speaker_specific, utt_id, seq_len):\n",
    "        speaker_output = torch.zeros(seq_len, self.output_size)  \n",
    "        for sp_idx in range(len(utt_id)):\n",
    "            h0 = torch.zeros(self.num_layers, 1, self.hidden_size).to(device)\n",
    "            out, hn = self.gru_list[sp_idx](speaker_specific[sp_idx], h0)\n",
    "            for uid in range(utt_id[sp_idx].size(0)):\n",
    "                speaker_output[utt_id[sp_idx][uid]] = out[0][uid].clone()\n",
    "        return speaker_output\n",
    "\n",
    "    def forward(self, x, speakers):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len    = x.size(1)\n",
    "        outputs = []\n",
    "        print('x shape : ', x.shape)\n",
    "        for i in range(batch_size):\n",
    "            speaker_specific, utt_id = self.segregateEmotions(x[i], speakers[i])\n",
    "            out = self.applyGRU(speaker_specific, utt_id, seq_len)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        final_output = torch.cat([outputs[i].unsqueeze(2) for i in range(len(outputs))], 2).permute(2,0,1)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalModel(L.LightningModule):\n",
    "        def __init__(self, \n",
    "                input_size_1, hidden_size_1, num_layers_1, output_size_1,      # module 1    \n",
    "                input_size_2, n_head_2, dm_ff_2, dp_2, num_layers_2, act_fn_2, # module 2\n",
    "                input_size_3, n_head_3, dm_ff_3, dp_3, num_layers_3, act_fn_3, # module 3\n",
    "                input_size_4, hidden_size_4, num_layers_4, output_size_4,      # module 4\n",
    "                input_size_5, n_head_5, dm_ff_5, dp_5, num_layers_5, act_fn_5, # module 5\n",
    "                input_size_6, hidden_size_6, num_layers_6, output_size_6,      # module 6\n",
    "                fc1_out, fc2_out, fc3_out, fc4_out, fc5_out, dp, #masking = False            # final Model parameters\n",
    "                ):\n",
    "                super(FinalModel, self).__init__()\n",
    "\n",
    "                #self.masking = masking\n",
    "\n",
    "                self.module1 = Module1GRU(input_size = input_size_1, num_layers = num_layers_1, hidden_size = hidden_size_1, output_size = output_size_1)\n",
    "                self.module2 = Module2TransformerEnc(input_size = input_size_2, n_head = n_head_2, dim_ff = dm_ff_2, dp = dp_2, num_layers = num_layers_2, act_fn = act_fn_2)\n",
    "                self.module3 = Module3TransformerEnc(input_size = input_size_3, n_head = n_head_3, dim_ff = dm_ff_3, dp = dp_3, num_layers = num_layers_3, act_fn = act_fn_3)\n",
    "                self.module4 = Module4GRU(input_size = input_size_4, num_layers = num_layers_4, hidden_size = hidden_size_4, output_size = output_size_4)\n",
    "                self.module5 = Module5TransformerEnc(input_size = input_size_5, n_head = n_head_5, dim_ff = dm_ff_5, dp = dp_5, num_layers = num_layers_5, act_fn = act_fn_5)\n",
    "                self.module6 = Module6GRU(input_size = input_size_6, num_layers = num_layers_6, hidden_size = hidden_size_6, output_size = output_size_6)\n",
    "\n",
    "                \n",
    "                self.sigmoid = nn.Sigmoid()\n",
    "                self.fc1 = nn.Linear(input_size_2+input_size_3+input_size_5, fc1_out)\n",
    "                self.classification = nn.Sequential(\n",
    "                        nn.Linear(output_size_1 + fc1_out + output_size_4 + output_size_6, fc2_out),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dp), \n",
    "                        nn.Linear(fc2_out, fc3_out),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dp),\n",
    "                        nn.Linear(fc3_out, fc4_out),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dp),\n",
    "                        nn.Linear(fc4_out, fc5_out),\n",
    "                        # nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "        def forward(self, x):\n",
    "                speaker = x['speaker']\n",
    "                emotion = x['emotion']\n",
    "                sentence_embeddings = x['sentence_embeddings']\n",
    "                valence = x['valence']\n",
    "\n",
    "                out1 = self.module1(emotion, speaker)\n",
    "                out2 = self.module2(sentence_embeddings)\n",
    "                out3 = self.module3(speaker)\n",
    "                out4 = self.module4(emotion)\n",
    "                out5 = self.module5(valence)\n",
    "                out6 = self.module6(valence, speaker)\n",
    "\n",
    "                out146 = torch.cat((out1, out4, out6), 2)\n",
    "                out234 = F.relu(self.fc1(torch.cat((out2, out3, out5), 2)))\n",
    "                \n",
    "                out123456 = torch.cat((out146, out234), 2)\n",
    "\n",
    "                out_tensor = torch.zeros(out123456.size(0), out123456.size(1))\n",
    "\n",
    "                for batch_idx in range(out123456.size(0)):\n",
    "                        for seq_idx in range(out123456.size(1)):\n",
    "                                # Obtén la salida de la red para el punto de datos actual\n",
    "                                op = self.classification(out123456[batch_idx][seq_idx])\n",
    "                                op = self.sigmoid(op)\n",
    "\n",
    "                                # Asumiendo que 'op' es un tensor unidimensional con la salida de la clasificación\n",
    "                                # Aquí, seleccionamos el primer elemento ya que op debería ser un scalar después de la sigmoid\n",
    "                                out_tensor[batch_idx, seq_idx] = op  # Ajusta esto según la estructura real de 'op'\n",
    "\n",
    "                # Retorna el tensor de salida en lugar de la lista\n",
    "                return out_tensor\n",
    "\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "                x, y = batch\n",
    "                y_hat = self(x)\n",
    "                print('y_hat shape : ', list(y_hat))\n",
    "                print('y shape : ', y['labels'])\n",
    "                loss = F.binary_cross_entropy(y_hat, y['labels'])\n",
    "                self.log('train_loss', loss)\n",
    "                return loss\n",
    "        \n",
    "        def validation_step(self, batch, batch_idx):\n",
    "                x, y = batch\n",
    "                y_hat = self(x)\n",
    "                loss = F.binary_cross_entropy(y_hat, y['labels'])\n",
    "                self.log('val_loss', loss)\n",
    "                return loss\n",
    "        \n",
    "        def configure_optimizers(self):\n",
    "                optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "                scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=5)\n",
    "                return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "        \n",
    "        def test_step(self, batch, batch_idx):\n",
    "                x, y = batch\n",
    "                y_hat = self(x)\n",
    "                loss = F.binary_cross_entropy(y_hat, y['labels'])\n",
    "                self.log('test_loss', loss)\n",
    "                return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgarc/SemEval/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = FinalModel(\n",
    "        input_size_1, hidden_size_1, num_layers_1, output_size_1,      # module 1    \n",
    "        input_size_2, n_head_2, dm_ff_2, dp_2, num_layers_2, act_fn_2, # module 2\n",
    "        input_size_3, n_head_3, dm_ff_3, dp_3, num_layers_3, act_fn_3, # module 3\n",
    "        input_size_4, hidden_size_4, num_layers_4, output_size_4,      # module 4\n",
    "        input_size_5, n_head_5, dm_ff_5, dp_5, num_layers_5, act_fn_5, # module 5\n",
    "        input_size_6, hidden_size_6, num_layers_6, output_size_6,      # module 6\n",
    "        fc1_out, fc2_out, fc3_out, fc4_out, fc5_out, dp=0.2, #masking = False            # final Model parameters\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "  | Name           | Type                  | Params\n",
      "---------------------------------------------------------\n",
      "0 | module1        | Module1GRU            | 284 K \n",
      "1 | module2        | Module2TransformerEnc | 27.6 M\n",
      "2 | module3        | Module3TransformerEnc | 5.8 M \n",
      "3 | module4        | Module4GRU            | 1.3 K \n",
      "4 | module5        | Module5TransformerEnc | 1.5 M \n",
      "5 | module6        | Module6GRU            | 242 K \n",
      "6 | sigmoid        | Sigmoid               | 0     \n",
      "7 | fc1            | Linear                | 855 K \n",
      "8 | classification | Sequential            | 1.0 M \n",
      "---------------------------------------------------------\n",
      "37.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "37.3 M    Total params\n",
      "149.266   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] x shape :  torch.Size([64, 17, 7])\n",
      "x shape :  torch.Size([64, 17, 7])\n",
      "y_hat shape :  [tensor([0.4616, 0.4700, 0.4644, 0.4665, 0.4756, 0.4663, 0.4635, 0.4646, 0.4755,\n",
      "        0.4602, 0.4779, 0.4698, 0.4569, 0.4650, 0.4717, 0.4756, 0.4596],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4691, 0.4716, 0.4572, 0.4639, 0.4633, 0.4736, 0.4622, 0.4647, 0.4657,\n",
      "        0.4665, 0.4689, 0.4727, 0.4685, 0.4711, 0.4738, 0.4631, 0.4697],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4691, 0.4736, 0.4602, 0.4765, 0.4748, 0.4760, 0.4644, 0.4652, 0.4706,\n",
      "        0.4647, 0.4726, 0.4565, 0.4619, 0.4631, 0.4663, 0.4688, 0.4722],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4710, 0.4691, 0.4714, 0.4655, 0.4657, 0.4701, 0.4754, 0.4790, 0.4630,\n",
      "        0.4623, 0.4679, 0.4684, 0.4691, 0.4669, 0.4712, 0.4686, 0.4697],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4585, 0.4596, 0.4652, 0.4666, 0.4704, 0.4660, 0.4649, 0.4705, 0.4673,\n",
      "        0.4750, 0.4676, 0.4714, 0.4593, 0.4727, 0.4661, 0.4679, 0.4738],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4729, 0.4570, 0.4762, 0.4667, 0.4690, 0.4656, 0.4666, 0.4663, 0.4644,\n",
      "        0.4607, 0.4705, 0.4696, 0.4574, 0.4771, 0.4644, 0.4698, 0.4728],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4653, 0.4661, 0.4662, 0.4582, 0.4665, 0.4676, 0.4671, 0.4698, 0.4634,\n",
      "        0.4683, 0.4594, 0.4675, 0.4619, 0.4716, 0.4702, 0.4737, 0.4721],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4679, 0.4602, 0.4613, 0.4636, 0.4734, 0.4625, 0.4752, 0.4790, 0.4638,\n",
      "        0.4633, 0.4708, 0.4702, 0.4756, 0.4684, 0.4742, 0.4589, 0.4736],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4704, 0.4771, 0.4635, 0.4617, 0.4633, 0.4695, 0.4685, 0.4659, 0.4786,\n",
      "        0.4613, 0.4740, 0.4607, 0.4704, 0.4754, 0.4606, 0.4657, 0.4712],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4629, 0.4708, 0.4652, 0.4634, 0.4709, 0.4676, 0.4612, 0.4708, 0.4661,\n",
      "        0.4680, 0.4599, 0.4614, 0.4649, 0.4682, 0.4709, 0.4682, 0.4775],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4664, 0.4668, 0.4631, 0.4682, 0.4636, 0.4620, 0.4614, 0.4700, 0.4648,\n",
      "        0.4775, 0.4745, 0.4556, 0.4626, 0.4757, 0.4713, 0.4668, 0.4735],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4713, 0.4683, 0.4729, 0.4677, 0.4721, 0.4647, 0.4660, 0.4782, 0.4659,\n",
      "        0.4572, 0.4668, 0.4646, 0.4676, 0.4719, 0.4648, 0.4750, 0.4736],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4695, 0.4710, 0.4690, 0.4559, 0.4622, 0.4685, 0.4738, 0.4695, 0.4641,\n",
      "        0.4643, 0.4711, 0.4728, 0.4782, 0.4588, 0.4660, 0.4732, 0.4676],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4736, 0.4620, 0.4739, 0.4777, 0.4651, 0.4627, 0.4727, 0.4639, 0.4620,\n",
      "        0.4632, 0.4606, 0.4667, 0.4691, 0.4737, 0.4660, 0.4727, 0.4659],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4617, 0.4699, 0.4632, 0.4657, 0.4669, 0.4694, 0.4732, 0.4661, 0.4698,\n",
      "        0.4661, 0.4657, 0.4653, 0.4618, 0.4686, 0.4759, 0.4693, 0.4629],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4749, 0.4689, 0.4675, 0.4718, 0.4593, 0.4746, 0.4672, 0.4696, 0.4657,\n",
      "        0.4638, 0.4694, 0.4653, 0.4769, 0.4606, 0.4775, 0.4719, 0.4743],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4622, 0.4697, 0.4648, 0.4677, 0.4702, 0.4656, 0.4790, 0.4794, 0.4637,\n",
      "        0.4610, 0.4607, 0.4735, 0.4753, 0.4646, 0.4685, 0.4774, 0.4735],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4706, 0.4667, 0.4679, 0.4745, 0.4599, 0.4612, 0.4591, 0.4670, 0.4694,\n",
      "        0.4754, 0.4678, 0.4702, 0.4698, 0.4749, 0.4709, 0.4693, 0.4754],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4620, 0.4747, 0.4695, 0.4681, 0.4755, 0.4703, 0.4741, 0.4636, 0.4646,\n",
      "        0.4654, 0.4700, 0.4611, 0.4625, 0.4632, 0.4602, 0.4661, 0.4654],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4807, 0.4631, 0.4790, 0.4686, 0.4632, 0.4712, 0.4681, 0.4777, 0.4677,\n",
      "        0.4607, 0.4617, 0.4668, 0.4656, 0.4750, 0.4700, 0.4735, 0.4660],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4715, 0.4727, 0.4696, 0.4712, 0.4719, 0.4682, 0.4627, 0.4679, 0.4528,\n",
      "        0.4637, 0.4632, 0.4802, 0.4711, 0.4645, 0.4619, 0.4650, 0.4694],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4693, 0.4656, 0.4662, 0.4686, 0.4673, 0.4719, 0.4750, 0.4660, 0.4666,\n",
      "        0.4684, 0.4684, 0.4674, 0.4605, 0.4651, 0.4616, 0.4664, 0.4721],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4743, 0.4753, 0.4721, 0.4819, 0.4703, 0.4709, 0.4706, 0.4630, 0.4686,\n",
      "        0.4620, 0.4693, 0.4683, 0.4722, 0.4624, 0.4735, 0.4687, 0.4654],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4720, 0.4576, 0.4631, 0.4705, 0.4769, 0.4681, 0.4744, 0.4719, 0.4589,\n",
      "        0.4598, 0.4666, 0.4614, 0.4639, 0.4653, 0.4574, 0.4627, 0.4629],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4683, 0.4631, 0.4701, 0.4589, 0.4643, 0.4688, 0.4760, 0.4649, 0.4678,\n",
      "        0.4561, 0.4625, 0.4697, 0.4772, 0.4618, 0.4674, 0.4669, 0.4712],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4694, 0.4706, 0.4718, 0.4692, 0.4603, 0.4747, 0.4641, 0.4693, 0.4684,\n",
      "        0.4580, 0.4653, 0.4646, 0.4826, 0.4634, 0.4664, 0.4789, 0.4638],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4609, 0.4703, 0.4647, 0.4843, 0.4595, 0.4683, 0.4778, 0.4647, 0.4568,\n",
      "        0.4721, 0.4706, 0.4789, 0.4761, 0.4720, 0.4641, 0.4625, 0.4740],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4665, 0.4791, 0.4631, 0.4757, 0.4707, 0.4706, 0.4693, 0.4700, 0.4628,\n",
      "        0.4712, 0.4703, 0.4641, 0.4687, 0.4738, 0.4743, 0.4744, 0.4615],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4715, 0.4607, 0.4549, 0.4695, 0.4646, 0.4703, 0.4617, 0.4674, 0.4668,\n",
      "        0.4706, 0.4592, 0.4633, 0.4729, 0.4684, 0.4606, 0.4644, 0.4646],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4639, 0.4734, 0.4657, 0.4697, 0.4663, 0.4598, 0.4652, 0.4675, 0.4729,\n",
      "        0.4635, 0.4750, 0.4614, 0.4740, 0.4670, 0.4615, 0.4770, 0.4609],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4743, 0.4736, 0.4767, 0.4676, 0.4760, 0.4697, 0.4745, 0.4610, 0.4696,\n",
      "        0.4766, 0.4613, 0.4562, 0.4681, 0.4625, 0.4639, 0.4682, 0.4648],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4681, 0.4733, 0.4660, 0.4732, 0.4767, 0.4700, 0.4692, 0.4745, 0.4727,\n",
      "        0.4725, 0.4760, 0.4718, 0.4721, 0.4610, 0.4662, 0.4689, 0.4689],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4799, 0.4839, 0.4686, 0.4717, 0.4589, 0.4690, 0.4761, 0.4647, 0.4745,\n",
      "        0.4586, 0.4653, 0.4663, 0.4723, 0.4603, 0.4662, 0.4788, 0.4703],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4635, 0.4741, 0.4677, 0.4662, 0.4649, 0.4817, 0.4741, 0.4717, 0.4673,\n",
      "        0.4669, 0.4565, 0.4662, 0.4630, 0.4694, 0.4615, 0.4672, 0.4816],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4692, 0.4682, 0.4626, 0.4673, 0.4634, 0.4729, 0.4609, 0.4671, 0.4732,\n",
      "        0.4694, 0.4678, 0.4699, 0.4759, 0.4734, 0.4609, 0.4657, 0.4685],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4672, 0.4686, 0.4642, 0.4649, 0.4616, 0.4609, 0.4806, 0.4617, 0.4665,\n",
      "        0.4695, 0.4631, 0.4675, 0.4673, 0.4713, 0.4747, 0.4751, 0.4628],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4684, 0.4626, 0.4886, 0.4656, 0.4670, 0.4773, 0.4655, 0.4726, 0.4578,\n",
      "        0.4672, 0.4651, 0.4652, 0.4657, 0.4679, 0.4748, 0.4713, 0.4653],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4765, 0.4637, 0.4678, 0.4594, 0.4654, 0.4686, 0.4661, 0.4624, 0.4637,\n",
      "        0.4599, 0.4806, 0.4659, 0.4735, 0.4718, 0.4650, 0.4663, 0.4661],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4660, 0.4629, 0.4642, 0.4680, 0.4639, 0.4692, 0.4737, 0.4679, 0.4768,\n",
      "        0.4616, 0.4682, 0.4697, 0.4741, 0.4721, 0.4566, 0.4770, 0.4613],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4601, 0.4642, 0.4721, 0.4706, 0.4733, 0.4646, 0.4739, 0.4706, 0.4660,\n",
      "        0.4685, 0.4606, 0.4636, 0.4755, 0.4707, 0.4702, 0.4721, 0.4682],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4649, 0.4642, 0.4684, 0.4786, 0.4606, 0.4602, 0.4776, 0.4663, 0.4662,\n",
      "        0.4685, 0.4640, 0.4523, 0.4588, 0.4677, 0.4610, 0.4694, 0.4709],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4595, 0.4637, 0.4746, 0.4708, 0.4615, 0.4725, 0.4715, 0.4659, 0.4614,\n",
      "        0.4627, 0.4618, 0.4608, 0.4619, 0.4697, 0.4631, 0.4641, 0.4809],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4644, 0.4636, 0.4601, 0.4672, 0.4703, 0.4637, 0.4706, 0.4668, 0.4627,\n",
      "        0.4734, 0.4725, 0.4685, 0.4655, 0.4726, 0.4564, 0.4649, 0.4625],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4757, 0.4642, 0.4671, 0.4683, 0.4578, 0.4625, 0.4688, 0.4756, 0.4618,\n",
      "        0.4709, 0.4667, 0.4695, 0.4676, 0.4723, 0.4695, 0.4685, 0.4665],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4668, 0.4715, 0.4617, 0.4647, 0.4646, 0.4627, 0.4620, 0.4691, 0.4584,\n",
      "        0.4712, 0.4697, 0.4673, 0.4692, 0.4818, 0.4709, 0.4686, 0.4655],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4668, 0.4688, 0.4704, 0.4669, 0.4636, 0.4746, 0.4606, 0.4685, 0.4612,\n",
      "        0.4670, 0.4660, 0.4580, 0.4610, 0.4663, 0.4647, 0.4663, 0.4674],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4660, 0.4672, 0.4749, 0.4745, 0.4701, 0.4688, 0.4659, 0.4614, 0.4671,\n",
      "        0.4676, 0.4817, 0.4715, 0.4647, 0.4673, 0.4774, 0.4603, 0.4759],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4632, 0.4656, 0.4641, 0.4710, 0.4768, 0.4674, 0.4695, 0.4665, 0.4746,\n",
      "        0.4706, 0.4675, 0.4671, 0.4674, 0.4698, 0.4737, 0.4563, 0.4671],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4679, 0.4650, 0.4784, 0.4709, 0.4818, 0.4714, 0.4618, 0.4602, 0.4660,\n",
      "        0.4652, 0.4715, 0.4652, 0.4740, 0.4654, 0.4644, 0.4716, 0.4638],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4673, 0.4703, 0.4708, 0.4715, 0.4676, 0.4612, 0.4780, 0.4804, 0.4671,\n",
      "        0.4702, 0.4691, 0.4749, 0.4592, 0.4690, 0.4624, 0.4722, 0.4682],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4627, 0.4656, 0.4729, 0.4601, 0.4717, 0.4674, 0.4701, 0.4773, 0.4722,\n",
      "        0.4589, 0.4619, 0.4670, 0.4596, 0.4688, 0.4794, 0.4628, 0.4756],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4650, 0.4763, 0.4651, 0.4595, 0.4729, 0.4707, 0.4657, 0.4718, 0.4616,\n",
      "        0.4686, 0.4588, 0.4709, 0.4676, 0.4718, 0.4583, 0.4675, 0.4716],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4650, 0.4679, 0.4617, 0.4661, 0.4648, 0.4703, 0.4519, 0.4624, 0.4760,\n",
      "        0.4642, 0.4747, 0.4773, 0.4721, 0.4660, 0.4676, 0.4700, 0.4745],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4635, 0.4700, 0.4776, 0.4688, 0.4596, 0.4625, 0.4708, 0.4740, 0.4558,\n",
      "        0.4678, 0.4721, 0.4693, 0.4656, 0.4753, 0.4801, 0.4648, 0.4719],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4679, 0.4685, 0.4602, 0.4654, 0.4681, 0.4535, 0.4700, 0.4657, 0.4604,\n",
      "        0.4773, 0.4766, 0.4762, 0.4621, 0.4672, 0.4759, 0.4674, 0.4676],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4784, 0.4616, 0.4670, 0.4730, 0.4787, 0.4614, 0.4712, 0.4693, 0.4638,\n",
      "        0.4707, 0.4718, 0.4685, 0.4704, 0.4672, 0.4720, 0.4723, 0.4654],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4666, 0.4620, 0.4765, 0.4634, 0.4652, 0.4675, 0.4502, 0.4690, 0.4687,\n",
      "        0.4702, 0.4750, 0.4729, 0.4627, 0.4705, 0.4636, 0.4720, 0.4766],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4630, 0.4691, 0.4716, 0.4640, 0.4766, 0.4601, 0.4743, 0.4660, 0.4766,\n",
      "        0.4688, 0.4649, 0.4644, 0.4588, 0.4654, 0.4686, 0.4633, 0.4591],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4649, 0.4704, 0.4658, 0.4747, 0.4780, 0.4750, 0.4781, 0.4685, 0.4787,\n",
      "        0.4775, 0.4669, 0.4587, 0.4713, 0.4694, 0.4759, 0.4672, 0.4745],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4696, 0.4611, 0.4569, 0.4743, 0.4623, 0.4715, 0.4632, 0.4698, 0.4672,\n",
      "        0.4715, 0.4607, 0.4663, 0.4680, 0.4667, 0.4642, 0.4748, 0.4759],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4713, 0.4715, 0.4669, 0.4677, 0.4733, 0.4622, 0.4719, 0.4684, 0.4670,\n",
      "        0.4701, 0.4681, 0.4691, 0.4720, 0.4675, 0.4771, 0.4610, 0.4724],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4653, 0.4645, 0.4749, 0.4800, 0.4609, 0.4654, 0.4668, 0.4645, 0.4648,\n",
      "        0.4667, 0.4599, 0.4730, 0.4569, 0.4662, 0.4653, 0.4654, 0.4651],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4706, 0.4703, 0.4727, 0.4830, 0.4650, 0.4627, 0.4721, 0.4700, 0.4592,\n",
      "        0.4718, 0.4633, 0.4629, 0.4668, 0.4669, 0.4770, 0.4670, 0.4738],\n",
      "       grad_fn=<UnbindBackward0>), tensor([0.4695, 0.4662, 0.4727, 0.4652, 0.4726, 0.4659, 0.4655, 0.4706, 0.4577,\n",
      "        0.4701, 0.4620, 0.4669, 0.4667, 0.4749, 0.4644, 0.4706, 0.4705],\n",
      "       grad_fn=<UnbindBackward0>)]\n",
      "y shape :  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0: 100%|██████████| 1/1 [00:10<00:00,  0.10it/s]x shape :  torch.Size([64, 19, 7])\n",
      "x shape :  torch.Size([64, 19, 7])\n",
      "Epoch 0: 100%|██████████| 1/1 [00:13<00:00,  0.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:13<00:00,  0.08it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(fast_dev_run=True)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
