{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:32:53.538726Z","iopub.status.busy":"2024-01-23T14:32:53.537902Z","iopub.status.idle":"2024-01-23T14:32:53.559167Z","shell.execute_reply":"2024-01-23T14:32:53.558220Z","shell.execute_reply.started":"2024-01-23T14:32:53.538686Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/sgarc/SemEval-2023-Task-10/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["Using device =>  cpu  torch  2.1.1+cu121\n"]}],"source":["import os\n","import transformers\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","import torch\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import confusion_matrix, classification_report\n","import lightning as L\n","import torchmetrics\n","from torchmetrics.functional.classification import binary_accuracy\n","from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.tensorboard import SummaryWriter\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","import pickle\n","import re\n","import copy\n","import time\n","import math\n","\n","# Scoring\n","from sklearn.metrics import classification_report, f1_score\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using device => \",device, ' torch ', torch.__version__)\n","torch.device(device)\n","\n","# hyper parameters\n","SEED = 42\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEED)\n","\n","#@title Hyper Parameters { display-mode: \"both\" }\n","\n","EPOCHS             = 20\n","MAX_NO_OF_SPEAKERS = 8\n","MAX_DIALOGUE_LEN   = 33\n","MAX_SEQUENCE_LEN   = 24\n","original_labels    = ['abuse', 'adoration', 'annoyance', 'awkwardness', 'benefit', 'boredom', 'calmness', 'challenge', 'cheer', 'confusion', 'curiosity', 'desire', 'excitement', 'guilt', 'horror', 'humour', 'impressed', 'loss', 'nervousness', 'nostalgia', 'pain', 'relief', 'satisfaction', 'scold', 'shock', 'sympathy', 'threat']\n","train_count        = [31, 190, 1051, 880, 220, 78, 752, 214, 534, 486, 545, 180, 867, 216, 280, 153, 257, 351, 398, 65, 36, 173, 136, 94, 372, 209, 263]\n","\n","EMOTIONS           = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n","ALPHA_TENSOR       = torch.Tensor([0.1110, 0.0271, 0.0268, 0.1745, 0.4715, 0.0684, 0.1206]).to(device)\n","\n","# DataLoader Hyperparamaters\n","BATCH_SIZE = 32\n","\n","# Module 1 hyperparamaters(speaker_specific_emotion_sequence) : GRU n-n\n","input_size_1  = 7\n","hidden_size_1 = 10 \n","num_layers_1  = 2 \n","output_size_1 = 10\n","\n","\n","# Module 2 hyperparamaters(utterance_context) : Transformer Enc\n","input_size_2 = 768\n","n_head_2     = 4\n","dm_ff_2      = 2048\n","dp_2         = 0.2\n","num_layers_2 = 4 \n","act_fn_2     = 'relu'\n","\n","# Module 3 hyperparamaters(speaker_context) : Transformer Enc\n","input_size_3 = 8\n","n_head_3     = 4\n","dm_ff_3      = 2048\n","dp_3         = 0.2\n","num_layers_3 = 4 \n","act_fn_3     = 'relu'\n","\n","# Module 4 hyperparamaters(global_emotion_sequence) : GRU\n","input_size_4  = 1\n","hidden_size_4 = 10 \n","num_layers_4  = 2 \n","output_size_4 = 10\n","\n","# Module 5 hyperparamaters(valence) : Transformer Enc\n","input_size_5 = 3\n","n_head_5     = 1\n","dm_ff_5      = 2048\n","dp_5         = 0.2\n","num_layers_5 = 4 \n","act_fn_5     = 'relu'\n","\n","# Module 5 hyperparamaters(valence) : GRU\n","#input_size_5  = 3\n","#hidden_size_5 = 10\n","#num_layers_5  = 2\n","#output_size_5 = 10\n","\n","\n","# Final Model Hyperparamerters:\n","fc1_out = 800\n","fc2_out = 650\n","fc3_out = 500\n","fc4_out = 350\n","fc5_out = 200\n","fc6_out = 50\n","fc7_out = len(EMOTIONS)\n","\n","# LSTM\n","input_size_6  = fc1_out + input_size_5 + 1\n","hidden_size_6 = 7\n","num_layers_6  = 3\n","output_size_6 = 7\n","\n","\n","#LSTM Parameters\n","#input_size_lstm = output_size_1 + fc1_out + output_size_4 + output_size_6   # Tamaño de entrada\n","\n","learning_rate = 0.001"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:33.930214Z","iopub.status.busy":"2024-01-23T14:13:33.929663Z","iopub.status.idle":"2024-01-23T14:13:42.459472Z","shell.execute_reply":"2024-01-23T14:13:42.458662Z","shell.execute_reply.started":"2024-01-23T14:13:33.930187Z"},"trusted":true},"outputs":[],"source":["#Pandas Train Dataset after pre-processing\n","with open('train_df.pkl', 'rb') as f:\n","    train_df = pickle.load(f)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:42.462404Z","iopub.status.busy":"2024-01-23T14:13:42.462028Z","iopub.status.idle":"2024-01-23T14:13:44.663447Z","shell.execute_reply":"2024-01-23T14:13:44.662582Z","shell.execute_reply.started":"2024-01-23T14:13:42.462370Z"},"trusted":true},"outputs":[],"source":["#Pandas Test Dataset after pre-processing\n","with open('test_df.pkl', 'rb') as f:\n","    test_df = pickle.load(f)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:44.692575Z","iopub.status.busy":"2024-01-23T14:13:44.692275Z","iopub.status.idle":"2024-01-23T14:13:44.702943Z","shell.execute_reply":"2024-01-23T14:13:44.702042Z","shell.execute_reply.started":"2024-01-23T14:13:44.692549Z"},"trusted":true},"outputs":[],"source":["class SemEvalDataset(Dataset):\n","    \"\"\"\n","        Class Dataset: Data\n","    \"\"\"\n","    def __init__(self, data):\n","        self.data = data\n","        self.len = len(self.data)\n","        print(list(train_df.columns))\n","        \n","    def __len__(self):\n","        return self.len\n","    \n","    def __getitem__(self, index):\n","        dict_x = {}\n","        dict_x['speaker'] = torch.tensor(self.data['speakers'][index], dtype=torch.float32)\n","        dict_x['triggers'] =  torch.tensor(self.data['triggers'][index], dtype=torch.float32)\n","        dict_x['sentence_embeddings'] = torch.tensor(self.data['sentence_embeddings'][index], dtype=torch.float32)\n","        dict_x['VAD'] = torch.tensor(self.data['VAD'][index], dtype=torch.float32)\n","        dict_x['VAD_mask'] = torch.tensor(self.data['VAD_mask'][index], dtype=torch.float32)\n","\n","        dict_y = {}\n","        dict_y['emotion'] = torch.tensor(self.data['emotions'][index], dtype=torch.float32)\n","\n","        return dict_x, dict_y"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:44.704924Z","iopub.status.busy":"2024-01-23T14:13:44.704232Z","iopub.status.idle":"2024-01-23T14:13:44.719671Z","shell.execute_reply":"2024-01-23T14:13:44.718981Z","shell.execute_reply.started":"2024-01-23T14:13:44.704889Z"},"trusted":true},"outputs":[],"source":["class SemEvalDatasetTest(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","        self.len = len(self.data)\n","        print(list(train_df.columns))\n","        \n","    def __len__(self):\n","        return self.len\n","    \n","    def __getitem__(self, index):\n","        dict_x = {}\n","        dict_x['speaker'] = torch.tensor(self.data['speakers'][index], dtype=torch.float32)\n","        dict_x['triggers'] =  torch.tensor(self.data['triggers'][index], dtype=torch.float32)\n","        dict_x['sentence_embeddings'] = torch.tensor(self.data['sentence_embeddings'][index], dtype=torch.float32)\n","        dict_x['VAD'] = torch.tensor(self.data['VAD'][index], dtype=torch.float32)\n","        dict_x['VAD_mask'] = torch.tensor(self.data['VAD_mask'][index], dtype=torch.float32)\n","\n","        return dict_x"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:44.721507Z","iopub.status.busy":"2024-01-23T14:13:44.720866Z","iopub.status.idle":"2024-01-23T14:13:44.732283Z","shell.execute_reply":"2024-01-23T14:13:44.731467Z","shell.execute_reply.started":"2024-01-23T14:13:44.721475Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['episode', 'speakers', 'emotions', 'utterances', 'triggers', 'sentence_embeddings', 'VAD', 'VAD_mask']\n","['episode', 'speakers', 'emotions', 'utterances', 'triggers', 'sentence_embeddings', 'VAD', 'VAD_mask']\n"]}],"source":["dataset = SemEvalDataset(train_df)\n","test = SemEvalDatasetTest(test_df)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:44.733525Z","iopub.status.busy":"2024-01-23T14:13:44.733273Z","iopub.status.idle":"2024-01-23T14:13:44.765026Z","shell.execute_reply":"2024-01-23T14:13:44.764148Z","shell.execute_reply.started":"2024-01-23T14:13:44.733504Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import random_split\n","total_size = len(dataset)\n","train_ratio = 0.8\n","val_ratio = 0.2\n","\n","train_size = int(total_size * train_ratio)\n","val_size = int(total_size * val_ratio)\n","\n","# Dividir el conjunto de datos\n","train_data, val_data = random_split(dataset, [train_size, val_size])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:44.768590Z","iopub.status.busy":"2024-01-23T14:13:44.768309Z","iopub.status.idle":"2024-01-23T14:13:44.773050Z","shell.execute_reply":"2024-01-23T14:13:44.772054Z","shell.execute_reply.started":"2024-01-23T14:13:44.768566Z"},"trusted":true},"outputs":[],"source":["train_loader  = DataLoader(dataset = train_data, batch_size = BATCH_SIZE, num_workers=3)#, shuffle=True, collate_fn= MELDCollate())\n","val_loader    = DataLoader(dataset = val_data, batch_size = BATCH_SIZE, num_workers=3)#, shuffle=True, collate_fn= MELDCollate())"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:44.774450Z","iopub.status.busy":"2024-01-23T14:13:44.774154Z","iopub.status.idle":"2024-01-23T14:13:44.784256Z","shell.execute_reply":"2024-01-23T14:13:44.783323Z","shell.execute_reply.started":"2024-01-23T14:13:44.774426Z"},"trusted":true},"outputs":[],"source":["#desired_batch_index = 4\n","#for i, batch in enumerate(train_loader):\n","# if i == desired_batch_index:\n","#     # 'batch' contendrá el batch en el índice especificado\n","#     print(f\"Batch {i}:\")\n","#     bat = batch\n","#     break"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:44.785652Z","iopub.status.busy":"2024-01-23T14:13:44.785363Z","iopub.status.idle":"2024-01-23T14:13:44.795395Z","shell.execute_reply":"2024-01-23T14:13:44.794589Z","shell.execute_reply.started":"2024-01-23T14:13:44.785622Z"},"trusted":true},"outputs":[],"source":["#idx = torch.nonzero(bat[0]['VAD_mask'].logical_not())[-1]\n","#bat[0]['VAD_mask'].T\n","#print(bat[0]['VAD_mask'].unsqueeze(-1).shape, '\\n\\n', bat[1]['emotion'].shape)\n","#test = torch.cat((bat[0]['VAD_mask'].unsqueeze(-1), bat[1]['emotion']), -1)\n","#test.argmax(-1)\n","#torch.hstack((test2[:,:idx+1]+1,test2[:,idx+1:]))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:44.809499Z","iopub.status.busy":"2024-01-23T14:13:44.809147Z","iopub.status.idle":"2024-01-23T14:13:44.825240Z","shell.execute_reply":"2024-01-23T14:13:44.824499Z","shell.execute_reply.started":"2024-01-23T14:13:44.809475Z"},"trusted":true},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, dim_model, dropout_p, max_len):\n","        super().__init__()\n","        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n","        # max_len determines how far the position can have an effect on a token (window)\n","        \n","        # Info\n","        self.dropout = nn.Dropout(dropout_p)\n","        \n","        # Encoding - From formula\n","        pos_encoding = torch.zeros(max_len, dim_model)\n","        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1)  # 0, 1, 2, 3, 4, 5\n","        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model)  # 1000^(2i/dim_model)\n","        \n","        # Reshape division_term to have the same number of rows as pos_encoding\n","        #division_term = division_term.view(1, -1)\n","\n","        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n","        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n","        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n","        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n","        \n","        # Saving buffer (same as parameter without gradients needed)\n","        pos_encoding = pos_encoding.unsqueeze(0)\n","        self.register_buffer(\"pos_encoding\", pos_encoding)\n","        \n","    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n","        # Residual connection + pos encoding\n","        return self.dropout(token_embedding + self.pos_encoding[:, :token_embedding.size(1), :]).requires_grad_(False)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:44.826902Z","iopub.status.busy":"2024-01-23T14:13:44.826555Z","iopub.status.idle":"2024-01-23T14:13:44.845594Z","shell.execute_reply":"2024-01-23T14:13:44.844855Z","shell.execute_reply.started":"2024-01-23T14:13:44.826868Z"},"trusted":true},"outputs":[],"source":["class Module6GRU(nn.Module):\n","    def __init__(self, input_size, num_layers, hidden_size, output_size):\n","        super(Module6GRU, self).__init__()\n","        self.num_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        # Since there are maximum of 8 speakers in a dialogue, so we decided to make 8 GRUs one for each speaker.\n","        self.gru_list= []\n","        for id in range(MAX_NO_OF_SPEAKERS):\n","            self.gru_list.append(nn.GRU(input_size, hidden_size, num_layers, batch_first = True))\n","        self.gru_modules = nn.ModuleList(self.gru_list)\n","\n","    def valence_specific(self, valence, speaker):\n","        speaker = speaker.unique(dim=0, return_inverse=True)[1]\n","        # Asegúrate de que el tensor de padding esté en el mismo dispositivo que 'speaker' y 'valence'.\n","        padding_tensor = torch.zeros(valence.size(1), device=speaker.device)\n","        # Ahora utiliza el tensor de padding que está en el dispositivo correcto.\n","        return [torch.where(speaker.unsqueeze(1) == i, valence[:speaker.size(0)], padding_tensor) for i in speaker.unique()], speaker.unique()\n","\n","    def applyGRU(self, speaker_valence, sp_idx , seq_len):\n","        speaker_output = torch.zeros(seq_len, self.output_size, device = device)\n","        for sp_idx, valence in zip(sp_idx, speaker_valence):\n","            # Verificar si hay alguna entrada para este hablante\n","            if valence.nonzero().size(0) == 0:\n","                continue\n","\n","            if sp_idx >= 8:\n","                # Manejar el error o ajustar sp_idx aquí\n","                sp_idx = 7\n","            # Asegúrate de que valence tenga al menos dos dimensiones\n","\n","            # Inicializar h0 como un tensor 2D\n","            h0 = torch.zeros(self.num_layers, self.hidden_size, device = device)  # Ahora h0 es 2D\n","            out, _ = self.gru_modules[sp_idx](valence, h0)\n","\n","            # Rellenar speaker_output con la salida correspondiente\n","            for uid, output in enumerate(out.squeeze(0)):\n","                speaker_output[uid] = output\n","\n","        return speaker_output\n","\n","\n","    def forward(self, x, speakers):\n","        batch_size = x.size(0)\n","        seq_len    = speakers.size(1)\n","        outputs = []\n","        for i in range(batch_size):\n","            speaker_specific, sp_idx = self.valence_specific(x[i], speakers[i])\n","            out = self.applyGRU(speaker_specific, sp_idx ,seq_len)\n","            outputs.append(out)\n","        \n","        final_output = torch.cat([outputs[i].unsqueeze(2) for i in range(len(outputs))], 2).permute(2,0,1)\n","        \n","        return final_output"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:44.847540Z","iopub.status.busy":"2024-01-23T14:13:44.847210Z","iopub.status.idle":"2024-01-23T14:13:44.864374Z","shell.execute_reply":"2024-01-23T14:13:44.863679Z","shell.execute_reply.started":"2024-01-23T14:13:44.847509Z"},"trusted":true},"outputs":[],"source":["class Module5TransformerEnc(nn.Module):\n","    # S, N, E : (seq_len, batch_size, input/embedding_size)\n","    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n","        super(Module5TransformerEnc, self).__init__()\n","        self.input_size = input_size\n","        \n","        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n","        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n","        \n","        \n","        self.positional_encoder = PositionalEncoding(\n","            dim_model=input_size, dropout_p=dp, max_len=MAX_SEQUENCE_LEN\n","        )\n","        \n","\n","    def forward(self, x, x_mask):\n","        # x shape: seq_len, batch_size, input_size\n","        x = self.positional_encoder(x)\n","        x = x.permute(1,0,2)\n","        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n","        \n","        out = self.encoder(x, src_key_padding_mask=x_mask)\n","        # out shape : (S, N, E)\n","        return out"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:44.866019Z","iopub.status.busy":"2024-01-23T14:13:44.865644Z","iopub.status.idle":"2024-01-23T14:13:44.877247Z","shell.execute_reply":"2024-01-23T14:13:44.876159Z","shell.execute_reply.started":"2024-01-23T14:13:44.865989Z"},"trusted":true},"outputs":[],"source":["class Module4GRU(nn.Module):\n","    def __init__(self, input_size, num_layers, hidden_size, output_size):\n","        super(Module4GRU, self).__init__()\n","        self.num_layers = num_layers\n","        self.hidden_size = hidden_size\n","\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n","        self.fc  = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device = device)\n","        out, _ = self.gru(x.unsqueeze(-1), h0)\n","        \n","        # shape of out :  (N, seq_len, hidden_size)     (torch.Size([10, 33, 8])) \n","        # shape of hn  :  (num_layers, N, hidden_size)     (torch.Size([2, 10, 8]))\n","        # shape of hn  :  (N, num_layers, hidden_size) and then flatten it to (N, num_layers*hiddem_size) 3D to 2D\n","        output = self.fc(out)\n","        # shape of output : [N, output_size]\n","\n","        return output"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:44.878983Z","iopub.status.busy":"2024-01-23T14:13:44.878410Z","iopub.status.idle":"2024-01-23T14:13:44.894291Z","shell.execute_reply":"2024-01-23T14:13:44.893386Z","shell.execute_reply.started":"2024-01-23T14:13:44.878949Z"},"trusted":true},"outputs":[],"source":["class Module3TransformerEnc(nn.Module):\n","    # S, N, E : (seq_len, batch_size, input/embedding_size)\n","    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n","        super(Module3TransformerEnc, self).__init__()\n","        self.input_size = input_size\n","        \n","        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n","        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n","        \n","        self.positional_encoder = PositionalEncoding(\n","            dim_model=input_size, dropout_p=dp, max_len=MAX_SEQUENCE_LEN\n","        )\n","        \n","\n","    def forward(self, x, x_mask):\n","        # x shape: seq_len, batch_size, input_size\n","        x = self.positional_encoder(x)\n","        x = x.permute(1,0,2)\n","        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n","        \n","        out = self.encoder(x, src_key_padding_mask=x_mask)\n","        # out shape : (S, N, E)\n","        return out"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:44.896093Z","iopub.status.busy":"2024-01-23T14:13:44.895504Z","iopub.status.idle":"2024-01-23T14:13:44.905520Z","shell.execute_reply":"2024-01-23T14:13:44.904903Z","shell.execute_reply.started":"2024-01-23T14:13:44.896062Z"},"trusted":true},"outputs":[],"source":["class Module2TransformerEnc(nn.Module):\n","    # S, N, E : (seq_len, batch_size, input/embedding_size)\n","    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n","        super(Module2TransformerEnc, self).__init__()\n","        self.input_size = input_size\n","        \n","        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n","        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n","        \n","        self.positional_encoder = PositionalEncoding(\n","            dim_model=input_size, dropout_p=dp, max_len=MAX_SEQUENCE_LEN\n","        )\n","\n","    def forward(self, x, x_mask):\n","        # x shape: seq_len, batch_size, input_size\n","        x = self.positional_encoder(x)\n","        x = x.permute(1,0,2)\n","        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n","        \n","        out = self.encoder(x, src_key_padding_mask=x_mask)\n","        # out shape : (S, N, E)\n","        return out"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T14:13:44.906985Z","iopub.status.busy":"2024-01-23T14:13:44.906697Z","iopub.status.idle":"2024-01-23T14:13:44.928413Z","shell.execute_reply":"2024-01-23T14:13:44.927559Z","shell.execute_reply.started":"2024-01-23T14:13:44.906963Z"},"trusted":true},"outputs":[],"source":["class Module1GRU(nn.Module):\n","    def __init__(self, input_size, num_layers, hidden_size, output_size):\n","        super(Module1GRU, self).__init__()\n","        self.num_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        # Since there are maximum of 8 speakers in a dialogue, so we decided to make 8 GRUs one for each speaker.\n","        self.gru_list= []\n","        for id in range(MAX_NO_OF_SPEAKERS):\n","            self.gru_list.append(nn.GRU(input_size, hidden_size, num_layers, batch_first = True))\n","        self.gru_modules = nn.ModuleList(self.gru_list)\n","        # self.fc  = nn.Linear(num_layers*hidden_size, output_size)\n","            \n","    \n","    def segregateEmotions(self, emotions, speakers):\n","        speaker_specific = []\n","        utt_id = []\n","        for i in range(MAX_NO_OF_SPEAKERS):\n","            speaker_tensor = torch.zeros(MAX_NO_OF_SPEAKERS, device = device)\n","            speaker_tensor[i] = 1\n","            emo = emotions[torch.nonzero((speakers == speaker_tensor).sum(dim=1) == speakers.size(1))].permute(1,0,2)\n","            if(emo.size(1) == 0):\n","                continue\n","            utt_id.append(torch.nonzero((speakers == speaker_tensor).sum(dim=1) == speakers.size(1))[0])\n","            speaker_specific.append(emo)\n","#             print('\\n emo size : ',emo.size())\n","#         print('\\n emo concat size : ',speaker_specific, utt_id)\n","        return speaker_specific, utt_id\n","    \n","    def applyGRU(self, speaker_specific, utt_id, seq_len):\n","        speaker_output = torch.zeros(seq_len, self.output_size, device = device)  \n","        for sp_idx in range(len(utt_id)):\n","            h0 = torch.zeros(self.num_layers, 1, self.hidden_size).to(device)\n","            out, hn = self.gru_list[sp_idx](speaker_specific[sp_idx], h0)\n","            for uid in range(utt_id[sp_idx].size(0)):\n","                speaker_output[utt_id[sp_idx][uid]] = out[0][uid].clone()\n","        return speaker_output\n","\n","    def forward(self, x, speakers):\n","        batch_size = x.size(0)\n","        seq_len    = x.size(1)\n","        outputs = []\n","        for i in range(batch_size):\n","            speaker_specific, utt_id = self.segregateEmotions(x[i], speakers[i])\n","            out = self.applyGRU(speaker_specific, utt_id, seq_len)\n","            outputs.append(out)\n","        \n","        final_output = torch.cat([outputs[i].unsqueeze(2) for i in range(len(outputs))], 2).permute(2,0,1)\n","        \n","        return final_output"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T16:04:56.939910Z","iopub.status.busy":"2024-01-23T16:04:56.939205Z","iopub.status.idle":"2024-01-23T16:04:56.972013Z","shell.execute_reply":"2024-01-23T16:04:56.971245Z","shell.execute_reply.started":"2024-01-23T16:04:56.939873Z"},"trusted":true},"outputs":[],"source":["class FinalModel(L.LightningModule):\n","        def __init__(self, \n","                input_size_1, hidden_size_1, num_layers_1, output_size_1,      # module 1    \n","                input_size_2, n_head_2, dm_ff_2, dp_2, num_layers_2, act_fn_2, # module 2\n","                input_size_3, n_head_3, dm_ff_3, dp_3, num_layers_3, act_fn_3, # module 3\n","                input_size_4, hidden_size_4, num_layers_4, output_size_4,      # module 4\n","                input_size_5, n_head_5, dm_ff_5, dp_5, num_layers_5, act_fn_5, # module 5\n","                input_size_6, hidden_size_6, num_layers_6, output_size_6,      # module 6\n","                fc1_out, fc2_out, fc3_out, fc4_out, fc5_out, fc6_out, fc7_out,dp, criterion #masking = False            # final Model parameters\n","                ):\n","                super(FinalModel, self).__init__()\n","\n","                #self.masking = masking\n","                self.criterion = criterion\n","                #self.module1 = Module1GRU(input_size = input_size_1, num_layers = num_layers_1, hidden_size = hidden_size_1, output_size = output_size_1)\n","                self.module2 = Module2TransformerEnc(input_size = input_size_2, n_head = n_head_2, dim_ff = dm_ff_2, dp = dp_2, num_layers = num_layers_2, act_fn = act_fn_2)\n","                self.module3 = Module3TransformerEnc(input_size = input_size_3, n_head = n_head_3, dim_ff = dm_ff_3, dp = dp_3, num_layers = num_layers_3, act_fn = act_fn_3)\n","                #self.module4 = Module4GRU(input_size = input_size_4, num_layers = num_layers_4, hidden_size = hidden_size_4, output_size = output_size_4)\n","                #self.module5 = Module5TransformerEnc(input_size = input_size_5, n_head = n_head_5, dim_ff = dm_ff_5, dp = dp_5, num_layers = num_layers_5, act_fn = act_fn_5)\n","                #self.module6 = Module6GRU(input_size = input_size_6, num_layers = num_layers_6, hidden_size = hidden_size_6, output_size = output_size_6)\n","\n","                \n","                self.fc1 = nn.Linear(input_size_2+input_size_3, fc1_out)\n","                self.classification = nn.Sequential(\n","                        nn.Linear((fc1_out ), fc2_out),#+ input_size_5\n","                        nn.ReLU(),\n","                        nn.Dropout(dp), \n","                        nn.Linear(fc2_out, fc3_out),\n","                        nn.ReLU(),\n","                        nn.Dropout(dp),\n","                        nn.Linear(fc3_out, fc4_out),\n","                        nn.ReLU(),\n","                        nn.Dropout(dp),\n","                        nn.Linear(fc4_out, fc5_out),\n","                        nn.ReLU(),\n","                        nn.Dropout(dp),\n","                        nn.Linear(fc5_out, fc6_out),\n","                        nn.ReLU(),\n","                        nn.Dropout(dp),\n","                        nn.Linear(fc6_out, fc7_out),\n","                        nn.Softmax(dim=-1)\n","                )\n","                self.lstm = nn.LSTM(input_size_6, hidden_size_6, num_layers_6, dropout=dp)\n","                \n","        def forward(self, x):\n","                speaker = x['speaker']\n","                triggers = x['triggers'].T\n","                sentence_embeddings = x['sentence_embeddings']\n","                VAD = x['VAD'].permute(1, 0, 2)\n","                VAD_mask = x['VAD_mask']\n","\n","                #out1 = self.module1(emotion, speaker)\n","                #print('Hay NaN Out1') if out1.isnan().any() == True else None\n","                out2 = self.module2(sentence_embeddings, VAD_mask)\n","                print('Hay NaN Out2') if out2.isnan().any() == True else None\n","                out3 = self.module3(speaker, VAD_mask)\n","                print('Hay NaN Out3') if out3.isnan().any() == True else None\n","                #out4 = self.module4(triggers)\n","                #print('Hay NaN Out4') if out4.isnan().any() == True else None\n","                #out5 = self.module5(VAD, VAD_mask).permute(1,0,2)\n","                #print('Hay NaN Out5') if out5.isnan().any() == True else None\n","                #out6 = self.module6(VAD, speaker)\n","                #print('Hay NaN Out6') if out6.isnan().any() == True else None\n","\n","                #out46 = torch.cat((out4, out6), 2)\n","                out235 = torch.cat((out2, out3), 2)\n","                out235 = F.relu(self.fc1(out235))\n","                #out23456 = torch.cat((out46, out235), 2),  triggers.unsqueeze(-1)\n","                #out23456_vad = torch.cat((out235), 2)#VAD,\n","                final_seq = self.classification(out235)\n","                return final_seq\n","            \n","\n","        def training_step(self, batch, batch_idx):\n","                x, y = batch\n","                y_hat = self(x)\n","                predics = y_hat\n","                labels = y['emotion'].permute(1, 0, 2)\n","                mask = x['VAD_mask'].T\n","                #labels_i = y['emotion'][i][torch.nonzero(x['VAD_mask'][i].logical_not()).squeeze()]\n","                #predics_i = y_hat[i][torch.nonzero(x['VAD_mask'][i].logical_not()).squeeze()]\n","                #y_hat = y_hat.view(-1, y_hat.size(2))\n","                #labels = y['emotion'].view(-1, y['emotion'].size(2))\n","                #padding_idxs = torch.any(labels != 0, dim=1)\n","                #labels = labels[padding_idxs]\n","                #predics = y_hat[padding_idxs]\n","                #print(predics, '\\n', labels, '\\n\\n')\n","                F1score = torchmetrics.classification.MulticlassF1Score(num_classes=7).to(device)\n","                    #print(predics_i, '\\n', labels_i, '\\n\\n')\n","                y_hat = y_hat.view(-1, y_hat.size(2))\n","                labels = y['emotion'].view(-1, y['emotion'].size(2))\n","                padding_idxs = torch.any(labels != 0, dim=1)\n","                labels = labels[padding_idxs]\n","                predics = y_hat[padding_idxs]\n","                print(predics, '\\n', labels, '\\n\\n')\n","                loss_mean = criterion(predics, labels)  # Acumula la pérdida en cada iteración\n","                score_mean = F1score(predics, labels)\n","                \n","                self.log('Train_Loss', loss_mean, on_epoch=True, prog_bar=True, logger=True)\n","                self.log('F1Score-Train', score_mean, on_epoch=True, prog_bar=True, logger=True)\n","\n","                return loss_mean  # Devuelve el promedio de la pérdida\n","\n","        \n","        def validation_step(self, batch, batch_idx):\n","                x, y = batch\n","                y_hat = self(x)\n","                predics = y_hat\n","                labels = y['emotion'].permute(1, 0, 2)\n","                mask = x['VAD_mask'].T\n","                #labels_i = y['emotion'][i][torch.nonzero(x['VAD_mask'][i].logical_not()).squeeze()]\n","                #predics_i = y_hat[i][torch.nonzero(x['VAD_mask'][i].logical_not()).squeeze()]\n","                #y_hat = y_hat.view(-1, y_hat.size(2))\n","                #labels = y['emotion'].view(-1, y['emotion'].size(2))\n","                #padding_idxs = torch.any(labels != 0, dim=1)\n","                #labels = labels[padding_idxs]\n","                #predics = y_hat[padding_idxs]\n","                #print(predics, '\\n', labels, '\\n\\n')\n","                F1score = torchmetrics.classification.MulticlassF1Score(num_classes=7).to(device)\n","                    #print(predics_i, '\\n', labels_i, '\\n\\n')\n","                y_hat = y_hat.view(-1, y_hat.size(2))\n","                labels = y['emotion'].view(-1, y['emotion'].size(2))\n","                padding_idxs = torch.any(labels != 0, dim=1)\n","                labels = labels[padding_idxs]\n","                predics = y_hat[padding_idxs]\n","                loss_mean = criterion(predics, labels)  # Acumula la pérdida en cada iteración\n","                score_mean = F1score(predics, labels)\n","                \n","                self.log('Val_Loss', loss_mean, on_epoch=True, prog_bar=True, logger=True)\n","                self.log('F1Score-Val',score_mean, on_epoch=True, prog_bar=True, logger=True)\n","    \n","\n","                return loss_mean  # Devuelve el promedio de la pérdida\n","        \n","        def configure_optimizers(self):\n","                optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n","                #lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.05)\n","                return optimizer#, [lr_scheduler]\n","    \n","        def test_step(self, batch, batch_idx):\n","                x, y = batch\n","                y_hat = self(x)\n","                predics = y_hat\n","                labels = y['emotion'].permute(1, 0, 2)\n","                mask = x['VAD_mask'].T\n","                #labels_i = y['emotion'][i][torch.nonzero(x['VAD_mask'][i].logical_not()).squeeze()]\n","                #predics_i = y_hat[i][torch.nonzero(x['VAD_mask'][i].logical_not()).squeeze()]\n","                #y_hat = y_hat.view(-1, y_hat.size(2))\n","                #labels = y['emotion'].view(-1, y['emotion'].size(2))\n","                #padding_idxs = torch.any(labels != 0, dim=1)\n","                #labels = labels[padding_idxs]\n","                #predics = y_hat[padding_idxs]\n","                #print(predics, '\\n', labels, '\\n\\n')\n","                F1score = torchmetrics.classification.MulticlassF1Score(num_classes=7).to(device)\n","                    #print(predics_i, '\\n', labels_i, '\\n\\n')\n","                y_hat = y_hat.view(-1, y_hat.size(2))\n","                labels = y['emotion'].view(-1, y['emotion'].size(2))\n","                padding_idxs = torch.any(labels != 0, dim=1)\n","                labels = labels[padding_idxs]\n","                predics = y_hat[padding_idxs]\n","                loss_mean = criterion(predics, labels)  # Acumula la pérdida en cada iteración\n","                score_mean = F1score(predics, labels)\n","                self.log('Test_Loss', loss_mean, on_epoch=True, prog_bar=True, logger=True)\n","                self.log('F1Score-Test', score_mean, on_epoch=True, prog_bar=True, logger=True)\n","                return loss_mean"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T16:04:56.974329Z","iopub.status.busy":"2024-01-23T16:04:56.973765Z","iopub.status.idle":"2024-01-23T16:04:57.100372Z","shell.execute_reply":"2024-01-23T16:04:57.099532Z","shell.execute_reply.started":"2024-01-23T16:04:56.974301Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/sgarc/SemEval-2023-Task-10/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]}],"source":["criterion = nn.CrossEntropyLoss()\n","model = FinalModel(\n","        input_size_1, hidden_size_1, num_layers_1, output_size_1,      # module 1\n","        input_size_2, n_head_2, dm_ff_2, dp_2, num_layers_2, act_fn_2, # module 2\n","        input_size_3, n_head_3, dm_ff_3, dp_3, num_layers_3, act_fn_3, # module 3\n","        input_size_4, hidden_size_4, num_layers_4, output_size_4,      # module 4\n","        input_size_5, n_head_5, dm_ff_5, dp_5, num_layers_5, act_fn_5, # module 5\n","        input_size_6, hidden_size_6, num_layers_6, output_size_6,      # module 6\n","        fc1_out, fc2_out, fc3_out, fc4_out, fc5_out, fc6_out, fc7_out, dp=0.2 ,criterion=criterion)# masking = False            # final Model parameters\n","        "]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T16:04:57.103017Z","iopub.status.busy":"2024-01-23T16:04:57.102356Z","iopub.status.idle":"2024-01-23T16:05:02.407127Z","shell.execute_reply":"2024-01-23T16:05:02.405839Z","shell.execute_reply.started":"2024-01-23T16:04:57.102980Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","Loading `train_dataloader` to estimate number of stepping batches.\n","/home/sgarc/SemEval-2023-Task-10/venv/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\n","  | Name           | Type                  | Params\n","---------------------------------------------------------\n","0 | criterion      | CrossEntropyLoss      | 0     \n","1 | module2        | Module2TransformerEnc | 27.6 M\n","2 | module3        | Module3TransformerEnc | 175 K \n","3 | fc1            | Linear                | 621 K \n","4 | classification | Sequential            | 1.1 M \n","5 | lstm           | LSTM                  | 23.7 K\n","---------------------------------------------------------\n","29.5 M    Trainable params\n","0         Non-trainable params\n","29.5 M    Total params\n","117.972   Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["trainer = L.Trainer()\n","trainer.fit(model, train_loader, val_loader)"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2024-01-23T16:05:02.419616Z","iopub.status.busy":"2024-01-23T16:05:02.419017Z","iopub.status.idle":"2024-01-23T16:05:05.251100Z","shell.execute_reply":"2024-01-23T16:05:05.249738Z","shell.execute_reply.started":"2024-01-23T16:05:02.419565Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc05202ee3544921bb9dec36e976fc45","version_major":2,"version_minor":0},"text/plain":["Testing: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">       F1Score-Test        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6715450286865234     </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         Test_Loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.7284884452819824     </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m      F1Score-Test       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6715450286865234    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m        Test_Loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.7284884452819824    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"}],"source":["trainer.test(model, dataloaders=train_loader)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4213823,"sourceId":7269192,"sourceType":"datasetVersion"},{"datasetId":4261406,"sourceId":7339725,"sourceType":"datasetVersion"},{"datasetId":4341228,"sourceId":7457937,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
