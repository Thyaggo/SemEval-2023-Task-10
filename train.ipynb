{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgarc/SemEval-2023-Task-10/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device =>  cpu  torch  2.1.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import lightning as L\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# Scoring\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device => \",device, ' torch ', torch.__version__)\n",
    "\n",
    "\n",
    "# hyper parameters\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "#@title Hyper Parameters { display-mode: \"both\" }\n",
    "\n",
    "EPOCHS             = 20\n",
    "MAX_NO_OF_SPEAKERS = 8\n",
    "MAX_DIALOGUE_LEN   = 33\n",
    "original_labels    = ['abuse', 'adoration', 'annoyance', 'awkwardness', 'benefit', 'boredom', 'calmness', 'challenge', 'cheer', 'confusion', 'curiosity', 'desire', 'excitement', 'guilt', 'horror', 'humour', 'impressed', 'loss', 'nervousness', 'nostalgia', 'pain', 'relief', 'satisfaction', 'scold', 'shock', 'sympathy', 'threat']\n",
    "train_count        = [31, 190, 1051, 880, 220, 78, 752, 214, 534, 486, 545, 180, 867, 216, 280, 153, 257, 351, 398, 65, 36, 173, 136, 94, 372, 209, 263]\n",
    "\n",
    "EMOTIONS           = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "# DataLoader Hyperparamaters\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Module 1 hyperparamaters(speaker_specific_emotion_sequence) : GRU n-n\n",
    "input_size_1  = 7\n",
    "hidden_size_1 = 10 \n",
    "num_layers_1  = 2 \n",
    "output_size_1 = 10\n",
    "\n",
    "\n",
    "# Module 2 hyperparamaters(utterance_context) : Transformer Enc\n",
    "input_size_2 = 768\n",
    "n_head_2     = 4\n",
    "dm_ff_2      = 2048\n",
    "dp_2         = 0.2\n",
    "num_layers_2 = 4 \n",
    "act_fn_2     = 'relu'\n",
    "\n",
    "# Module 3 hyperparamaters(speaker_context) : Transformer Enc\n",
    "input_size_3 = 8\n",
    "n_head_3     = 4\n",
    "dm_ff_3      = 2048\n",
    "dp_3         = 0.2\n",
    "num_layers_3 = 4 \n",
    "act_fn_3     = 'relu'\n",
    "\n",
    "# Module 4 hyperparamaters(global_emotion_sequence) : GRU\n",
    "input_size_4  = 7\n",
    "hidden_size_4 = 10 \n",
    "num_layers_4  = 2 \n",
    "output_size_4 = 7\n",
    "\n",
    "# Module 5 hyperparamaters(valence) : Transformer Enc\n",
    "input_size_5 = 3\n",
    "n_head_5     = 3\n",
    "dm_ff_5      = 2048\n",
    "dp_5         = 0.2\n",
    "num_layers_5 = 4 \n",
    "act_fn_5     = 'relu'\n",
    "\n",
    "# Module 6 hyperparamaters(speaker_specific_valence_sequence) : GRU\n",
    "input_size_6  = 3\n",
    "hidden_size_6 = 10\n",
    "num_layers_6  = 2\n",
    "output_size_6 = 10\n",
    "\n",
    "# Final Model Hyperparamerters:\n",
    "fc1_out = 800\n",
    "fc2_out = 800\n",
    "fc3_out = 400\n",
    "fc4_out = 100\n",
    "fc5_out = 1\n",
    "\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_df.pkl', 'rb') as f:\n",
    "    train_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemEvalDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.len = len(self.data)\n",
    "        print(list(train_df.columns))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        dict_x = {}\n",
    "        dict_x['speaker'] = torch.tensor(self.data['speakers'][index], dtype=torch.float64)\n",
    "        dict_x['emotion'] = torch.tensor(self.data['emotions'][index], dtype=torch.float64)\n",
    "        dict_x['sentence_embeddings'] = torch.tensor(self.data['sentence_embeddings'][index], dtype=torch.float64)\n",
    "        dict_x['VAD'] = torch.tensor(self.data['VAD'][index], dtype=torch.float64)\n",
    "\n",
    "        dict_y = {}\n",
    "        dict_y['triggers'] =  torch.tensor(self.data['triggers'][index], dtype=torch.float64)\n",
    "\n",
    "        return dict_x, dict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['episode', 'speakers', 'emotions', 'utterances', 'triggers', 'sentence_embeddings', 'VAD']\n"
     ]
    }
   ],
   "source": [
    "dataset = SemEvalDataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "total_size = len(dataset)\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = int(total_size * val_ratio)\n",
    "\n",
    "# Dividir el conjunto de datos\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MELDCollate:\n",
    "    def __init__(self, pad_value = 0):\n",
    "        self.pad_value = pad_value\n",
    "    def __call__(self, batch):\n",
    "        speaker             = pad_sequence([item[0]['speaker'] for item in batch], batch_first = True)\n",
    "        emotion             = pad_sequence([item[0]['emotion'] for item in batch], batch_first = True)\n",
    "        sentence_embeddings = pad_sequence([item[0]['sentence_embeddings'] for item in batch], batch_first = True)\n",
    "        VAD             = pad_sequence([item[0]['VAD'] for item in batch], batch_first = True)\n",
    "        # print('\\noriginal list : ',[item[0]['speaker'] for item in batch], '\\n\\npadded list : ', speaker)\n",
    "        labels              = pad_sequence([item[1]['triggers'] for item in batch], batch_first = True)\n",
    "\n",
    "        dict_x = { 'speaker': speaker, 'emotion':emotion,  'sentence_embeddings':sentence_embeddings, 'VAD':VAD}\n",
    "        dict_y = {'labels': labels}\n",
    "\n",
    "        return dict_x, dict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader  = DataLoader(dataset = train_data, batch_size = 64, shuffle=True, collate_fn= MELDCollate())\n",
    "val_loader    = DataLoader(dataset = val_data, batch_size = 64, shuffle=True, collate_fn= MELDCollate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired_batch_index = 4\n",
    "# for i, batch in enumerate(train_loader):\n",
    "#     if i == desired_batch_index:\n",
    "#         # 'batch' contendrá el batch en el índice especificado\n",
    "#         print(f\"Batch {i}:\")\n",
    "#         print(batch[1]['labels'].shape)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91065/3579761553.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  dict_x['sentence_embeddings'] = torch.tensor(self.data['sentence_embeddings'][index], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "tesst = train_data.__getitem__(1)\n",
    "print(tesst[0]['sentence_embeddings'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module6GRU(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, output_size):\n",
    "        super(Module6GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Since there are maximum of 8 speakers in a dialogue, so we decided to make 8 GRUs one for each speaker.\n",
    "        self.gru_list= []\n",
    "        for id in range(MAX_NO_OF_SPEAKERS):\n",
    "            self.gru_list.append(nn.GRU(input_size, hidden_size, num_layers, batch_first = True))\n",
    "        self.gru_modules = nn.ModuleList(self.gru_list)\n",
    "\n",
    "    def valence_specific(self, valence, speaker):\n",
    "        speaker = speaker.unique(dim=0, return_inverse=True)[1]\n",
    "        # Asegúrate de que el tensor de padding esté en el mismo dispositivo que 'speaker' y 'valence'.\n",
    "        padding_tensor = torch.zeros(valence.size(1), device=speaker.device)\n",
    "        # Ahora utiliza el tensor de padding que está en el dispositivo correcto.\n",
    "        return [torch.where(speaker.unsqueeze(1) == i, valence, padding_tensor) for i in speaker.unique()], speaker.unique()\n",
    "\n",
    "    def applyGRU(self, speaker_valence, sp_idx , seq_len):\n",
    "        speaker_output = torch.zeros(seq_len, self.output_size, device = device)\n",
    "        for sp_idx, valence in zip(sp_idx, speaker_valence):\n",
    "            # Verificar si hay alguna entrada para este hablante\n",
    "            if valence.nonzero().size(0) == 0:\n",
    "                continue\n",
    "\n",
    "            if sp_idx >= 8:\n",
    "                print(sp_idx)\n",
    "                # Manejar el error o ajustar sp_idx aquí\n",
    "                sp_idx = 7\n",
    "            # Asegúrate de que valence tenga al menos dos dimensiones\n",
    "\n",
    "            # Inicializar h0 como un tensor 2D\n",
    "            h0 = torch.zeros(self.num_layers, self.hidden_size, device = device)  # Ahora h0 es 2D\n",
    "            out, _ = self.gru_modules[sp_idx](valence, h0)\n",
    "\n",
    "            # Rellenar speaker_output con la salida correspondiente\n",
    "            for uid, output in enumerate(out.squeeze(0)):\n",
    "                speaker_output[uid] = output\n",
    "\n",
    "        return speaker_output\n",
    "\n",
    "\n",
    "    def forward(self, x, speakers):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len    = x.size(1)\n",
    "        outputs = []\n",
    "        for i in range(batch_size):\n",
    "            speaker_specific, sp_idx = self.valence_specific(x[i], speakers[i])\n",
    "            out = self.applyGRU(speaker_specific, sp_idx ,seq_len)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        final_output = torch.cat([outputs[i].unsqueeze(2) for i in range(len(outputs))], 2).permute(2,0,1)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module5TransformerEnc(nn.Module):\n",
    "    # S, N, E : (seq_len, batch_size, input/embedding_size)\n",
    "    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n",
    "        super(Module5TransformerEnc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src): # src_shape : (S, N, E)\n",
    "        pad_value = torch.zeros(self.input_size).to(device)\n",
    "        # pad_value shape : (E), value : [0,0,0, ...]\n",
    "        src = src.transpose(0,1)\n",
    "        \n",
    "        # src_shape : (N, S, E)\n",
    "\n",
    "        src_mask = torch.all(torch.eq(src,pad_value),2)\n",
    "        print(src)\n",
    "        print(src_mask)\n",
    "        # src_mask shape : (N, S), value : for each batch, it is contains seq_len sized tensors and contains true for pad and false for others\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: seq_len, batch_size, input_size \n",
    "        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n",
    "        x_mask = self.make_src_mask(x)\n",
    "        out = self.encoder(x)  \n",
    "        # out shape : (S, N, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module4GRU(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, output_size):\n",
    "        super(Module4GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc  = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('x shape : ', x.shape)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        \n",
    "        # shape of out :  (N, seq_len, hidden_size)     (torch.Size([10, 33, 8])) \n",
    "        # shape of hn  :  (num_layers, N, hidden_size)     (torch.Size([2, 10, 8]))\n",
    "        # shape of hn  :  (N, num_layers, hidden_size) and then flatten it to (N, num_layers*hiddem_size) 3D to 2D\n",
    "        output = self.fc(out)\n",
    "        # shape of output : [N, output_size]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module3TransformerEnc(nn.Module):\n",
    "    # S, N, E : (seq_len, batch_size, input/embedding_size)\n",
    "    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n",
    "        super(Module3TransformerEnc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src): # src_shape : (S, N, E)\n",
    "        pad_value = torch.zeros(self.input_size).to(device)\n",
    "        # pad_value shape : (E), value : [0,0,0, ...]\n",
    "        src = src.transpose(0,1)\n",
    "        # src_shape : (N, S, E)\n",
    "\n",
    "        src_mask = torch.all(torch.eq(src,pad_value),2)\n",
    "        \n",
    "        # src_mask shape : (N, S), value : for each batch, it is contains seq_len sized tensors and contains true for pad and false for others\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: seq_len, batch_size, input_size \n",
    "        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n",
    "        x_mask = self.make_src_mask(x)\n",
    "        out = self.encoder(x, src_key_padding_mask = x_mask)  \n",
    "        # out shape : (S, N, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module2TransformerEnc(nn.Module):\n",
    "    # S, N, E : (seq_len, batch_size, input/embedding_size)\n",
    "    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n",
    "        super(Module2TransformerEnc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src): # src_shape : (S, N, E)\n",
    "        pad_value = torch.zeros(self.input_size)\n",
    "        # pad_value shape : (E), value : [0,0,0, ...]\n",
    "        src = src.transpose(0,1)\n",
    "        # src_shape : (N, S, E)\n",
    "\n",
    "        src_mask = torch.all(torch.eq(src,pad_value),2)\n",
    "        \n",
    "        # src_mask shape : (N, S), value : for each batch, it is contains seq_len sized tensors and contains true for pad and false for others\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: seq_len, batch_size, input_size \n",
    "        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n",
    "        x_mask = self.make_src_mask(x)\n",
    "        out = self.encoder(x, src_key_padding_mask = x_mask)  \n",
    "        # out shape : (S, N, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module1GRU(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, output_size):\n",
    "        super(Module1GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Since there are maximum of 8 speakers in a dialogue, so we decided to make 8 GRUs one for each speaker.\n",
    "        self.gru_list= []\n",
    "        for id in range(MAX_NO_OF_SPEAKERS):\n",
    "            self.gru_list.append(nn.GRU(input_size, hidden_size, num_layers, batch_first = True))\n",
    "        self.gru_modules = nn.ModuleList(self.gru_list)\n",
    "        # self.fc  = nn.Linear(num_layers*hidden_size, output_size)\n",
    "            \n",
    "    \n",
    "    def segregateEmotions(self, emotions, speakers):\n",
    "        speaker_specific = []\n",
    "        utt_id = []\n",
    "        for i in range(MAX_NO_OF_SPEAKERS):\n",
    "            speaker_tensor = torch.zeros(MAX_NO_OF_SPEAKERS, dtype = float)\n",
    "            speaker_tensor[i] = 1\n",
    "            emo = emotions[torch.nonzero((speakers == speaker_tensor).sum(dim=1) == speakers.size(1))].permute(1,0,2)\n",
    "            if(emo.size(1) == 0):\n",
    "                continue\n",
    "            utt_id.append(torch.nonzero((speakers == speaker_tensor).sum(dim=1) == speakers.size(1))[0])\n",
    "            speaker_specific.append(emo)\n",
    "#             print('\\n emo size : ',emo.size())\n",
    "#         print('\\n emo concat size : ',speaker_specific, utt_id)\n",
    "        return speaker_specific, utt_id\n",
    "    \n",
    "    def applyGRU(self, speaker_specific, utt_id, seq_len):\n",
    "        speaker_output = torch.zeros(seq_len, self.output_size)  \n",
    "        for sp_idx in range(len(utt_id)):\n",
    "            h0 = torch.zeros(self.num_layers, 1, self.hidden_size).to(device)\n",
    "            out, hn = self.gru_list[sp_idx](speaker_specific[sp_idx], h0)\n",
    "            for uid in range(utt_id[sp_idx].size(0)):\n",
    "                speaker_output[utt_id[sp_idx][uid]] = out[0][uid].clone()\n",
    "        return speaker_output\n",
    "\n",
    "    def forward(self, x, speakers):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len    = x.size(1)\n",
    "        outputs = []\n",
    "        print('x shape : ', x.shape)\n",
    "        for i in range(batch_size):\n",
    "            speaker_specific, utt_id = self.segregateEmotions(x[i], speakers[i])\n",
    "            out = self.applyGRU(speaker_specific, utt_id, seq_len)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        final_output = torch.cat([outputs[i].unsqueeze(2) for i in range(len(outputs))], 2).permute(2,0,1)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalModel(L.LightningModule):\n",
    "        def __init__(self, \n",
    "                input_size_1, hidden_size_1, num_layers_1, output_size_1,      # module 1    \n",
    "                input_size_2, n_head_2, dm_ff_2, dp_2, num_layers_2, act_fn_2, # module 2\n",
    "                input_size_3, n_head_3, dm_ff_3, dp_3, num_layers_3, act_fn_3, # module 3\n",
    "                input_size_4, hidden_size_4, num_layers_4, output_size_4,      # module 4\n",
    "                input_size_5, n_head_5, dm_ff_5, dp_5, num_layers_5, act_fn_5, # module 5\n",
    "                input_size_6, hidden_size_6, num_layers_6, output_size_6,      # module 6\n",
    "                fc1_out, fc2_out, fc3_out, fc4_out, fc5_out, dp, #masking = False            # final Model parameters\n",
    "                ):\n",
    "                super(FinalModel, self).__init__()\n",
    "\n",
    "                #self.masking = masking\n",
    "\n",
    "                self.module1 = Module1GRU(input_size = input_size_1, num_layers = num_layers_1, hidden_size = hidden_size_1, output_size = output_size_1)\n",
    "                self.module2 = Module2TransformerEnc(input_size = input_size_2, n_head = n_head_2, dim_ff = dm_ff_2, dp = dp_2, num_layers = num_layers_2, act_fn = act_fn_2)\n",
    "                self.module3 = Module3TransformerEnc(input_size = input_size_3, n_head = n_head_3, dim_ff = dm_ff_3, dp = dp_3, num_layers = num_layers_3, act_fn = act_fn_3)\n",
    "                self.module4 = Module4GRU(input_size = input_size_4, num_layers = num_layers_4, hidden_size = hidden_size_4, output_size = output_size_4)\n",
    "                self.module5 = Module5TransformerEnc(input_size = input_size_5, n_head = n_head_5, dim_ff = dm_ff_5, dp = dp_5, num_layers = num_layers_5, act_fn = act_fn_5)\n",
    "                self.module6 = Module6GRU(input_size = input_size_6, num_layers = num_layers_6, hidden_size = hidden_size_6, output_size = output_size_6)\n",
    "\n",
    "                \n",
    "                self.fc1 = nn.Linear(input_size_2+input_size_3+input_size_5, fc1_out)\n",
    "                self.classification = nn.Sequential(\n",
    "                        nn.Linear(output_size_1 + fc1_out + output_size_4 + output_size_6, fc2_out),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dp), \n",
    "                        nn.Linear(fc2_out, fc3_out),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dp),\n",
    "                        nn.Linear(fc3_out, fc4_out),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dp),\n",
    "                        nn.Linear(fc4_out, fc5_out),\n",
    "                        nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "        def forward(self, x):\n",
    "                speaker = x['speaker']\n",
    "                emotion = x['emotion']\n",
    "                sentence_embeddings = x['sentence_embeddings']\n",
    "                valence = x['VAD']\n",
    "\n",
    "                out1 = self.module1(emotion, speaker)\n",
    "                out2 = self.module2(sentence_embeddings)\n",
    "                out3 = self.module3(speaker)\n",
    "                out4 = self.module4(emotion)\n",
    "                out5 = self.module5(valence)\n",
    "                out6 = self.module6(valence, speaker)\n",
    "\n",
    "                out146 = torch.cat((out1, out4, out6), 2)\n",
    "                out234 = torch.cat((out2, out3, out5), 2)\n",
    "                out234 = F.relu(self.fc1(out234))\n",
    "                \n",
    "                out123456 = torch.cat((out146, out234), 2)\n",
    "\n",
    "                out_tensor = torch.zeros(out123456.size(0), out123456.size(1))\n",
    "\n",
    "                for batch_idx in range(out123456.size(0)):\n",
    "                        for seq_idx in range(out123456.size(1)):\n",
    "                                # Obtén la salida de la red para el punto de datos actual\n",
    "                                op = self.classification(out123456[batch_idx][seq_idx])\n",
    "\n",
    "                                # Asumiendo que 'op' es un tensor unidimensional con la salida de la clasificación\n",
    "                                # Aquí, seleccionamos el primer elemento ya que op debería ser un scalar después de la sigmoid\n",
    "                                out_tensor[batch_idx, seq_idx] = op  # Ajusta esto según la estructura real de 'op'\n",
    "\n",
    "                # Retorna el tensor de salida en lugar de la lista\n",
    "                return out_tensor\n",
    "\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "                x, y = batch\n",
    "                y_hat = self(x)\n",
    "                print('y_hat shape : ', y_hat)\n",
    "                print('y shape : ', y['labels'])\n",
    "                loss = F.binary_cross_entropy(y_hat, y['labels'])\n",
    "                self.log('train_loss', loss)\n",
    "                return loss\n",
    "        \n",
    "        def validation_step(self, batch, batch_idx):\n",
    "                x, y = batch\n",
    "                y_hat = self(x)\n",
    "                loss = F.binary_cross_entropy(y_hat, y['labels'])\n",
    "                self.log('val_loss', loss)\n",
    "                return loss\n",
    "        \n",
    "        def configure_optimizers(self):\n",
    "                optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "                scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=5)\n",
    "                return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "        \n",
    "        def test_step(self, batch, batch_idx):\n",
    "                x, y = batch\n",
    "                y_hat = self(x)\n",
    "                loss = F.binary_cross_entropy(y_hat, y['labels'])\n",
    "                self.log('test_loss', loss)\n",
    "                return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgarc/SemEval-2023-Task-10/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = FinalModel(\n",
    "        input_size_1, hidden_size_1, num_layers_1, output_size_1,      # module 1    \n",
    "        input_size_2, n_head_2, dm_ff_2, dp_2, num_layers_2, act_fn_2, # module 2\n",
    "        input_size_3, n_head_3, dm_ff_3, dp_3, num_layers_3, act_fn_3, # module 3\n",
    "        input_size_4, hidden_size_4, num_layers_4, output_size_4,      # module 4\n",
    "        input_size_5, n_head_5, dm_ff_5, dp_5, num_layers_5, act_fn_5, # module 5\n",
    "        input_size_6, hidden_size_6, num_layers_6, output_size_6,      # module 6\n",
    "        fc1_out, fc2_out, fc3_out, fc4_out, fc5_out, dp=0.2, #masking = False            # final Model parameters\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/sgarc/SemEval-2023-Task-10/venv/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "Missing logger folder: /home/sgarc/SemEval-2023-Task-10/lightning_logs\n",
      "\n",
      "  | Name           | Type                  | Params\n",
      "---------------------------------------------------------\n",
      "0 | module1        | Module1GRU            | 9.8 K \n",
      "1 | module2        | Module2TransformerEnc | 27.6 M\n",
      "2 | module3        | Module3TransformerEnc | 175 K \n",
      "3 | module4        | Module4GRU            | 1.3 K \n",
      "4 | module5        | Module5TransformerEnc | 72.0 K\n",
      "5 | module6        | Module6GRU            | 8.9 K \n",
      "6 | fc1            | Linear                | 624 K \n",
      "7 | classification | Sequential            | 1.0 M \n",
      "---------------------------------------------------------\n",
      "29.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "29.5 M    Total params\n",
      "117.939   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]x shape :  torch.Size([64, 24, 7])\n",
      "x shape :  torch.Size([64, 24, 7])\n",
      "tensor([[[0.1758, 0.1437, 0.1508],\n",
      "         [0.3985, 0.2933, 0.3520],\n",
      "         [0.2538, 0.1587, 0.1857],\n",
      "         ...,\n",
      "         [0.1586, 0.1041, 0.1254],\n",
      "         [0.1515, 0.1343, 0.1218],\n",
      "         [0.1493, 0.1170, 0.2263]],\n",
      "\n",
      "        [[0.2470, 0.1123, 0.2043],\n",
      "         [0.3603, 0.2205, 0.2831],\n",
      "         [0.2983, 0.1325, 0.2503],\n",
      "         ...,\n",
      "         [0.2584, 0.2145, 0.2610],\n",
      "         [0.0677, 0.0603, 0.0353],\n",
      "         [0.1737, 0.1400, 0.2380]],\n",
      "\n",
      "        [[0.2172, 0.1809, 0.1671],\n",
      "         [0.2390, 0.1360, 0.1762],\n",
      "         [0.1723, 0.1376, 0.1753],\n",
      "         ...,\n",
      "         [0.1367, 0.2050, 0.1637],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]]])\n",
      "tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False,  True,  True],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True]])\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:03<00:03,  0.26it/s]x shape :  torch.Size([64, 23, 7])\n",
      "x shape :  torch.Size([64, 23, 7])\n",
      "tensor([[[0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.3922, 0.2360, 0.2925],\n",
      "         ...,\n",
      "         [0.4922, 0.3947, 0.3367],\n",
      "         [0.4790, 0.3325, 0.4050],\n",
      "         [0.2209, 0.1369, 0.2014]],\n",
      "\n",
      "        [[0.2488, 0.1888, 0.2212],\n",
      "         [0.1794, 0.1136, 0.1550],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.1870, 0.0967, 0.1245],\n",
      "         [0.3901, 0.2792, 0.3488]],\n",
      "\n",
      "        [[0.0029, 0.1311, 0.0807],\n",
      "         [0.1635, 0.0795, 0.1298],\n",
      "         [0.3193, 0.1177, 0.1753],\n",
      "         ...,\n",
      "         [0.1938, 0.1688, 0.1150],\n",
      "         [0.0311, 0.0567, 0.0544],\n",
      "         [0.3187, 0.1779, 0.2687]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]]])\n",
      "tensor([[ True,  True, False,  ..., False, False, False],\n",
      "        [False, False,  True,  ...,  True, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True]])\n",
      "Epoch 0:   0%|          | 0/50 [00:00<?, ?it/s]                            x shape :  torch.Size([64, 19, 7])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer()\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
