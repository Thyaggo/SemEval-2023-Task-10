{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device =>  cpu  torch  2.1.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import lightning as L\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# Scoring\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device => \",device, ' torch ', torch.__version__)\n",
    "\n",
    "\n",
    "# hyper parameters\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "#@title Hyper Parameters { display-mode: \"both\" }\n",
    "\n",
    "EPOCHS             = 20\n",
    "MAX_NO_OF_SPEAKERS = 231\n",
    "MAX_DIALOGUE_LEN   = 33\n",
    "original_labels    = ['abuse', 'adoration', 'annoyance', 'awkwardness', 'benefit', 'boredom', 'calmness', 'challenge', 'cheer', 'confusion', 'curiosity', 'desire', 'excitement', 'guilt', 'horror', 'humour', 'impressed', 'loss', 'nervousness', 'nostalgia', 'pain', 'relief', 'satisfaction', 'scold', 'shock', 'sympathy', 'threat']\n",
    "train_count        = [31, 190, 1051, 880, 220, 78, 752, 214, 534, 486, 545, 180, 867, 216, 280, 153, 257, 351, 398, 65, 36, 173, 136, 94, 372, 209, 263]\n",
    "\n",
    "EMOTIONS           = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "# DataLoader Hyperparamaters\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Module 1 hyperparamaters(speaker_specific_emotion_sequence) : GRU n-n\n",
    "input_size_1  = 7\n",
    "hidden_size_1 = 10 \n",
    "num_layers_1  = 2 \n",
    "output_size_1 = 10\n",
    "\n",
    "\n",
    "# Module 2 hyperparamaters(utterance_context) : Transformer Enc\n",
    "input_size_2 = 768\n",
    "n_head_2     = 4\n",
    "dm_ff_2      = 2048\n",
    "dp_2         = 0.2\n",
    "num_layers_2 = 4 \n",
    "act_fn_2     = 'relu'\n",
    "\n",
    "# Module 3 hyperparamaters(speaker_context) : Transformer Enc\n",
    "input_size_3 = 231\n",
    "n_head_3     = 3\n",
    "dm_ff_3      = 2048\n",
    "dp_3         = 0.2\n",
    "num_layers_3 = 4 \n",
    "act_fn_3     = 'relu'\n",
    "\n",
    "# Module 4 hyperparamaters(global_emotion_sequence) : GRU\n",
    "input_size_4  = 7\n",
    "hidden_size_4 = 10 \n",
    "num_layers_4  = 2 \n",
    "output_size_4 = 7\n",
    "\n",
    "# Module 5 hyperparamaters(valence) : Transformer Enc\n",
    "input_size_5 = 69\n",
    "n_head_5     = 3\n",
    "dm_ff_5      = 2048\n",
    "dp_5         = 0.2\n",
    "num_layers_5 = 4 \n",
    "act_fn_5     = 'relu'\n",
    "\n",
    "# Module 6 hyperparamaters(speaker_specific_valence_sequence) : GRU\n",
    "input_size_6  = 1\n",
    "hidden_size_6 = 10\n",
    "num_layers_6  = 2\n",
    "output_size_6 = 10\n",
    "\n",
    "# Final Model Hyperparamerters:\n",
    "fc1_out = 800\n",
    "fc2_out = 800\n",
    "fc3_out = 400\n",
    "fc4_out = 100\n",
    "fc5_out = 1\n",
    "\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_df.pkl', 'rb') as f:\n",
    "    train_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemEvalDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.len = len(self.data)\n",
    "        print(list(train_df.columns))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        dict_x = {}\n",
    "        dict_x['speaker'] = torch.tensor(self.data['speakers'][index], dtype=torch.float32)\n",
    "        dict_x['emotion'] = torch.tensor(self.data['emotions'][index], dtype=torch.float32)\n",
    "        dict_x['sentence_embeddings'] = torch.tensor(self.data['sentence_embeddings'][index], dtype=torch.float32)\n",
    "        dict_x['valence'] = torch.tensor(self.data['valence'][index], dtype=torch.float32)\n",
    "\n",
    "        dict_y = {}\n",
    "        dict_y['triggers'] =  torch.tensor(self.data['triggers'][index], dtype=torch.float32)\n",
    "\n",
    "        return dict_x, dict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['episode', 'speakers', 'emotions', 'utterances', 'triggers', 'sentence_embeddings', 'valence']\n"
     ]
    }
   ],
   "source": [
    "dataset = SemEvalDataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "total_size = len(dataset)\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = int(total_size * val_ratio)\n",
    "\n",
    "# Dividir el conjunto de datos\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MELDCollate:\n",
    "    def __init__(self, pad_value = 0):\n",
    "        self.pad_value = pad_value\n",
    "    def __call__(self, batch):\n",
    "        speaker             = pad_sequence([item[0]['speaker'] for item in batch], batch_first = True)\n",
    "        emotion             = pad_sequence([item[0]['emotion'] for item in batch], batch_first = True)\n",
    "        sentence_embeddings = pad_sequence([item[0]['sentence_embeddings'] for item in batch], batch_first = True)\n",
    "        valence             = pad_sequence([item[0]['valence'] for item in batch], batch_first = True)\n",
    "        # print('\\noriginal list : ',[item[0]['speaker'] for item in batch], '\\n\\npadded list : ', speaker)\n",
    "        labels              = pad_sequence([item[1]['triggers'] for item in batch], batch_first = True)\n",
    "\n",
    "        dict_x = { 'speaker': speaker, 'emotion':emotion,  'sentence_embeddings':sentence_embeddings, 'valence':valence}\n",
    "        dict_y = {'labels': labels}\n",
    "\n",
    "        return dict_x, dict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader  = DataLoader(dataset = train_data, batch_size = 64, shuffle=True, collate_fn= MELDCollate())\n",
    "val_loader    = DataLoader(dataset = val_data, batch_size = 64, shuffle=True, collate_fn= MELDCollate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4:\n",
      "torch.Size([64, 22])\n"
     ]
    }
   ],
   "source": [
    "desired_batch_index = 4\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i == desired_batch_index:\n",
    "        # 'batch' contendrá el batch en el índice especificado\n",
    "        print(f\"Batch {i}:\")\n",
    "        print(batch[1]['labels'].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 231]) torch.Size([7, 7]) torch.Size([7, 768]) torch.Size([7, 69])\n"
     ]
    }
   ],
   "source": [
    "tesst = train_data.__getitem__(1)\n",
    "print(tesst[0]['speaker'].shape, tesst[0]['emotion'].shape, tesst[0]['sentence_embeddings'].shape, tesst[0]['valence'].shape)\n",
    "# print(tesst[0]['valence'].shape)\n",
    "# print(tesst[0]['speaker'].size())\n",
    "# test = tesst[0]['speaker'].unique(dim=0, return_inverse=True)[1]\n",
    "# [torch.where(test == i, tesst[0]['valence'].mean(1), 0) for i in test.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module6GRU(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, output_size):\n",
    "        super(Module6GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Since there are maximum of 8 speakers in a dialogue, so we decided to make 8 GRUs one for each speaker.\n",
    "        self.gru_list= []\n",
    "        for id in range(MAX_NO_OF_SPEAKERS):\n",
    "            self.gru_list.append(nn.GRU(input_size, hidden_size, num_layers, batch_first = True))\n",
    "        self.gru_modules = nn.ModuleList(self.gru_list)\n",
    "\n",
    "    def valence_specific(self, valence, speaker):\n",
    "        speaker = speaker.unique(dim = 0, return_inverse=True)[1]\n",
    "        return [torch.where(speaker == i , valence.mean(1), 0) for i in speaker.unique()]\n",
    "\n",
    "    def applyGRU(self, speaker_valence, seq_len):\n",
    "        speaker_output = torch.zeros(seq_len, self.output_size)\n",
    "\n",
    "        for sp_idx, valence in enumerate(speaker_valence):\n",
    "            # Verificar si hay alguna entrada para este hablante\n",
    "            if valence.nonzero().size(0) == 0:\n",
    "                continue\n",
    "\n",
    "            # Asegúrate de que valence tenga al menos dos dimensiones\n",
    "            valence = valence.unsqueeze(1)\n",
    "\n",
    "            # Inicializar h0 como un tensor 2D\n",
    "            h0 = torch.zeros(self.num_layers, self.hidden_size)  # Ahora h0 es 2D\n",
    "\n",
    "            out, _ = self.gru_modules[sp_idx](valence, h0)\n",
    "\n",
    "            # Rellenar speaker_output con la salida correspondiente\n",
    "            for uid, output in enumerate(out.squeeze(0)):\n",
    "                speaker_output[uid] = output\n",
    "\n",
    "        return speaker_output\n",
    "\n",
    "\n",
    "    def forward(self, x, speakers):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len    = x.size(1)\n",
    "        outputs = []\n",
    "        for i in range(batch_size):\n",
    "            speaker_specific = self.valence_specific(x[i], speakers[i])\n",
    "            out = self.applyGRU(speaker_specific, seq_len)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        final_output = torch.cat([outputs[i].unsqueeze(2) for i in range(len(outputs))], 2).permute(2,0,1)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module5TransformerEnc(nn.Module):\n",
    "    # S, N, E : (seq_len, batch_size, input/embedding_size)\n",
    "    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n",
    "        super(Module5TransformerEnc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src): # src_shape : (S, N, E)\n",
    "        pad_value = torch.zeros(self.input_size).to(device)\n",
    "        # pad_value shape : (E), value : [0,0,0, ...]\n",
    "        src = src.transpose(0,1)\n",
    "        # src_shape : (N, S, E)\n",
    "\n",
    "        src_mask = torch.all(torch.eq(src,pad_value),2)\n",
    "        \n",
    "        # src_mask shape : (N, S), value : for each batch, it is contains seq_len sized tensors and contains true for pad and false for others\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: seq_len, batch_size, input_size \n",
    "        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n",
    "        x_mask = self.make_src_mask(x)\n",
    "        out = self.encoder(x, src_key_padding_mask = x_mask)  \n",
    "        # out shape : (S, N, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module4GRU(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, output_size):\n",
    "        super(Module4GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc  = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('x shape : ', x.shape)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        \n",
    "        # shape of out :  (N, seq_len, hidden_size)     (torch.Size([10, 33, 8])) \n",
    "        # shape of hn  :  (num_layers, N, hidden_size)     (torch.Size([2, 10, 8]))\n",
    "        # shape of hn  :  (N, num_layers, hidden_size) and then flatten it to (N, num_layers*hiddem_size) 3D to 2D\n",
    "        output = self.fc(out)\n",
    "        # shape of output : [N, output_size]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module3TransformerEnc(nn.Module):\n",
    "    # S, N, E : (seq_len, batch_size, input/embedding_size)\n",
    "    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n",
    "        super(Module3TransformerEnc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src): # src_shape : (S, N, E)\n",
    "        pad_value = torch.zeros(self.input_size).to(device)\n",
    "        # pad_value shape : (E), value : [0,0,0, ...]\n",
    "        src = src.transpose(0,1)\n",
    "        # src_shape : (N, S, E)\n",
    "\n",
    "        src_mask = torch.all(torch.eq(src,pad_value),2)\n",
    "        \n",
    "        # src_mask shape : (N, S), value : for each batch, it is contains seq_len sized tensors and contains true for pad and false for others\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: seq_len, batch_size, input_size \n",
    "        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n",
    "        x_mask = self.make_src_mask(x)\n",
    "        out = self.encoder(x, src_key_padding_mask = x_mask)  \n",
    "        # out shape : (S, N, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module2TransformerEnc(nn.Module):\n",
    "    # S, N, E : (seq_len, batch_size, input/embedding_size)\n",
    "    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n",
    "        super(Module2TransformerEnc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src): # src_shape : (S, N, E)\n",
    "        pad_value = torch.zeros(self.input_size)\n",
    "        # pad_value shape : (E), value : [0,0,0, ...]\n",
    "        src = src.transpose(0,1)\n",
    "        # src_shape : (N, S, E)\n",
    "\n",
    "        src_mask = torch.all(torch.eq(src,pad_value),2)\n",
    "        \n",
    "        # src_mask shape : (N, S), value : for each batch, it is contains seq_len sized tensors and contains true for pad and false for others\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: seq_len, batch_size, input_size \n",
    "        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n",
    "        x_mask = self.make_src_mask(x)\n",
    "        out = self.encoder(x, src_key_padding_mask = x_mask)  \n",
    "        # out shape : (S, N, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module1GRU(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, output_size):\n",
    "        super(Module1GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Since there are maximum of 8 speakers in a dialogue, so we decided to make 8 GRUs one for each speaker.\n",
    "        self.gru_list= []\n",
    "        for id in range(MAX_NO_OF_SPEAKERS):\n",
    "            self.gru_list.append(nn.GRU(input_size, hidden_size, num_layers, batch_first = True))\n",
    "        self.gru_modules = nn.ModuleList(self.gru_list)\n",
    "        # self.fc  = nn.Linear(num_layers*hidden_size, output_size)\n",
    "            \n",
    "    \n",
    "    def segregateEmotions(self, emotions, speakers):\n",
    "        speaker_specific = []\n",
    "        utt_id = []\n",
    "        for i in range(MAX_NO_OF_SPEAKERS):\n",
    "            speaker_tensor = torch.zeros(MAX_NO_OF_SPEAKERS, dtype = float)\n",
    "            speaker_tensor[i] = 1\n",
    "            emo = emotions[torch.nonzero((speakers == speaker_tensor).sum(dim=1) == speakers.size(1))].permute(1,0,2)\n",
    "            if(emo.size(1) == 0):\n",
    "                continue\n",
    "            utt_id.append(torch.nonzero((speakers == speaker_tensor).sum(dim=1) == speakers.size(1))[0])\n",
    "            speaker_specific.append(emo)\n",
    "#             print('\\n emo size : ',emo.size())\n",
    "#         print('\\n emo concat size : ',speaker_specific, utt_id)\n",
    "        return speaker_specific, utt_id\n",
    "    \n",
    "    def applyGRU(self, speaker_specific, utt_id, seq_len):\n",
    "        speaker_output = torch.zeros(seq_len, self.output_size)  \n",
    "        for sp_idx in range(len(utt_id)):\n",
    "            h0 = torch.zeros(self.num_layers, 1, self.hidden_size).to(device)\n",
    "            out, hn = self.gru_list[sp_idx](speaker_specific[sp_idx], h0)\n",
    "            for uid in range(utt_id[sp_idx].size(0)):\n",
    "                speaker_output[utt_id[sp_idx][uid]] = out[0][uid].clone()\n",
    "        return speaker_output\n",
    "\n",
    "    def forward(self, x, speakers):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len    = x.size(1)\n",
    "        outputs = []\n",
    "        print('x shape : ', x.shape)\n",
    "        for i in range(batch_size):\n",
    "            speaker_specific, utt_id = self.segregateEmotions(x[i], speakers[i])\n",
    "            out = self.applyGRU(speaker_specific, utt_id, seq_len)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        final_output = torch.cat([outputs[i].unsqueeze(2) for i in range(len(outputs))], 2).permute(2,0,1)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalModel(L.LightningModule):\n",
    "        def __init__(self, \n",
    "                input_size_1, hidden_size_1, num_layers_1, output_size_1,      # module 1    \n",
    "                input_size_2, n_head_2, dm_ff_2, dp_2, num_layers_2, act_fn_2, # module 2\n",
    "                input_size_3, n_head_3, dm_ff_3, dp_3, num_layers_3, act_fn_3, # module 3\n",
    "                input_size_4, hidden_size_4, num_layers_4, output_size_4,      # module 4\n",
    "                input_size_5, n_head_5, dm_ff_5, dp_5, num_layers_5, act_fn_5, # module 5\n",
    "                input_size_6, hidden_size_6, num_layers_6, output_size_6,      # module 6\n",
    "                fc1_out, fc2_out, fc3_out, fc4_out, fc5_out, dp, #masking = False            # final Model parameters\n",
    "                ):\n",
    "                super(FinalModel, self).__init__()\n",
    "\n",
    "                #self.masking = masking\n",
    "\n",
    "                self.module1 = Module1GRU(input_size = input_size_1, num_layers = num_layers_1, hidden_size = hidden_size_1, output_size = output_size_1)\n",
    "                self.module2 = Module2TransformerEnc(input_size = input_size_2, n_head = n_head_2, dim_ff = dm_ff_2, dp = dp_2, num_layers = num_layers_2, act_fn = act_fn_2)\n",
    "                self.module3 = Module3TransformerEnc(input_size = input_size_3, n_head = n_head_3, dim_ff = dm_ff_3, dp = dp_3, num_layers = num_layers_3, act_fn = act_fn_3)\n",
    "                self.module4 = Module4GRU(input_size = input_size_4, num_layers = num_layers_4, hidden_size = hidden_size_4, output_size = output_size_4)\n",
    "                self.module5 = Module5TransformerEnc(input_size = input_size_5, n_head = n_head_5, dim_ff = dm_ff_5, dp = dp_5, num_layers = num_layers_5, act_fn = act_fn_5)\n",
    "                self.module6 = Module6GRU(input_size = input_size_6, num_layers = num_layers_6, hidden_size = hidden_size_6, output_size = output_size_6)\n",
    "\n",
    "                \n",
    "                self.sigmoid = nn.Sigmoid()\n",
    "                self.fc1 = nn.Linear(input_size_2+input_size_3+input_size_5, fc1_out)\n",
    "                self.classification = nn.Sequential(\n",
    "                        nn.Linear(output_size_1 + fc1_out + output_size_4 + output_size_6, fc2_out),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dp), \n",
    "                        nn.Linear(fc2_out, fc3_out),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dp),\n",
    "                        nn.Linear(fc3_out, fc4_out),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dp),\n",
    "                        nn.Linear(fc4_out, fc5_out),\n",
    "                        # nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "        def forward(self, x):\n",
    "                speaker = x['speaker']\n",
    "                emotion = x['emotion']\n",
    "                sentence_embeddings = x['sentence_embeddings']\n",
    "                valence = x['valence']\n",
    "\n",
    "                out1 = self.module1(emotion, speaker)\n",
    "                out2 = self.module2(sentence_embeddings)\n",
    "                out3 = self.module3(speaker)\n",
    "                out4 = self.module4(emotion)\n",
    "                out5 = self.module5(valence)\n",
    "                out6 = self.module6(valence, speaker)\n",
    "\n",
    "                out146 = torch.cat((out1, out4, out6), 2)\n",
    "                out234 = F.relu(self.fc1(torch.cat((out2, out3, out5), 2)))\n",
    "                \n",
    "                out123456 = torch.cat((out146, out234), 2)\n",
    "\n",
    "                out_tensor = torch.zeros(out123456.size(0), out123456.size(1))\n",
    "\n",
    "                for batch_idx in range(out123456.size(0)):\n",
    "                        for seq_idx in range(out123456.size(1)):\n",
    "                                # Obtén la salida de la red para el punto de datos actual\n",
    "                                op = self.classification(out123456[batch_idx][seq_idx])\n",
    "                                op = self.sigmoid(op)\n",
    "\n",
    "                                # Asumiendo que 'op' es un tensor unidimensional con la salida de la clasificación\n",
    "                                # Aquí, seleccionamos el primer elemento ya que op debería ser un scalar después de la sigmoid\n",
    "                                out_tensor[batch_idx, seq_idx] = op  # Ajusta esto según la estructura real de 'op'\n",
    "\n",
    "                # Retorna el tensor de salida en lugar de la lista\n",
    "                return out_tensor\n",
    "\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "                x, y = batch\n",
    "                y_hat = self(x)\n",
    "                print('y_hat shape : ', y_hat)\n",
    "                print('y shape : ', y['labels'])\n",
    "                loss = F.binary_cross_entropy(y_hat, y['labels'])\n",
    "                self.log('train_loss', loss)\n",
    "                return loss\n",
    "        \n",
    "        def validation_step(self, batch, batch_idx):\n",
    "                x, y = batch\n",
    "                y_hat = self(x)\n",
    "                loss = F.binary_cross_entropy(y_hat, y['labels'])\n",
    "                self.log('val_loss', loss)\n",
    "                return loss\n",
    "        \n",
    "        def configure_optimizers(self):\n",
    "                optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "                scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=5)\n",
    "                return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "        \n",
    "        def test_step(self, batch, batch_idx):\n",
    "                x, y = batch\n",
    "                y_hat = self(x)\n",
    "                loss = F.binary_cross_entropy(y_hat, y['labels'])\n",
    "                self.log('test_loss', loss)\n",
    "                return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinalModel(\n",
    "        input_size_1, hidden_size_1, num_layers_1, output_size_1,      # module 1    \n",
    "        input_size_2, n_head_2, dm_ff_2, dp_2, num_layers_2, act_fn_2, # module 2\n",
    "        input_size_3, n_head_3, dm_ff_3, dp_3, num_layers_3, act_fn_3, # module 3\n",
    "        input_size_4, hidden_size_4, num_layers_4, output_size_4,      # module 4\n",
    "        input_size_5, n_head_5, dm_ff_5, dp_5, num_layers_5, act_fn_5, # module 5\n",
    "        input_size_6, hidden_size_6, num_layers_6, output_size_6,      # module 6\n",
    "        fc1_out, fc2_out, fc3_out, fc4_out, fc5_out, dp=0.2, #masking = False            # final Model parameters\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type                  | Params\n",
      "---------------------------------------------------------\n",
      "0 | module1        | Module1GRU            | 284 K \n",
      "1 | module2        | Module2TransformerEnc | 27.6 M\n",
      "2 | module3        | Module3TransformerEnc | 5.8 M \n",
      "3 | module4        | Module4GRU            | 1.3 K \n",
      "4 | module5        | Module5TransformerEnc | 1.5 M \n",
      "5 | module6        | Module6GRU            | 242 K \n",
      "6 | sigmoid        | Sigmoid               | 0     \n",
      "7 | fc1            | Linear                | 855 K \n",
      "8 | classification | Sequential            | 1.0 M \n",
      "---------------------------------------------------------\n",
      "37.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "37.3 M    Total params\n",
      "149.266   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]x shape :  torch.Size([64, 23, 7])\n",
      "x shape :  torch.Size([64, 23, 7])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sgarc/SemEval/train.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/sgarc/SemEval/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39mEPOCHS)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/sgarc/SemEval/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, train_loader, val_loader)\n",
      "File \u001b[0;32m~/SemEval/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/SemEval/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/SemEval/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/SemEval/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/SemEval/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1032\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1033\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1035\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/SemEval/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1062\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1059\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1062\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1064\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1066\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/SemEval/venv/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SemEval/venv/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[1;32m    133\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/SemEval/venv/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    385\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m step_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    389\u001b[0m     \u001b[39melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_args)\n\u001b[1;32m    393\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    396\u001b[0m     \u001b[39m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/SemEval/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/SemEval/venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    402\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 403\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/sgarc/SemEval/train.ipynb Cell 18\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/sgarc/SemEval/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m x, y \u001b[39m=\u001b[39m batch\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/sgarc/SemEval/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/sgarc/SemEval/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(y_hat, y[\u001b[39m'\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/sgarc/SemEval/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/sgarc/SemEval/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/SemEval/venv/lib/python3.10/site-packages/torch/nn/functional.py:3122\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3119\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3120\u001b[0m     weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3122\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight, reduction_enum)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=EPOCHS)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
