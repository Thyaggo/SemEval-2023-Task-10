{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device =>  cpu  torch  2.1.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import lightning as L\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# Scoring\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device => \",device, ' torch ', torch.__version__)\n",
    "\n",
    "\n",
    "# hyper parameters\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "#@title Hyper Parameters { display-mode: \"both\" }\n",
    "\n",
    "EPOCHS             = 20\n",
    "MAX_NO_OF_SPEAKERS = 232\n",
    "MAX_DIALOGUE_LEN   = 33\n",
    "original_labels    = ['abuse', 'adoration', 'annoyance', 'awkwardness', 'benefit', 'boredom', 'calmness', 'challenge', 'cheer', 'confusion', 'curiosity', 'desire', 'excitement', 'guilt', 'horror', 'humour', 'impressed', 'loss', 'nervousness', 'nostalgia', 'pain', 'relief', 'satisfaction', 'scold', 'shock', 'sympathy', 'threat']\n",
    "train_count        = [31, 190, 1051, 880, 220, 78, 752, 214, 534, 486, 545, 180, 867, 216, 280, 153, 257, 351, 398, 65, 36, 173, 136, 94, 372, 209, 263]\n",
    "\n",
    "EMOTIONS           = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "# DataLoader Hyperparamaters\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Module 1 hyperparamaters(speaker_specific_emotion_sequence) : GRU n-n\n",
    "input_size_1  = 7\n",
    "hidden_size_1 = 10 \n",
    "num_layers_1  = 2 \n",
    "output_size_1 = 10\n",
    "\n",
    "\n",
    "# Module 2 hyperparamaters(utterance_context) : Transformer Enc\n",
    "input_size_2 = 768\n",
    "n_head_2     = 4\n",
    "dm_ff_2      = 2048\n",
    "dp_2         = 0.2\n",
    "num_layers_2 = 4 \n",
    "act_fn_2     = 'relu'\n",
    "\n",
    "# Module 3 hyperparamaters(speaker_context) : Transformer Enc\n",
    "input_size_3 = 232\n",
    "n_head_3     = 4\n",
    "dm_ff_3      = 2048\n",
    "dp_3         = 0.2\n",
    "num_layers_3 = 4 \n",
    "act_fn_3     = 'relu'\n",
    "\n",
    "# Module 4 hyperparamaters(global_emotion_sequence) : GRU\n",
    "input_size_4  = 7\n",
    "hidden_size_4 = 10 \n",
    "num_layers_4  = 2 \n",
    "output_size_4 = 7\n",
    "\n",
    "# Module 5 hyperparamaters(valence) : Transformer Enc\n",
    "input_size_5 = 33\n",
    "n_head_5     = 4\n",
    "dm_ff_5      = 512\n",
    "dp_5         = 0.2\n",
    "num_layers_5 = 4 \n",
    "act_fn_5     = 'relu'\n",
    "\n",
    "# Module 6 hyperparamaters(speaker_specific_valence_sequence) : GRU\n",
    "input_size_6  = 33\n",
    "hidden_size_6 = 10 \n",
    "num_layers_6  = 2 \n",
    "output_size_6 = 33\n",
    "\n",
    "# Final Model Hyperparamerters:\n",
    "fc1_out = 800\n",
    "fc2_out = 800\n",
    "fc3_out = 400\n",
    "fc4_out = 100\n",
    "fc5_out = 1\n",
    "\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_df.pkl', 'rb') as f:\n",
    "    train_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemEvalDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.len = len(self.data)\n",
    "        print(list(train_df.columns))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        dict_x = {}\n",
    "        dict_x['speaker'] = torch.tensor(self.data['speakers'][index], dtype=torch.int)\n",
    "        dict_x['emotion'] = torch.tensor(self.data['emotions'][index], dtype=torch.int)\n",
    "        dict_x['sentence_embeddings'] = torch.tensor(self.data['sentence_embeddings'][index], dtype=torch.float64)\n",
    "        dict_x['valence'] = torch.tensor(self.data['valence'][index])\n",
    "\n",
    "        dict_y = {}\n",
    "        dict_y['triggers'] =  torch.tensor(self.data['triggers'][index], dtype=torch.float32)\n",
    "\n",
    "        return dict_x, dict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['episode', 'speakers', 'emotions', 'utterances', 'triggers', 'sentence_embeddings', 'valence']\n"
     ]
    }
   ],
   "source": [
    "dataset = SemEvalDataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "total_size = len(dataset)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = int(total_size * val_ratio)\n",
    "test_size = total_size - train_size - val_size  # Asegurarse de que la suma sea total_size\n",
    "\n",
    "# Dividir el conjunto de datos\n",
    "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(i) for i in train_df['speakers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MELDCollate:\n",
    "    def __init__(self, pad_value = 0):\n",
    "        self.pad_value = pad_value\n",
    "    def __call__(self, batch):\n",
    "        speaker             = pad_sequence([item[0]['speaker'] for item in batch], batch_first = True)\n",
    "        emotion             = pad_sequence([item[0]['emotion'] for item in batch], batch_first = True)\n",
    "        sentence_embeddings = pad_sequence([item[0]['sentence_embeddings'] for item in batch], batch_first = True)\n",
    "        valence             = pad_sequence([item[0]['valence'] for item in batch], batch_first = True)\n",
    "        # print('\\noriginal list : ',[item[0]['speaker'] for item in batch], '\\n\\npadded list : ', speaker)\n",
    "        labels              = pad_sequence([item[1]['triggers'] for item in batch], batch_first = True)\n",
    "\n",
    "        dict_x = { 'speaker': speaker, 'emotion':emotion,  'sentence_embeddings':sentence_embeddings, 'valence':valence}\n",
    "        dict_y = {'labels': labels}\n",
    "\n",
    "        return dict_x, dict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader  = DataLoader(dataset = train_data, batch_size = 64, shuffle=True, collate_fn= MELDCollate())\n",
    "val_loader    = DataLoader(dataset = val_data, batch_size = 64, shuffle=True, collate_fn= MELDCollate())\n",
    "test_loader   = DataLoader(dataset = test_data, batch_size = 64, shuffle=True, collate_fn= MELDCollate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired_batch_index = 6\n",
    "# for i, batch in enumerate(train_loader):\n",
    "#     if i == desired_batch_index:\n",
    "#         # 'batch' contendrá el batch en el índice especificado\n",
    "#         print(f\"Batch {i}:\")\n",
    "#         print(batch)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 33])\n",
      "torch.Size([6, 231])\n",
      "(tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32), tensor([1, 0, 1, 0, 0, 0]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([0.0000, 0.0232, 0.0000, 0.0344, 0.1057, 0.0437]),\n",
       " tensor([0.1013, 0.0000, 0.0455, 0.0000, 0.0000, 0.0000])]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tesst = train_data.__getitem__(45)\n",
    "print(tesst[0]['valence'].shape)\n",
    "print(tesst[0]['speaker'].shape)\n",
    "print(tesst[0]['speaker'].unique(dim=0, return_inverse=True))\n",
    "test = tesst[0]['speaker'].unique(dim=0, return_inverse=True)[1]\n",
    "[torch.where(test == i, tesst[0]['valence'].mean(1), 0) for i in test.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module6GRU(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, output_size):\n",
    "        super(Module6GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Since there are maximum of 8 speakers in a dialogue, so we decided to make 8 GRUs one for each speaker.\n",
    "        self.gru_list= []\n",
    "        for id in range(MAX_NO_OF_SPEAKERS):\n",
    "            self.gru_list.append(nn.GRU(input_size, hidden_size, num_layers, batch_first = True))\n",
    "        self.gru_modules = nn.ModuleList(self.gru_list)\n",
    "\n",
    "    def valence_specific(valence, speaker):\n",
    "        speaker = speaker.unique(dim = 0, return_inverse=True)[1]\n",
    "        return [torch.where(speaker == i , valence, 0) for i in speaker.unique()]\n",
    "\n",
    "    def applyGRU(self, speaker_valence, seq_len):\n",
    "        speaker_output = torch.zeros(seq_len, self.output_size).to(device)\n",
    "        for sp_idx, valence in enumerate(speaker_valence):\n",
    "            # Verificar si hay alguna entrada para este hablante\n",
    "            if valence.nonzero().size(0) == 0:\n",
    "                continue\n",
    "\n",
    "            h0 = torch.zeros(self.num_layers, 1, self.hidden_size)\n",
    "            out, _ = self.gru_modules[sp_idx](valence, h0)\n",
    "\n",
    "            # Rellenar speaker_output con la salida correspondiente\n",
    "            for uid, output in enumerate(out):\n",
    "                speaker_output[uid] = output\n",
    "\n",
    "        return speaker_output\n",
    "\n",
    "    def forward(self, x, speakers):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len    = x.size(1)\n",
    "        outputs = []\n",
    "        for i in range(batch_size):\n",
    "            speaker_specific = self.valence_specific(x[i], speakers[i])\n",
    "            out = self.applyGRU(speaker_specific, seq_len)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        final_output = torch.cat([outputs[i].unsqueeze(2) for i in range(len(outputs))], 2).permute(2,0,1)\n",
    "        \n",
    "        return final_output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module5TransformerEnc(nn.Module):\n",
    "    # S, N, E : (seq_len, batch_size, input/embedding_size)\n",
    "    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n",
    "        super(Module5TransformerEnc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src): # src_shape : (S, N, E)\n",
    "        pad_value = torch.zeros(self.input_size).to(device)\n",
    "        # pad_value shape : (E), value : [0,0,0, ...]\n",
    "        ##src = src.transpose(0,1)\n",
    "        # src_shape : (N, S, E)\n",
    "\n",
    "        src_mask = torch.all(torch.eq(src,pad_value),2)\n",
    "        \n",
    "        # src_mask shape : (N, S), value : for each batch, it is contains seq_len sized tensors and contains true for pad and false for others\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: seq_len, batch_size, input_size \n",
    "        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n",
    "        x_mask = self.make_src_mask(x)\n",
    "        out = self.encoder(x, src_key_padding_mask = x_mask)  \n",
    "        # out shape : (S, N, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module4GRU(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, output_size):\n",
    "        super(Module4GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc  = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, hn = self.gru(x, h0)\n",
    "        \n",
    "        # shape of out :  (N, seq_len, hidden_size)     (torch.Size([10, 33, 8])) \n",
    "        # shape of hn  :  (num_layers, N, hidden_size)     (torch.Size([2, 10, 8]))\n",
    "        hn = torch.flatten(hn.permute(1,0,2), start_dim=1)\n",
    "        # shape of hn  :  (N, num_layers, hidden_size) and then flatten it to (N, num_layers*hiddem_size) 3D to 2D\n",
    "        output = self.fc(out)\n",
    "        # shape of output : [N, output_size]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module3TransformerEnc(nn.Module):\n",
    "    # S, N, E : (seq_len, batch_size, input/embedding_size)\n",
    "    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n",
    "        super(Module3TransformerEnc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src): # src_shape : (S, N, E)\n",
    "        pad_value = torch.zeros(self.input_size).to(device)\n",
    "        # pad_value shape : (E), value : [0,0,0, ...]\n",
    "        ##src = src.transpose(0,1)\n",
    "        # src_shape : (N, S, E)\n",
    "\n",
    "        src_mask = torch.all(torch.eq(src,pad_value),2)\n",
    "        \n",
    "        # src_mask shape : (N, S), value : for each batch, it is contains seq_len sized tensors and contains true for pad and false for others\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: seq_len, batch_size, input_size \n",
    "        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n",
    "        x_mask = self.make_src_mask(x)\n",
    "        out = self.encoder(x, src_key_padding_mask = x_mask)  \n",
    "        # out shape : (S, N, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module2TransformerEnc(nn.Module):\n",
    "    # S, N, E : (seq_len, batch_size, input/embedding_size)\n",
    "    def __init__(self, input_size, n_head, dim_ff, dp, num_layers, act_fn = 'relu'):\n",
    "        super(Module2TransformerEnc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model = input_size, nhead = n_head, dim_feedforward = dim_ff, dropout=dp, activation=act_fn)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src): # src_shape : (S, N, E)\n",
    "        pad_value = torch.zeros(self.input_size)\n",
    "        # pad_value shape : (E), value : [0,0,0, ...]\n",
    "        ##src = src.transpose(0,1)\n",
    "        # src_shape : (N, S, E)\n",
    "\n",
    "        src_mask = torch.all(torch.eq(src,pad_value),2)\n",
    "        \n",
    "        # src_mask shape : (N, S), value : for each batch, it is contains seq_len sized tensors and contains true for pad and false for others\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: seq_len, batch_size, input_size \n",
    "        # Since batch_first is not a parameter in trasformer so the input must be S, N, E\n",
    "        x_mask = self.make_src_mask(x)\n",
    "        out = self.encoder(x, src_key_padding_mask = x_mask)  \n",
    "        # out shape : (S, N, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module1GRU(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size, output_size):\n",
    "        super(Module1GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Since there are maximum of 8 speakers in a dialogue, so we decided to make 8 GRUs one for each speaker.\n",
    "        self.gru_list= []\n",
    "        for id in range(MAX_NO_OF_SPEAKERS):\n",
    "            self.gru_list.append(nn.GRU(input_size, hidden_size, num_layers, batch_first = True))\n",
    "        self.gru_modules = nn.ModuleList(self.gru_list)\n",
    "        # self.fc  = nn.Linear(num_layers*hidden_size, output_size)\n",
    "            \n",
    "    \n",
    "    def segregateEmotions(self, emotions, speakers):\n",
    "        speaker_specific = []\n",
    "        utt_id = []\n",
    "        for i in range(MAX_NO_OF_SPEAKERS):\n",
    "            speaker_tensor = torch.zeros(MAX_NO_OF_SPEAKERS, dtype = float)\n",
    "            speaker_tensor[i] = 1\n",
    "            emo = emotions[torch.nonzero((speakers == speaker_tensor).sum(dim=1) == speakers.size(1))].permute(1,0,2)\n",
    "            if(emo.size(1) == 0):\n",
    "                continue\n",
    "            utt_id.append(torch.nonzero((speakers == speaker_tensor).sum(dim=1) == speakers.size(1)).permute(1,0)[0])\n",
    "            speaker_specific.append(emo)\n",
    "#             print('\\n emo size : ',emo.size())\n",
    "#         print('\\n emo concat size : ',speaker_specific, utt_id)\n",
    "        return speaker_specific, utt_id\n",
    "    \n",
    "    def applyGRU(self, speaker_specific, utt_id, seq_len):\n",
    "        speaker_output = torch.zeros(seq_len, self.output_size)  \n",
    "        for sp_idx in range(len(utt_id)):\n",
    "            h0 = torch.zeros(self.num_layers, 1, self.hidden_size).to(device)\n",
    "            out, hn = self.gru_list[sp_idx](speaker_specific[sp_idx], h0)\n",
    "            for uid in range(utt_id[sp_idx].size(0)):\n",
    "                speaker_output[utt_id[sp_idx][uid]] = out[0][uid].clone()\n",
    "        return speaker_output\n",
    "\n",
    "    def forward(self, x, speakers):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len    = x.size(1)\n",
    "        outputs = []\n",
    "        for i in range(batch_size):\n",
    "            speaker_specific, utt_id = self.segregateEmotions(x[i], speakers[i])\n",
    "            out = self.applyGRU(speaker_specific, utt_id, seq_len)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        final_output = torch.cat([outputs[i].unsqueeze(2) for i in range(len(outputs))], 2).permute(2,0,1)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalModel(L.LightningModule):\n",
    "        def __init__(self, \n",
    "                input_size_1, hidden_size_1, num_layers_1, output_size_1,      # module 1    \n",
    "                input_size_2, n_head_2, dm_ff_2, dp_2, num_layers_2, act_fn_2, # module 2\n",
    "                input_size_3, n_head_3, dm_ff_3, dp_3, num_layers_3, act_fn_3, # module 3\n",
    "                input_size_4, hidden_size_4, num_layers_4, output_size_4,      # module 4\n",
    "                input_size_5, n_head_5, dm_ff_5, dp_5, num_layers_5, act_fn_5, # module 5\n",
    "                input_size_6, hidden_size_6, num_layers_6, output_size_6,      # module 6\n",
    "                fc1_out, fc2_out, fc3_out, fc4_out, fc5_out, dp, #masking = False            # final Model parameters\n",
    "                ):\n",
    "                super(FinalModel, self).__init__()\n",
    "\n",
    "                #self.masking = masking\n",
    "\n",
    "                self.module1 = Module1GRU(input_size = input_size_1, num_layers = num_layers_1, hidden_size = hidden_size_1, output_size = output_size_1)\n",
    "                self.module2 = Module2TransformerEnc(input_size = input_size_2, n_head = n_head_2, dim_ff = dm_ff_2, dp = dp_2, num_layers = num_layers_2, act_fn = act_fn_2)\n",
    "                self.module3 = Module3TransformerEnc(input_size = input_size_3, n_head = n_head_3, dim_ff = dm_ff_3, dp = dp_3, num_layers = num_layers_3, act_fn = act_fn_3)\n",
    "                self.module4 = Module4GRU(input_size = input_size_4, num_layers = num_layers_4, hidden_size = hidden_size_4, output_size = output_size_4)\n",
    "                self.module5 = Module5TransformerEnc(input_size = input_size_5, n_head = n_head_5, dim_ff = dm_ff_5, dp = dp_5, num_layers = num_layers_5, act_fn = act_fn_5)\n",
    "                self.module6 = Module6GRU(input_size = input_size_6, num_layers = num_layers_6, hidden_size = hidden_size_6, output_size = output_size_6)\n",
    "\n",
    "                \n",
    "                self.sigmoid = nn.Sigmoid()\n",
    "                self.fc1 = nn.Linear(input_size_2+input_size_3+input_size_5, fc1_out)\n",
    "                self.classification = nn.Sequential(\n",
    "                        nn.Linear(output_size_1 + fc1_out + output_size_4 + output_size_6, fc2_out),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dp), \n",
    "                        nn.Linear(fc2_out, fc3_out),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dp),\n",
    "                        nn.Linear(fc3_out, fc4_out),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dp),\n",
    "                        nn.Linear(fc4_out, fc5_out),\n",
    "                        # nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "        def forward(self, x):\n",
    "                speaker = x['speaker']\n",
    "                emotion = x['emotion']\n",
    "                sentence_embeddings = x['sentence_embeddings']\n",
    "                valence = x['valence']\n",
    "\n",
    "                out1 = self.module1(emotion, speaker)\n",
    "                out2 = self.module2(sentence_embeddings).permute(1,0,2)\n",
    "                out3 = self.module3(speaker).permute(1,0,2)\n",
    "                out4 = self.module4(emotion)\n",
    "                out5 = self.module5(valence).permute(1,0,2)\n",
    "                out6 = self.module6(valence, speaker)\n",
    "\n",
    "                out146 = torch.cat((out1, out4, out6), 2)\n",
    "                out234 = F.relu(self.fc1(torch.cat((out2, out3, out5), 2)))\n",
    "                \n",
    "                out123456 = torch.cat((out146, out234), 2)\n",
    "\n",
    "                outputs = []\n",
    "                for batch_idx in range(out123456.size(0)):\n",
    "                        seq_out = []\n",
    "                        for seq_idx in range(out123456.size(1)):\n",
    "                                op = self.classification(out123456[batch_idx][seq_idx])\n",
    "                                op = self.sigmoid(op)\n",
    "                                seq_out.append(op)\n",
    "                        outputs.append(seq_out)\n",
    "                return outputs\n",
    "\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "                x, y = batch\n",
    "                y_hat = self(x)\n",
    "                loss = F.binary_cross_entropy(y_hat, y['labels'])\n",
    "                self.log('train_loss', loss)\n",
    "                return loss\n",
    "        \n",
    "        def validation_step(self, batch, batch_idx):\n",
    "                x, y = batch\n",
    "                y_hat = self(x)\n",
    "                loss = F.binary_cross_entropy(y_hat, y['labels'])\n",
    "                self.log('val_loss', loss)\n",
    "                return loss\n",
    "        \n",
    "        def configure_optimizers(self):\n",
    "                optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "                return optimizer\n",
    "        \n",
    "        def test_step(self, batch, batch_idx):\n",
    "                x, y = batch\n",
    "                y_hat = self(x)\n",
    "                loss = F.binary_cross_entropy(y_hat, y['labels'])\n",
    "                self.log('test_loss', loss)\n",
    "                return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgarc/SemEval-2023-Task-10/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/sgarc/SemEval-2023-Task-10/train.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m FinalModel(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m         input_size_1, hidden_size_1, num_layers_1, output_size_1,      \u001b[39m# module 1    \u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         input_size_2, n_head_2, dm_ff_2, dp_2, num_layers_2, act_fn_2, \u001b[39m# module 2\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         input_size_3, n_head_3, dm_ff_3, dp_3, num_layers_3, act_fn_3, \u001b[39m# module 3\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         input_size_4, hidden_size_4, num_layers_4, output_size_4,      \u001b[39m# module 4\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         input_size_5, n_head_5, dm_ff_5, dp_5, num_layers_5, act_fn_5, \u001b[39m# module 5\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         input_size_6, hidden_size_6, num_layers_6, output_size_6,      \u001b[39m# module 6\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         fc1_out, fc2_out, fc3_out, fc4_out, fc5_out, dp\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, \u001b[39m#masking = False            # final Model parameters\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         )\n",
      "\u001b[1;32m/home/sgarc/SemEval-2023-Task-10/train.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule3 \u001b[39m=\u001b[39m Module3TransformerEnc(input_size \u001b[39m=\u001b[39m input_size_3, n_head \u001b[39m=\u001b[39m n_head_3, dim_ff \u001b[39m=\u001b[39m dm_ff_3, dp \u001b[39m=\u001b[39m dp_3, num_layers \u001b[39m=\u001b[39m num_layers_3, act_fn \u001b[39m=\u001b[39m act_fn_3)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule4 \u001b[39m=\u001b[39m Module4GRU(input_size \u001b[39m=\u001b[39m input_size_4, num_layers \u001b[39m=\u001b[39m num_layers_4, hidden_size \u001b[39m=\u001b[39m hidden_size_4, output_size \u001b[39m=\u001b[39m output_size_4)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule5 \u001b[39m=\u001b[39m Module5TransformerEnc(input_size \u001b[39m=\u001b[39;49m input_size_5, n_head \u001b[39m=\u001b[39;49m n_head_5, dim_ff \u001b[39m=\u001b[39;49m dm_ff_5, dp \u001b[39m=\u001b[39;49m dp_5, num_layers \u001b[39m=\u001b[39;49m num_layers_5, act_fn \u001b[39m=\u001b[39;49m act_fn_5)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule6 \u001b[39m=\u001b[39m Module6GRU(input_size \u001b[39m=\u001b[39m input_size_6, num_layers \u001b[39m=\u001b[39m num_layers_6, hidden_size \u001b[39m=\u001b[39m hidden_size_6, output_size \u001b[39m=\u001b[39m output_size_6)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSigmoid()\n",
      "\u001b[1;32m/home/sgarc/SemEval-2023-Task-10/train.ipynb Cell 18\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39msuper\u001b[39m(Module5TransformerEnc, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m=\u001b[39m input_size\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mTransformerEncoderLayer(d_model \u001b[39m=\u001b[39;49m input_size, nhead \u001b[39m=\u001b[39;49m n_head, dim_feedforward \u001b[39m=\u001b[39;49m dim_ff, dropout\u001b[39m=\u001b[39;49mdp, activation\u001b[39m=\u001b[39;49mact_fn)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sgarc/SemEval-2023-Task-10/train.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mTransformerEncoder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layer, num_layers\u001b[39m=\u001b[39mnum_layers)\n",
      "File \u001b[0;32m~/SemEval-2023-Task-10/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:553\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.__init__\u001b[0;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, bias, device, dtype)\u001b[0m\n\u001b[1;32m    551\u001b[0m factory_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: device, \u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m: dtype}\n\u001b[1;32m    552\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m--> 553\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn \u001b[39m=\u001b[39m MultiheadAttention(d_model, nhead, dropout\u001b[39m=\u001b[39;49mdropout,\n\u001b[1;32m    554\u001b[0m                                     bias\u001b[39m=\u001b[39;49mbias, batch_first\u001b[39m=\u001b[39;49mbatch_first,\n\u001b[1;32m    555\u001b[0m                                     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs)\n\u001b[1;32m    556\u001b[0m \u001b[39m# Implementation of Feedforward model\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear1 \u001b[39m=\u001b[39m Linear(d_model, dim_feedforward, bias\u001b[39m=\u001b[39mbias, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[0;32m~/SemEval-2023-Task-10/venv/lib/python3.10/site-packages/torch/nn/modules/activation.py:991\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[0;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39m=\u001b[39m batch_first\n\u001b[1;32m    990\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim \u001b[39m=\u001b[39m embed_dim \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m num_heads\n\u001b[0;32m--> 991\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim \u001b[39m*\u001b[39m num_heads \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39m\"\u001b[39m\u001b[39membed_dim must be divisible by num_heads\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qkv_same_embed_dim:\n\u001b[1;32m    994\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((embed_dim, embed_dim), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "model = FinalModel(\n",
    "        input_size_1, hidden_size_1, num_layers_1, output_size_1,      # module 1    \n",
    "        input_size_2, n_head_2, dm_ff_2, dp_2, num_layers_2, act_fn_2, # module 2\n",
    "        input_size_3, n_head_3, dm_ff_3, dp_3, num_layers_3, act_fn_3, # module 3\n",
    "        input_size_4, hidden_size_4, num_layers_4, output_size_4,      # module 4\n",
    "        input_size_5, n_head_5, dm_ff_5, dp_5, num_layers_5, act_fn_5, # module 5\n",
    "        input_size_6, hidden_size_6, num_layers_6, output_size_6,      # module 6\n",
    "        fc1_out, fc2_out, fc3_out, fc4_out, fc5_out, dp=0.2, #masking = False            # final Model parameters\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(fast_dev_run=True)\n",
    "trainer.fit(model, train_loader, val_loader, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
